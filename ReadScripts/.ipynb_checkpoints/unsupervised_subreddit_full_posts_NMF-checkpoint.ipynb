{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "908e1448",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "462c1d63",
   "metadata": {},
   "outputs": [],
   "source": [
    "dftry2 = pd.read_csv(\"posts_st.txt\",\n",
    "                    sep = '|',\n",
    "                    names = ['AUTHOR','ID','SCORE','CREATED_DATE','TITLE','NUM_COMMENTS','SELFTEXT'],\n",
    "                    header=None, lineterminator = \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0fd963eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>AUTHOR</th>\n",
       "      <th>ID</th>\n",
       "      <th>SCORE</th>\n",
       "      <th>CREATED_DATE</th>\n",
       "      <th>TITLE</th>\n",
       "      <th>NUM_COMMENTS</th>\n",
       "      <th>SELFTEXT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>43395</th>\n",
       "      <td>Floker1</td>\n",
       "      <td>cdwiln</td>\n",
       "      <td>24310</td>\n",
       "      <td>2019-07-16</td>\n",
       "      <td>Imagine Mike and Nancy finding out that all th...</td>\n",
       "      <td>718</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43139</th>\n",
       "      <td>Utgoffalec</td>\n",
       "      <td>cees1t</td>\n",
       "      <td>21899</td>\n",
       "      <td>2019-07-17</td>\n",
       "      <td>Hi! My name is Alec Utgoff and I play Dr. Alex...</td>\n",
       "      <td>1659</td>\n",
       "      <td>You can follow me on Twitter @AlecUtgoff and I...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42027</th>\n",
       "      <td>coloredneon</td>\n",
       "      <td>cibkdz</td>\n",
       "      <td>16975</td>\n",
       "      <td>2019-07-27</td>\n",
       "      <td>Wills storyline doesnt seem to be about sexual...</td>\n",
       "      <td>813</td>\n",
       "      <td>I keep seeing posts and comments about Wills s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43275</th>\n",
       "      <td>brettcliffordgelman</td>\n",
       "      <td>ce4jk4</td>\n",
       "      <td>13026</td>\n",
       "      <td>2019-07-16</td>\n",
       "      <td>Im Brett Gelman aka Murray from Stranger Thing...</td>\n",
       "      <td>1225</td>\n",
       "      <td>Hi Reddit! Brett Gelman here. Im an actor and ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45096</th>\n",
       "      <td>AndreyIvchenko</td>\n",
       "      <td>cbiolx</td>\n",
       "      <td>10260</td>\n",
       "      <td>2019-07-10</td>\n",
       "      <td>Hello Reddit family, it's Andrey Ivchenko here...</td>\n",
       "      <td>1114</td>\n",
       "      <td>Really excited to be here with you all today f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62402</th>\n",
       "      <td>lendmeahann</td>\n",
       "      <td>4y0nsh</td>\n",
       "      <td>0</td>\n",
       "      <td>2016-08-16</td>\n",
       "      <td>A thought I've had since watching...</td>\n",
       "      <td>5</td>\n",
       "      <td>This isn't necessarily a theory, but I had thi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33978</th>\n",
       "      <td>AnyAd8934</td>\n",
       "      <td>m4etve</td>\n",
       "      <td>0</td>\n",
       "      <td>2021-03-13</td>\n",
       "      <td>Things I would like to see in season 4</td>\n",
       "      <td>4</td>\n",
       "      <td>Keith-yes,he works now at that video store,he ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45055</th>\n",
       "      <td>Pronay7</td>\n",
       "      <td>cbkeea</td>\n",
       "      <td>0</td>\n",
       "      <td>2019-07-10</td>\n",
       "      <td>I think they will link the mysteries of the St...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62406</th>\n",
       "      <td>[deleted]</td>\n",
       "      <td>4y0en0</td>\n",
       "      <td>0</td>\n",
       "      <td>2016-08-16</td>\n",
       "      <td>The sherif</td>\n",
       "      <td>0</td>\n",
       "      <td>[deleted]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55767</th>\n",
       "      <td>Jellysnake3486</td>\n",
       "      <td>7aws9n</td>\n",
       "      <td>0</td>\n",
       "      <td>2017-11-05</td>\n",
       "      <td>Why did everyone keep saying 11 dies at the en...</td>\n",
       "      <td>4</td>\n",
       "      <td>[removed]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>65507 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                    AUTHOR      ID  SCORE CREATED_DATE  \\\n",
       "43395              Floker1  cdwiln  24310   2019-07-16   \n",
       "43139           Utgoffalec  cees1t  21899   2019-07-17   \n",
       "42027          coloredneon  cibkdz  16975   2019-07-27   \n",
       "43275  brettcliffordgelman  ce4jk4  13026   2019-07-16   \n",
       "45096       AndreyIvchenko  cbiolx  10260   2019-07-10   \n",
       "...                    ...     ...    ...          ...   \n",
       "62402          lendmeahann  4y0nsh      0   2016-08-16   \n",
       "33978            AnyAd8934  m4etve      0   2021-03-13   \n",
       "45055              Pronay7  cbkeea      0   2019-07-10   \n",
       "62406            [deleted]  4y0en0      0   2016-08-16   \n",
       "55767       Jellysnake3486  7aws9n      0   2017-11-05   \n",
       "\n",
       "                                                   TITLE NUM_COMMENTS  \\\n",
       "43395  Imagine Mike and Nancy finding out that all th...          718   \n",
       "43139  Hi! My name is Alec Utgoff and I play Dr. Alex...         1659   \n",
       "42027  Wills storyline doesnt seem to be about sexual...          813   \n",
       "43275  Im Brett Gelman aka Murray from Stranger Thing...         1225   \n",
       "45096  Hello Reddit family, it's Andrey Ivchenko here...         1114   \n",
       "...                                                  ...          ...   \n",
       "62402               A thought I've had since watching...            5   \n",
       "33978             Things I would like to see in season 4            4   \n",
       "45055  I think they will link the mysteries of the St...            0   \n",
       "62406                                         The sherif            0   \n",
       "55767  Why did everyone keep saying 11 dies at the en...            4   \n",
       "\n",
       "                                                SELFTEXT  \n",
       "43395                                                NaN  \n",
       "43139  You can follow me on Twitter @AlecUtgoff and I...  \n",
       "42027  I keep seeing posts and comments about Wills s...  \n",
       "43275  Hi Reddit! Brett Gelman here. Im an actor and ...  \n",
       "45096  Really excited to be here with you all today f...  \n",
       "...                                                  ...  \n",
       "62402  This isn't necessarily a theory, but I had thi...  \n",
       "33978  Keith-yes,he works now at that video store,he ...  \n",
       "45055                                                NaN  \n",
       "62406                                          [deleted]  \n",
       "55767                                          [removed]  \n",
       "\n",
       "[65507 rows x 7 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dftry2.sort_values(by=['SCORE'],ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7d5e8867",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 65507 entries, 0 to 65506\n",
      "Data columns (total 7 columns):\n",
      " #   Column        Non-Null Count  Dtype \n",
      "---  ------        --------------  ----- \n",
      " 0   AUTHOR        65507 non-null  object\n",
      " 1   ID            65507 non-null  object\n",
      " 2   SCORE         65507 non-null  int64 \n",
      " 3   CREATED_DATE  65507 non-null  object\n",
      " 4   TITLE         65473 non-null  object\n",
      " 5   NUM_COMMENTS  65494 non-null  object\n",
      " 6   SELFTEXT      64029 non-null  object\n",
      "dtypes: int64(1), object(6)\n",
      "memory usage: 3.5+ MB\n"
     ]
    }
   ],
   "source": [
    "dftry2.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc3e27ef",
   "metadata": {},
   "source": [
    "### Removing where the title or the selftext are null and removing the posts where the moderator for the Subreddit deleted/removed the post or the original author deleted/removed the post."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "981939d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "dftry2.dropna(subset=['TITLE'], inplace = True)\n",
    "dftry2.dropna(subset=['SELFTEXT'], inplace = True)\n",
    "df_touse = dftry2[dftry2['SELFTEXT'] != \"[removed]\"].copy()\n",
    "df_to_use_fin = df_touse[df_touse['SELFTEXT'] != \"[deleted]\"].copy()\n",
    "df_to_use_fin['FULL_POSTS'] = df_to_use_fin['TITLE']+\".\"+\" \"+df_touse['SELFTEXT']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f0a29ac6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(43302, 8)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_to_use_fin.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "67075b83",
   "metadata": {},
   "outputs": [],
   "source": [
    "check = df_to_use_fin[df_to_use_fin['ID'] == 'vow5wm']\n",
    "#check = dftry[dftry['ID'] == '7pzv67']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9cb90f63",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>AUTHOR</th>\n",
       "      <th>ID</th>\n",
       "      <th>SCORE</th>\n",
       "      <th>CREATED_DATE</th>\n",
       "      <th>TITLE</th>\n",
       "      <th>NUM_COMMENTS</th>\n",
       "      <th>SELFTEXT</th>\n",
       "      <th>FULL_POSTS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>15179</th>\n",
       "      <td>StrangerTesting</td>\n",
       "      <td>vow5wm</td>\n",
       "      <td>1</td>\n",
       "      <td>2022-07-01</td>\n",
       "      <td>Stranger Things Season 4 Volume 2 Series Discu...</td>\n",
       "      <td>1</td>\n",
       "      <td># In this thread you can discuss the entirety ...</td>\n",
       "      <td>Stranger Things Season 4 Volume 2 Series Discu...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                AUTHOR      ID  SCORE CREATED_DATE  \\\n",
       "15179  StrangerTesting  vow5wm      1   2022-07-01   \n",
       "\n",
       "                                                   TITLE NUM_COMMENTS  \\\n",
       "15179  Stranger Things Season 4 Volume 2 Series Discu...            1   \n",
       "\n",
       "                                                SELFTEXT  \\\n",
       "15179  # In this thread you can discuss the entirety ...   \n",
       "\n",
       "                                              FULL_POSTS  \n",
       "15179  Stranger Things Season 4 Volume 2 Series Discu...  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "47220668",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "43302"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df_to_use_fin.ID.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7215aece",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_to_use_fin['CREATED_DATE'] = pd.to_datetime(df_to_use_fin['CREATED_DATE'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c056df61",
   "metadata": {},
   "source": [
    "### Only keeping posts from the release date of season 3 to current"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "450d6137",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_to_use_fin = df_to_use_fin[df_to_use_fin['CREATED_DATE']> \"2019-07-04\"].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5327fcb2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(31643, 8)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_to_use_fin.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d3377738",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>AUTHOR</th>\n",
       "      <th>ID</th>\n",
       "      <th>SCORE</th>\n",
       "      <th>CREATED_DATE</th>\n",
       "      <th>TITLE</th>\n",
       "      <th>NUM_COMMENTS</th>\n",
       "      <th>SELFTEXT</th>\n",
       "      <th>FULL_POSTS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Lower-Adhesiveness-3</td>\n",
       "      <td>y1yfqt</td>\n",
       "      <td>1</td>\n",
       "      <td>2022-10-12</td>\n",
       "      <td>The upsidedown turns on Vecna, can he survive?</td>\n",
       "      <td>1</td>\n",
       "      <td>For some reason Vecna loses all his control ov...</td>\n",
       "      <td>The upsidedown turns on Vecna, can he survive?...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>mrs_bombastic22</td>\n",
       "      <td>y1yf6g</td>\n",
       "      <td>1</td>\n",
       "      <td>2022-10-12</td>\n",
       "      <td>Season 4 rewatch - 2 questions</td>\n",
       "      <td>1</td>\n",
       "      <td>When 001 went on his killing spree in the lab....</td>\n",
       "      <td>Season 4 rewatch - 2 questions. When 001 went ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>ReliefComplex4339</td>\n",
       "      <td>y1sa5w</td>\n",
       "      <td>1</td>\n",
       "      <td>2022-10-12</td>\n",
       "      <td>Dear Billy and Vecna</td>\n",
       "      <td>1</td>\n",
       "      <td>So by episode 2-3 the kids were already convin...</td>\n",
       "      <td>Dear Billy and Vecna. So by episode 2-3 the ki...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>EstablishmentWild263</td>\n",
       "      <td>y1s62k</td>\n",
       "      <td>1</td>\n",
       "      <td>2022-10-12</td>\n",
       "      <td>The Demogorgon(S1) is the child of Vecna</td>\n",
       "      <td>1</td>\n",
       "      <td>After four seasons Ive noticed the secon...</td>\n",
       "      <td>The Demogorgon(S1) is the child of Vecna.     ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Lrac247</td>\n",
       "      <td>y1qzzh</td>\n",
       "      <td>1</td>\n",
       "      <td>2022-10-12</td>\n",
       "      <td>Anyone like my new creation?</td>\n",
       "      <td>1</td>\n",
       "      <td>&amp;amp;#x200B;https://preview.redd.it/67sxlhr77a...</td>\n",
       "      <td>Anyone like my new creation?. &amp;amp;#x200B;http...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48331</th>\n",
       "      <td>embarrassingaf123</td>\n",
       "      <td>c99gpu</td>\n",
       "      <td>9</td>\n",
       "      <td>2019-07-05</td>\n",
       "      <td>Next time I have a relationship struggle, I'm ...</td>\n",
       "      <td>8</td>\n",
       "      <td>This guy seems to just work</td>\n",
       "      <td>Next time I have a relationship struggle, I'm ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48333</th>\n",
       "      <td>Aseph88</td>\n",
       "      <td>c99g2p</td>\n",
       "      <td>1</td>\n",
       "      <td>2019-07-05</td>\n",
       "      <td>[minor spoilers]Thought Kline's sign looked fa...</td>\n",
       "      <td>0</td>\n",
       "      <td>Sorry for getting political but couldnt help b...</td>\n",
       "      <td>[minor spoilers]Thought Kline's sign looked fa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48334</th>\n",
       "      <td>Imperial3agle</td>\n",
       "      <td>c99fxn</td>\n",
       "      <td>1</td>\n",
       "      <td>2019-07-05</td>\n",
       "      <td>[No Spoilers] How sure are we that there will ...</td>\n",
       "      <td>10</td>\n",
       "      <td>By the way, I have only seen the first 4 episo...</td>\n",
       "      <td>[No Spoilers] How sure are we that there will ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48335</th>\n",
       "      <td>dudeRedditSucksNow</td>\n",
       "      <td>c99ft9</td>\n",
       "      <td>1</td>\n",
       "      <td>2019-07-05</td>\n",
       "      <td>In episode 2 where Joyce is watching Friends, ...</td>\n",
       "      <td>2</td>\n",
       "      <td>We can confirm an exact timeline of the show u...</td>\n",
       "      <td>In episode 2 where Joyce is watching Friends, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48336</th>\n",
       "      <td>Marty_McFlyJR</td>\n",
       "      <td>c99fc6</td>\n",
       "      <td>627</td>\n",
       "      <td>2019-07-05</td>\n",
       "      <td>Uhm... Why so I feel like I'm the only one...</td>\n",
       "      <td>69</td>\n",
       "      <td>That feels bad about Billy? He was literally a...</td>\n",
       "      <td>Uhm... Why so I feel like I'm the only one.......</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>31643 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                     AUTHOR      ID  SCORE CREATED_DATE  \\\n",
       "1      Lower-Adhesiveness-3  y1yfqt      1   2022-10-12   \n",
       "2           mrs_bombastic22  y1yf6g      1   2022-10-12   \n",
       "8         ReliefComplex4339  y1sa5w      1   2022-10-12   \n",
       "9      EstablishmentWild263  y1s62k      1   2022-10-12   \n",
       "12                  Lrac247  y1qzzh      1   2022-10-12   \n",
       "...                     ...     ...    ...          ...   \n",
       "48331     embarrassingaf123  c99gpu      9   2019-07-05   \n",
       "48333               Aseph88  c99g2p      1   2019-07-05   \n",
       "48334         Imperial3agle  c99fxn      1   2019-07-05   \n",
       "48335    dudeRedditSucksNow  c99ft9      1   2019-07-05   \n",
       "48336         Marty_McFlyJR  c99fc6    627   2019-07-05   \n",
       "\n",
       "                                                   TITLE NUM_COMMENTS  \\\n",
       "1         The upsidedown turns on Vecna, can he survive?            1   \n",
       "2                         Season 4 rewatch - 2 questions            1   \n",
       "8                                   Dear Billy and Vecna            1   \n",
       "9               The Demogorgon(S1) is the child of Vecna            1   \n",
       "12                          Anyone like my new creation?            1   \n",
       "...                                                  ...          ...   \n",
       "48331  Next time I have a relationship struggle, I'm ...            8   \n",
       "48333  [minor spoilers]Thought Kline's sign looked fa...            0   \n",
       "48334  [No Spoilers] How sure are we that there will ...           10   \n",
       "48335  In episode 2 where Joyce is watching Friends, ...            2   \n",
       "48336      Uhm... Why so I feel like I'm the only one...           69   \n",
       "\n",
       "                                                SELFTEXT  \\\n",
       "1      For some reason Vecna loses all his control ov...   \n",
       "2      When 001 went on his killing spree in the lab....   \n",
       "8      So by episode 2-3 the kids were already convin...   \n",
       "9            After four seasons Ive noticed the secon...   \n",
       "12     &amp;#x200B;https://preview.redd.it/67sxlhr77a...   \n",
       "...                                                  ...   \n",
       "48331                        This guy seems to just work   \n",
       "48333  Sorry for getting political but couldnt help b...   \n",
       "48334  By the way, I have only seen the first 4 episo...   \n",
       "48335  We can confirm an exact timeline of the show u...   \n",
       "48336  That feels bad about Billy? He was literally a...   \n",
       "\n",
       "                                              FULL_POSTS  \n",
       "1      The upsidedown turns on Vecna, can he survive?...  \n",
       "2      Season 4 rewatch - 2 questions. When 001 went ...  \n",
       "8      Dear Billy and Vecna. So by episode 2-3 the ki...  \n",
       "9      The Demogorgon(S1) is the child of Vecna.     ...  \n",
       "12     Anyone like my new creation?. &amp;#x200B;http...  \n",
       "...                                                  ...  \n",
       "48331  Next time I have a relationship struggle, I'm ...  \n",
       "48333  [minor spoilers]Thought Kline's sign looked fa...  \n",
       "48334  [No Spoilers] How sure are we that there will ...  \n",
       "48335  In episode 2 where Joyce is watching Friends, ...  \n",
       "48336  Uhm... Why so I feel like I'm the only one.......  \n",
       "\n",
       "[31643 rows x 8 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_to_use_fin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6c93748a",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy import displacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6ed0567c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#pip install https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.0.0/en_core_web_sm-3.0.0.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "04e7f84b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import string\n",
    "import nltk\n",
    "import en_core_web_sm\n",
    "#nlp = en_core_web_sm.load()\n",
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "dcd7d1ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text Cleaning Function\n",
    "import re\n",
    "\n",
    "def clean(text):\n",
    "    allowed_postags=[\"NOUN\", \"VERB\", \"ADJ\", \"ADV\", \"ADP\", \"PROPN\"]\n",
    "    \n",
    "    try: \n",
    "        text1 = re.sub(r\"http\\S+\", \"\", text)\n",
    "        #Convert text in lower case\n",
    "        text2 = text1.lower()\n",
    "        #text3 = text2.replace(\"[^a-zA-Z#]\", \" \")\n",
    "        #Removing Punctuations\n",
    "        punc_removed = text2.translate(str.maketrans('', '', string.punctuation))\n",
    "        doc= nlp(punc_removed)\n",
    "        text_out = [token.lemma_ for token in doc if token.is_stop == False and \\\n",
    "                    token.is_alpha and len(token)>2 and token.pos_ in allowed_postags\\\n",
    "                   ]\n",
    "        #txt = ' '.join(text_out)\n",
    "    except:\n",
    "        #txt = ''\n",
    "        text_out = ''\n",
    "    return text_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f1825580",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_to_use_fin['token_lemma'] = df_to_use_fin['FULL_POSTS'].apply(clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f439c06e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     /Users/jacquelineskunda/nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "polarity checked successfull\n"
     ]
    }
   ],
   "source": [
    "nltk.download('vader_lexicon')\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "\n",
    "sid = SentimentIntensityAnalyzer()\n",
    "\n",
    "def polarity(text):\n",
    "    pol = sid.polarity_scores(text)\n",
    "    return pol\n",
    "#polarity checking\n",
    "def joiner(text):\n",
    "    txt = ' '.join(text)\n",
    "    return txt\n",
    "df_to_use_fin['FULL_POSTS_CLEAN'] = df_to_use_fin['token_lemma'].apply(joiner)\n",
    "df_to_use_fin['polarity'] = df_to_use_fin['FULL_POSTS_CLEAN'].apply(polarity)\n",
    "df_to_use_fin['compound']  = df_to_use_fin['polarity'].apply(lambda score_dict: score_dict['compound'])\n",
    "\n",
    "df_to_use_fin['sentiment'] = df_to_use_fin['compound'].apply(lambda x: \"Positive\" if x>0 else(\"Negative\" if x<0 else \"Neutral\") )\n",
    "print(\"polarity checked successfull\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1bcb6b6b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(31643, 13)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_to_use_fin.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b943840c",
   "metadata": {},
   "source": [
    "### Remove posts where the tokenized and lemmatized title+selftext, aka POSTS, have a length less than 5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a40ceed0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(30660, 14)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_to_use_fin['tok_len'] = df_to_use_fin['token_lemma'].apply(lambda x: len(x))\n",
    "df_to_use_fin.groupby(['tok_len'])['tok_len'].count()#[48310]\n",
    "df_to_use_fin = df_to_use_fin[df_to_use_fin['tok_len']>=5].copy()\n",
    "df_to_use_fin.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "21b3670b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>AUTHOR</th>\n",
       "      <th>ID</th>\n",
       "      <th>SCORE</th>\n",
       "      <th>CREATED_DATE</th>\n",
       "      <th>TITLE</th>\n",
       "      <th>NUM_COMMENTS</th>\n",
       "      <th>SELFTEXT</th>\n",
       "      <th>FULL_POSTS</th>\n",
       "      <th>token_lemma</th>\n",
       "      <th>FULL_POSTS_CLEAN</th>\n",
       "      <th>polarity</th>\n",
       "      <th>compound</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>tok_len</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Lower-Adhesiveness-3</td>\n",
       "      <td>y1yfqt</td>\n",
       "      <td>1</td>\n",
       "      <td>2022-10-12</td>\n",
       "      <td>The upsidedown turns on Vecna, can he survive?</td>\n",
       "      <td>1</td>\n",
       "      <td>For some reason Vecna loses all his control ov...</td>\n",
       "      <td>The upsidedown turns on Vecna, can he survive?...</td>\n",
       "      <td>[upsidedown, turn, vecna, survive, reason, vec...</td>\n",
       "      <td>upsidedown turn vecna survive reason vecna los...</td>\n",
       "      <td>{'neg': 0.365, 'neu': 0.635, 'pos': 0.0, 'comp...</td>\n",
       "      <td>-0.8720</td>\n",
       "      <td>Negative</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>mrs_bombastic22</td>\n",
       "      <td>y1yf6g</td>\n",
       "      <td>1</td>\n",
       "      <td>2022-10-12</td>\n",
       "      <td>Season 4 rewatch - 2 questions</td>\n",
       "      <td>1</td>\n",
       "      <td>When 001 went on his killing spree in the lab....</td>\n",
       "      <td>Season 4 rewatch - 2 questions. When 001 went ...</td>\n",
       "      <td>[season, rewatch, question, go, killing, spree...</td>\n",
       "      <td>season rewatch question go killing spree lab k...</td>\n",
       "      <td>{'neg': 0.503, 'neu': 0.497, 'pos': 0.0, 'comp...</td>\n",
       "      <td>-0.8779</td>\n",
       "      <td>Negative</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 AUTHOR      ID  SCORE CREATED_DATE  \\\n",
       "1  Lower-Adhesiveness-3  y1yfqt      1   2022-10-12   \n",
       "2       mrs_bombastic22  y1yf6g      1   2022-10-12   \n",
       "\n",
       "                                            TITLE NUM_COMMENTS  \\\n",
       "1  The upsidedown turns on Vecna, can he survive?            1   \n",
       "2                  Season 4 rewatch - 2 questions            1   \n",
       "\n",
       "                                            SELFTEXT  \\\n",
       "1  For some reason Vecna loses all his control ov...   \n",
       "2  When 001 went on his killing spree in the lab....   \n",
       "\n",
       "                                          FULL_POSTS  \\\n",
       "1  The upsidedown turns on Vecna, can he survive?...   \n",
       "2  Season 4 rewatch - 2 questions. When 001 went ...   \n",
       "\n",
       "                                         token_lemma  \\\n",
       "1  [upsidedown, turn, vecna, survive, reason, vec...   \n",
       "2  [season, rewatch, question, go, killing, spree...   \n",
       "\n",
       "                                    FULL_POSTS_CLEAN  \\\n",
       "1  upsidedown turn vecna survive reason vecna los...   \n",
       "2  season rewatch question go killing spree lab k...   \n",
       "\n",
       "                                            polarity  compound sentiment  \\\n",
       "1  {'neg': 0.365, 'neu': 0.635, 'pos': 0.0, 'comp...   -0.8720  Negative   \n",
       "2  {'neg': 0.503, 'neu': 0.497, 'pos': 0.0, 'comp...   -0.8779  Negative   \n",
       "\n",
       "   tok_len  \n",
       "1       23  \n",
       "2       11  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_to_use_fin.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "60094a79",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'lda_model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/sk/13d9bc1s7wsbnd2qv8jmz03m0000gn/T/ipykernel_87613/3589617857.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mnum_topics\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_topics\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mtt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlda_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_topic_terms\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0mtopic\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m', '\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdictionary\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpair\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mpair\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtt\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"TOPIC: {} \\nTOPIC WORDS : {}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtopic\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'lda_model' is not defined"
     ]
    }
   ],
   "source": [
    "# Printing Topics and Its keywords\n",
    "num_topics = 10\n",
    "for i in range(num_topics):\n",
    "    tt = lda_model.get_topic_terms(i,10)\n",
    "    topic = ', '.join([dictionary[pair[0]] for pair in tt])\n",
    "    print(\"TOPIC: {} \\nTOPIC WORDS : {}\".format(i+1, topic ))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5d9ee09",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import CoherenceModel\n",
    "# Compute Coherence Score\n",
    "coherence_model_lda = CoherenceModel(model=lda_model, \n",
    "                                     texts=df_to_use_fin['token_lemma'], \n",
    "                                     dictionary=dictionary, coherence='c_v')\n",
    "coherence_lda = coherence_model_lda.get_coherence()\n",
    "print('\\nCoherence Score: ', coherence_lda)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d47e6107",
   "metadata": {},
   "source": [
    "## Non Negative Matrix Factorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8b77493e",
   "metadata": {},
   "outputs": [],
   "source": [
    "RANDOM_SEED = 655"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3c30f881",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.cluster import DBSCAN\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "\n",
    "# import the dataset from sklearn\n",
    "from sklearn.decomposition import NMF\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# import other required libs\n",
    "import pandas as pd\n",
    "\n",
    "# string manipulation libs\n",
    "import re\n",
    "import string\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "#metrics and etc\n",
    "from sklearn import metrics\n",
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c0561028",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>AUTHOR</th>\n",
       "      <th>ID</th>\n",
       "      <th>SCORE</th>\n",
       "      <th>CREATED_DATE</th>\n",
       "      <th>TITLE</th>\n",
       "      <th>NUM_COMMENTS</th>\n",
       "      <th>SELFTEXT</th>\n",
       "      <th>FULL_POSTS</th>\n",
       "      <th>token_lemma</th>\n",
       "      <th>FULL_POSTS_CLEAN</th>\n",
       "      <th>polarity</th>\n",
       "      <th>compound</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>tok_len</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Lower-Adhesiveness-3</td>\n",
       "      <td>y1yfqt</td>\n",
       "      <td>1</td>\n",
       "      <td>2022-10-12</td>\n",
       "      <td>The upsidedown turns on Vecna, can he survive?</td>\n",
       "      <td>1</td>\n",
       "      <td>For some reason Vecna loses all his control ov...</td>\n",
       "      <td>The upsidedown turns on Vecna, can he survive?...</td>\n",
       "      <td>[upsidedown, turn, vecna, survive, reason, vec...</td>\n",
       "      <td>upsidedown turn vecna survive reason vecna los...</td>\n",
       "      <td>{'neg': 0.365, 'neu': 0.635, 'pos': 0.0, 'comp...</td>\n",
       "      <td>-0.872</td>\n",
       "      <td>Negative</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 AUTHOR      ID  SCORE CREATED_DATE  \\\n",
       "1  Lower-Adhesiveness-3  y1yfqt      1   2022-10-12   \n",
       "\n",
       "                                            TITLE NUM_COMMENTS  \\\n",
       "1  The upsidedown turns on Vecna, can he survive?            1   \n",
       "\n",
       "                                            SELFTEXT  \\\n",
       "1  For some reason Vecna loses all his control ov...   \n",
       "\n",
       "                                          FULL_POSTS  \\\n",
       "1  The upsidedown turns on Vecna, can he survive?...   \n",
       "\n",
       "                                         token_lemma  \\\n",
       "1  [upsidedown, turn, vecna, survive, reason, vec...   \n",
       "\n",
       "                                    FULL_POSTS_CLEAN  \\\n",
       "1  upsidedown turn vecna survive reason vecna los...   \n",
       "\n",
       "                                            polarity  compound sentiment  \\\n",
       "1  {'neg': 0.365, 'neu': 0.635, 'pos': 0.0, 'comp...    -0.872  Negative   \n",
       "\n",
       "   tok_len  \n",
       "1       23  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df_to_use_fin.copy()\n",
    "df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "46ddff69",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "import pandas as pd\n",
    "tf_idf_vectorizer = TfidfVectorizer(max_features=20000, min_df=10) #1\n",
    "tf_idf = tf_idf_vectorizer.fit_transform(df_to_use_fin['FULL_POSTS_CLEAN']) #2\n",
    "#tf_idf_array = tf_idf.toarray() \n",
    "X = pd.DataFrame(tf_idf.toarray(),columns=tf_idf_vectorizer.get_feature_names_out())#4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c5fafe87",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(30660, 6205)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "4c1bcc0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/site-packages/sklearn/decomposition/_nmf.py:289: FutureWarning: The 'init' value, when 'init=None' and n_components is less than n_samples and n_features, will be changed from 'nndsvd' to 'nndsvda' in 1.1 (renaming of 0.26).\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "#Trying NMF on just 10 topics with basic parameters\n",
    "# Create an NMF instance: model\n",
    "# the 10 components will be the topics\n",
    "model = NMF(n_components=10, random_state=RANDOM_SEED)\n",
    " \n",
    "# Fit the model to TF-IDF\n",
    "model.fit(tf_idf)\n",
    " \n",
    "# Transform the TF-IDF: nmf_features\n",
    "nmf_features = model.transform(tf_idf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "940b8237",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(30660, 10)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nmf_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "4130ee73",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 6205)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.components_.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a5fa0189",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/site-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>abandon</th>\n",
       "      <th>abduct</th>\n",
       "      <th>abduction</th>\n",
       "      <th>ability</th>\n",
       "      <th>able</th>\n",
       "      <th>abnormal</th>\n",
       "      <th>abomination</th>\n",
       "      <th>abrupt</th>\n",
       "      <th>abruptly</th>\n",
       "      <th>absence</th>\n",
       "      <th>...</th>\n",
       "      <th>youth</th>\n",
       "      <th>youtube</th>\n",
       "      <th>youtuber</th>\n",
       "      <th>yuri</th>\n",
       "      <th>yuris</th>\n",
       "      <th>zap</th>\n",
       "      <th>zombie</th>\n",
       "      <th>zone</th>\n",
       "      <th>zoom</th>\n",
       "      <th>zoomer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.041512</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.001860</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.013834</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.007931</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000036</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.002545</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.002577</td>\n",
       "      <td>0.045405</td>\n",
       "      <td>0.024546</td>\n",
       "      <td>0.000102</td>\n",
       "      <td>0.004553</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4.778247e-04</td>\n",
       "      <td>0.002820</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001506</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000195</td>\n",
       "      <td>0.011448</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000227</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.013844</td>\n",
       "      <td>0.000819</td>\n",
       "      <td>0.085204</td>\n",
       "      <td>0.097155</td>\n",
       "      <td>0.001754</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.051545e-04</td>\n",
       "      <td>0.000685</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.004194</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000509</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.004161</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.000328</td>\n",
       "      <td>0.002160</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001599</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001288</td>\n",
       "      <td>0.000706</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000499</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000218</td>\n",
       "      <td>0.052714</td>\n",
       "      <td>0.008938</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000967</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.006318</td>\n",
       "      <td>0.003255</td>\n",
       "      <td>0.002311</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.006913</td>\n",
       "      <td>0.001194</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.019531</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000342</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.001738</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000934</td>\n",
       "      <td>0.001486</td>\n",
       "      <td>0.038945</td>\n",
       "      <td>0.003965</td>\n",
       "      <td>0.006955</td>\n",
       "      <td>0.003852</td>\n",
       "      <td>0.001731</td>\n",
       "      <td>0.007652</td>\n",
       "      <td>0.000180</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.008907</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4.796296e-07</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.004058</td>\n",
       "      <td>0.045972</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000526</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.008539</td>\n",
       "      <td>0.001263</td>\n",
       "      <td>0.002463</td>\n",
       "      <td>0.004147</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.003570</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.008257</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.005281</td>\n",
       "      <td>1.103637e-03</td>\n",
       "      <td>0.002412</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.003734</td>\n",
       "      <td>0.000798</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000272</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.006592</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000645</td>\n",
       "      <td>0.003741</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.823314e-03</td>\n",
       "      <td>0.006137</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.029501</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.002318</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000169</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.017108</td>\n",
       "      <td>0.007403</td>\n",
       "      <td>0.001189</td>\n",
       "      <td>0.046372</td>\n",
       "      <td>0.087922</td>\n",
       "      <td>0.001265</td>\n",
       "      <td>0.001313</td>\n",
       "      <td>0.002276</td>\n",
       "      <td>2.008793e-03</td>\n",
       "      <td>0.004270</td>\n",
       "      <td>...</td>\n",
       "      <td>0.004844</td>\n",
       "      <td>0.010969</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.004816</td>\n",
       "      <td>0.000029</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.018604</td>\n",
       "      <td>0.009473</td>\n",
       "      <td>0.006571</td>\n",
       "      <td>0.005248</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.007451</td>\n",
       "      <td>0.012407</td>\n",
       "      <td>0.006424</td>\n",
       "      <td>0.105463</td>\n",
       "      <td>0.266319</td>\n",
       "      <td>0.001130</td>\n",
       "      <td>0.000608</td>\n",
       "      <td>0.000966</td>\n",
       "      <td>8.924901e-04</td>\n",
       "      <td>0.001298</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001233</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000035</td>\n",
       "      <td>0.001518</td>\n",
       "      <td>0.003479</td>\n",
       "      <td>0.005744</td>\n",
       "      <td>0.002980</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 6205 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    abandon    abduct  abduction   ability      able  abnormal  abomination  \\\n",
       "0  0.000000  0.000000   0.000000  0.000000  0.041512  0.000000     0.000000   \n",
       "1  0.002545  0.000000   0.002577  0.045405  0.024546  0.000102     0.004553   \n",
       "2  0.000000  0.013844   0.000819  0.085204  0.097155  0.001754     0.000000   \n",
       "3  0.000328  0.002160   0.000000  0.001599  0.000000  0.001288     0.000706   \n",
       "4  0.006913  0.001194   0.000000  0.000000  0.019531  0.000000     0.000342   \n",
       "5  0.000000  0.000000   0.000000  0.000000  0.008907  0.000000     0.000000   \n",
       "6  0.003570  0.000000   0.000000  0.000000  0.008257  0.000000     0.000000   \n",
       "7  0.006592  0.000000   0.000000  0.000000  0.000000  0.000645     0.003741   \n",
       "8  0.017108  0.007403   0.001189  0.046372  0.087922  0.001265     0.001313   \n",
       "9  0.007451  0.012407   0.006424  0.105463  0.266319  0.001130     0.000608   \n",
       "\n",
       "     abrupt      abruptly   absence  ...     youth   youtube  youtuber  \\\n",
       "0  0.000000  0.000000e+00  0.001860  ...  0.000000  0.013834  0.000000   \n",
       "1  0.000000  4.778247e-04  0.002820  ...  0.001506  0.000000  0.000000   \n",
       "2  0.000000  3.051545e-04  0.000685  ...  0.000000  0.000000  0.004194   \n",
       "3  0.000000  0.000000e+00  0.000499  ...  0.000218  0.052714  0.008938   \n",
       "4  0.000000  0.000000e+00  0.001738  ...  0.000000  0.000934  0.001486   \n",
       "5  0.000000  4.796296e-07  0.000000  ...  0.004058  0.045972  0.000000   \n",
       "6  0.005281  1.103637e-03  0.002412  ...  0.000000  0.000000  0.000000   \n",
       "7  0.000000  1.823314e-03  0.006137  ...  0.000000  0.000000  0.000000   \n",
       "8  0.002276  2.008793e-03  0.004270  ...  0.004844  0.010969  0.000000   \n",
       "9  0.000966  8.924901e-04  0.001298  ...  0.001233  0.000000  0.000000   \n",
       "\n",
       "       yuri     yuris       zap    zombie      zone      zoom    zoomer  \n",
       "0  0.000000  0.000000  0.000000  0.000000  0.007931  0.000000  0.000036  \n",
       "1  0.000000  0.000000  0.000195  0.011448  0.000000  0.000000  0.000227  \n",
       "2  0.000000  0.000509  0.000000  0.000000  0.000000  0.004161  0.000000  \n",
       "3  0.000000  0.000967  0.000000  0.006318  0.003255  0.002311  0.000000  \n",
       "4  0.038945  0.003965  0.006955  0.003852  0.001731  0.007652  0.000180  \n",
       "5  0.000526  0.000000  0.008539  0.001263  0.002463  0.004147  0.000000  \n",
       "6  0.003734  0.000798  0.000000  0.000000  0.000000  0.000000  0.000272  \n",
       "7  0.029501  0.000000  0.000000  0.002318  0.000000  0.000000  0.000169  \n",
       "8  0.004816  0.000029  0.000000  0.018604  0.009473  0.006571  0.005248  \n",
       "9  0.000000  0.000035  0.001518  0.003479  0.005744  0.002980  0.000000  \n",
       "\n",
       "[10 rows x 6205 columns]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "components_df = pd.DataFrame(model.components_, columns=tf_idf_vectorizer.get_feature_names())\n",
    "components_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "8be7547f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For topic 1 the words with the highest value are:\n",
      "season     5.448604\n",
      "episode    0.918254\n",
      "watch      0.778134\n",
      "end        0.537894\n",
      "good       0.434750\n",
      "release    0.376154\n",
      "think      0.375202\n",
      "finish     0.370733\n",
      "time       0.342504\n",
      "new        0.307801\n",
      "Name: 0, dtype: float64\n",
      "\n",
      "\n",
      "For topic 2 the words with the highest value are:\n",
      "mind       2.590619\n",
      "flayer     2.562181\n",
      "billy      0.401938\n",
      "control    0.329030\n",
      "monster    0.265460\n",
      "form       0.185981\n",
      "hive       0.151228\n",
      "flay       0.151105\n",
      "human      0.135448\n",
      "create     0.132393\n",
      "Name: 1, dtype: float64\n",
      "\n",
      "\n",
      "For topic 3 the words with the highest value are:\n",
      "vecna     3.345580\n",
      "kill      0.764863\n",
      "max       0.739547\n",
      "think     0.627203\n",
      "theory    0.596514\n",
      "power     0.570444\n",
      "victim    0.297155\n",
      "die       0.277801\n",
      "eddie     0.265225\n",
      "henry     0.250669\n",
      "Name: 2, dtype: float64\n",
      "\n",
      "\n",
      "For topic 4 the words with the highest value are:\n",
      "thing       2.655268\n",
      "strange     2.438979\n",
      "watch       0.816778\n",
      "stranger    0.561598\n",
      "game        0.259764\n",
      "good        0.212300\n",
      "netflix     0.200372\n",
      "want        0.197301\n",
      "series      0.188004\n",
      "fan         0.183411\n",
      "Name: 3, dtype: float64\n",
      "\n",
      "\n",
      "For topic 5 the words with the highest value are:\n",
      "hopper      2.676849\n",
      "russian     1.047797\n",
      "joyce       0.834287\n",
      "american    0.732104\n",
      "alive       0.566067\n",
      "theory      0.493342\n",
      "end         0.437693\n",
      "think       0.434773\n",
      "russia      0.375375\n",
      "scene       0.374620\n",
      "Name: 4, dtype: float64\n",
      "\n",
      "\n",
      "For topic 6 the words with the highest value are:\n",
      "song          3.407556\n",
      "play          1.096119\n",
      "music         0.623877\n",
      "episode       0.451238\n",
      "scene         0.426913\n",
      "eddie         0.359795\n",
      "soundtrack    0.353737\n",
      "favorite      0.332530\n",
      "run           0.305276\n",
      "hill          0.297765\n",
      "Name: 5, dtype: float64\n",
      "\n",
      "\n",
      "For topic 7 the words with the highest value are:\n",
      "steve           3.244436\n",
      "nancy           2.239503\n",
      "jonathan        1.277234\n",
      "robin           1.227578\n",
      "dustin          0.736214\n",
      "love            0.299362\n",
      "johnathan       0.230752\n",
      "relationship    0.211115\n",
      "harrington      0.180520\n",
      "eddie           0.175608\n",
      "Name: 6, dtype: float64\n",
      "\n",
      "\n",
      "For topic 8 the words with the highest value are:\n",
      "character    3.880220\n",
      "die          1.400797\n",
      "eddie        1.016666\n",
      "death        0.816650\n",
      "main         0.814227\n",
      "favorite     0.726353\n",
      "kill         0.585001\n",
      "like         0.382377\n",
      "opinion      0.377980\n",
      "feel         0.372935\n",
      "Name: 7, dtype: float64\n",
      "\n",
      "\n",
      "For topic 9 the words with the highest value are:\n",
      "like      1.643327\n",
      "mike      1.587248\n",
      "think     1.156033\n",
      "feel      0.967210\n",
      "know      0.964858\n",
      "people    0.716002\n",
      "love      0.640232\n",
      "time      0.624697\n",
      "go        0.619606\n",
      "say       0.604147\n",
      "Name: 8, dtype: float64\n",
      "\n",
      "\n",
      "For topic 10 the words with the highest value are:\n",
      "upside        2.407992\n",
      "gate          1.642390\n",
      "open          1.201234\n",
      "demogorgon    0.990593\n",
      "hawkin        0.714897\n",
      "portal        0.607499\n",
      "world         0.579868\n",
      "question      0.544523\n",
      "russian       0.468837\n",
      "time          0.405141\n",
      "Name: 9, dtype: float64\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for topic in range(components_df.shape[0]):\n",
    "    tmp = components_df.iloc[topic]\n",
    "    print(f'For topic {topic+1} the words with the highest value are:')\n",
    "    print(tmp.nlargest(10))\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "459ba9d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d124ee40",
   "metadata": {},
   "source": [
    "### NMF looks like it is producing clearer topics than LDA, lets hyper parameter tune."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "69a568a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "##https://www.kaggle.com/code/akashram/topic-modeling-intro-implementation/notebook#Non-Negative-Matrix-Factorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "b895063e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbb6d0b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_model = Word2Vec(df_to_use_fin['token_lemma'], vector_size=20000, min_count=20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2feae468",
   "metadata": {},
   "outputs": [],
   "source": [
    "kcluster=[4,6,8,10,12,20,50]\n",
    "\n",
    "topic_models = []\n",
    "# try each value of k\n",
    "for k in kcluster:\n",
    "    print(\"Applying NMF for k=%d ...\" % k )\n",
    "    # run NMF\n",
    "    model = NMF(max_iter=1000,init=\"nndsvd\", n_components=k) \n",
    "    W = model.fit_transform(tf_idf)\n",
    "    H = model.components_    \n",
    "    # store for later\n",
    "    topic_models.append( (k,W,H) )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "672eb4e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_descriptor(all_terms, H, topic_index, top):\n",
    "    # reverse sort the values to sort the indices\n",
    "    top_indices = np.argsort( H[topic_index,:] )[::-1]\n",
    "    # now get the terms corresponding to the top-ranked indices\n",
    "    top_terms = []\n",
    "    for term_index in top_indices[0:top]:\n",
    "        top_terms.append(all_terms[term_index])\n",
    "    return top_terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81c06c10",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_coherence( w2v_model, term_rankings ):\n",
    "    overall_coherence = 0.0\n",
    "    for topic_index in range(len(term_rankings)):\n",
    "        # check each pair of terms\n",
    "        pair_scores = []\n",
    "        for pair in combinations( term_rankings[topic_index], 2 ):\n",
    "            #print(str(pair[0]) + \" \" + str(pair[1]))\n",
    "            pair_scores.append(w2v_model.wv.similarity(pair[0], pair[1]))\n",
    "        # get the mean for all pairs in this topic\n",
    "        topic_score = sum(pair_scores) / len(pair_scores)\n",
    "        overall_coherence += topic_score\n",
    "    # get the mean score across all topics\n",
    "    return overall_coherence / len(term_rankings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28c88f59",
   "metadata": {},
   "outputs": [],
   "source": [
    "#term_rankings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "656fd6bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import combinations\n",
    "\n",
    "k_values = []\n",
    "coherences = []\n",
    "for (k,W,H) in topic_models:\n",
    "    # Get all of the topic descriptors - the term_rankings, based on top 10 terms\n",
    "    term_rankings = []\n",
    "    for topic_index in range(k):\n",
    "        term_rankings.append( get_descriptor( terms, H, topic_index, 10 ) )\n",
    "    # Now calculate the coherence based on our Word2vec model\n",
    "    k_values.append( k )\n",
    "    coherences.append( calculate_coherence( w2v_model, term_rankings ) )\n",
    "    print(\"K=%02d: Coherence=%.4f\" % ( k, coherences[-1] ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18452477",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "%matplotlib inline\n",
    "plt.style.use(\"ggplot\")\n",
    "matplotlib.rcParams.update({\"font.size\": 14})\n",
    "\n",
    "fig = plt.figure(figsize=(13,7))\n",
    "# create the line plot\n",
    "ax = plt.plot( k_values, coherences )\n",
    "plt.xticks(k_values)\n",
    "plt.xlabel(\"Number of Topics\")\n",
    "plt.ylabel(\"Mean Coherence\")\n",
    "# add the points\n",
    "plt.scatter( k_values, coherences, s=120)\n",
    "# find and annotate the maximum point on the plot\n",
    "ymax = max(coherences)\n",
    "xpos = coherences.index(ymax)\n",
    "best_k = k_values[xpos]\n",
    "plt.annotate( \"k=%d\" % best_k, xy=(best_k, ymax), xytext=(best_k, ymax), textcoords=\"offset points\", fontsize=16)\n",
    "# show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4cd2d56",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
