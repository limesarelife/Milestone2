{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8cbc4d3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "#add other dependencies for visuals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fce295fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#path_link=\"Data/Seasons_Episodes/output/charlines_df.csv\"\n",
    "#df_charlines = pd.read_csv(path_link)\n",
    "df_charlines = pd.read_csv(\"https://raw.githubusercontent.com/limesarelife/SIADS_696_Milestone2/main/Data/Seasons_Episodes/output/charlines_df.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e86b59a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_charlines.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c0b9bea8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>index</th>\n",
       "      <th>Character</th>\n",
       "      <th>Dialog</th>\n",
       "      <th>Season</th>\n",
       "      <th>Episode</th>\n",
       "      <th>Fix Name</th>\n",
       "      <th>Character_Fix</th>\n",
       "      <th>Spoke_Before</th>\n",
       "      <th>Character_Keep</th>\n",
       "      <th>Length Dialog</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>HOPPER</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>True</td>\n",
       "      <td>HOPPER</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Keep</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>HOPPER</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>True</td>\n",
       "      <td>HOPPER</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Keep</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>HOPPER</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>True</td>\n",
       "      <td>HOPPER</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Keep</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  index Character Dialog  Season  Episode Fix Name Character_Fix  \\\n",
       "0           0      3    HOPPER    NaN       3        5     True        HOPPER   \n",
       "1           0      3    HOPPER    NaN       3        5     True        HOPPER   \n",
       "2           0      3    HOPPER    NaN       3        5     True        HOPPER   \n",
       "\n",
       "  Spoke_Before Character_Keep  Length Dialog  \n",
       "0          NaN           Keep              0  \n",
       "1          NaN           Keep              0  \n",
       "2          NaN           Keep              0  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_charlines.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "642167e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_charlines.loc[df_charlines[\"Spoke_Before\"].isnull(),'Spoke_Before'] = df_charlines['Character_Fix']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6648e6fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>index</th>\n",
       "      <th>Character</th>\n",
       "      <th>Dialog</th>\n",
       "      <th>Season</th>\n",
       "      <th>Episode</th>\n",
       "      <th>Fix Name</th>\n",
       "      <th>Character_Fix</th>\n",
       "      <th>Spoke_Before</th>\n",
       "      <th>Character_Keep</th>\n",
       "      <th>Length Dialog</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>HOPPER</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>True</td>\n",
       "      <td>HOPPER</td>\n",
       "      <td>HOPPER</td>\n",
       "      <td>Keep</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>HOPPER</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>True</td>\n",
       "      <td>HOPPER</td>\n",
       "      <td>HOPPER</td>\n",
       "      <td>Keep</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>HOPPER</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>True</td>\n",
       "      <td>HOPPER</td>\n",
       "      <td>HOPPER</td>\n",
       "      <td>Keep</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  index Character Dialog  Season  Episode Fix Name Character_Fix  \\\n",
       "0           0      3    HOPPER    NaN       3        5     True        HOPPER   \n",
       "1           0      3    HOPPER    NaN       3        5     True        HOPPER   \n",
       "2           0      3    HOPPER    NaN       3        5     True        HOPPER   \n",
       "\n",
       "  Spoke_Before Character_Keep  Length Dialog  \n",
       "0       HOPPER           Keep              0  \n",
       "1       HOPPER           Keep              0  \n",
       "2       HOPPER           Keep              0  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_charlines.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "21483c24",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>index</th>\n",
       "      <th>Character</th>\n",
       "      <th>Dialog</th>\n",
       "      <th>Season</th>\n",
       "      <th>Episode</th>\n",
       "      <th>Fix Name</th>\n",
       "      <th>Character_Fix</th>\n",
       "      <th>Spoke_Before</th>\n",
       "      <th>Character_Keep</th>\n",
       "      <th>Length Dialog</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6065</th>\n",
       "      <td>39</td>\n",
       "      <td>270</td>\n",
       "      <td>CLAUDIA</td>\n",
       "      <td>Where did you see it last?</td>\n",
       "      <td>2</td>\n",
       "      <td>9</td>\n",
       "      <td>True</td>\n",
       "      <td>CLAUDIA</td>\n",
       "      <td>SUSAN</td>\n",
       "      <td>Keep</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6066</th>\n",
       "      <td>39</td>\n",
       "      <td>270</td>\n",
       "      <td>CLAUDIA</td>\n",
       "      <td>What's in there that's so important  anyway?  ...</td>\n",
       "      <td>2</td>\n",
       "      <td>9</td>\n",
       "      <td>True</td>\n",
       "      <td>CLAUDIA</td>\n",
       "      <td>SUSAN</td>\n",
       "      <td>Keep</td>\n",
       "      <td>132</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6067</th>\n",
       "      <td>39</td>\n",
       "      <td>270</td>\n",
       "      <td>CLAUDIA</td>\n",
       "      <td>That's your ride!  Dusty??? The bag from Melva...</td>\n",
       "      <td>2</td>\n",
       "      <td>9</td>\n",
       "      <td>True</td>\n",
       "      <td>CLAUDIA</td>\n",
       "      <td>SUSAN</td>\n",
       "      <td>Keep</td>\n",
       "      <td>75</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Unnamed: 0  index Character  \\\n",
       "6065          39    270   CLAUDIA   \n",
       "6066          39    270   CLAUDIA   \n",
       "6067          39    270   CLAUDIA   \n",
       "\n",
       "                                                 Dialog  Season  Episode  \\\n",
       "6065                         Where did you see it last?       2        9   \n",
       "6066  What's in there that's so important  anyway?  ...       2        9   \n",
       "6067  That's your ride!  Dusty??? The bag from Melva...       2        9   \n",
       "\n",
       "     Fix Name Character_Fix Spoke_Before Character_Keep  Length Dialog  \n",
       "6065     True       CLAUDIA        SUSAN           Keep             26  \n",
       "6066     True       CLAUDIA        SUSAN           Keep            132  \n",
       "6067     True       CLAUDIA        SUSAN           Keep             75  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_charlines.tail(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d69b2555",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_charlines.dropna(subset=['Dialog'],inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8e9f9a48",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5457, 10)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_charlines.drop(['Unnamed: 0'], axis=1, inplace = True)\n",
    "df_charlines.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "df1182db",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_charlines['Dialog_Check'] = df_charlines['Dialog'].apply(lambda x: x.isdigit())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "18a7e515",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_charlines['Remove_Lines'] = df_charlines['Dialog'].apply(lambda x: True if \"1/\" in x else False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d00f7d96",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_charlines[df_charlines['Remove_Lines']==True]\n",
    "#removing odd script lines when based on location setting that were caught\n",
    "df_charlines2 = df_charlines[df_charlines['Remove_Lines']==False].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5beb0b4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reordering columns\n",
    "cols = list(df_charlines2)\n",
    "# move the column to head of list using index, pop and insert\n",
    "cols.insert(0, cols.pop(cols.index('Character_Fix')))\n",
    "#print(cols)\n",
    "#['Character_Fix', 'Character', 'Dialog', 'Season', 'Episode', 'Fix Name', 'Length Dialog', 'Character_Keep', \n",
    "#'Dialog_Check', 'Remove_Lines']\n",
    "# use index to reorder\n",
    "df_charlines2 = df_charlines2.loc[:, cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7604f885",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Character_Fix</th>\n",
       "      <th>index</th>\n",
       "      <th>Character</th>\n",
       "      <th>Dialog</th>\n",
       "      <th>Season</th>\n",
       "      <th>Episode</th>\n",
       "      <th>Fix Name</th>\n",
       "      <th>Spoke_Before</th>\n",
       "      <th>Character_Keep</th>\n",
       "      <th>Length Dialog</th>\n",
       "      <th>Dialog_Check</th>\n",
       "      <th>Remove_Lines</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>HOPPER</td>\n",
       "      <td>3</td>\n",
       "      <td>HOPPER</td>\n",
       "      <td>Looks like somebody’s home. As the truck parks...</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>True</td>\n",
       "      <td>HOPPER</td>\n",
       "      <td>Keep</td>\n",
       "      <td>341</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>HOPPER</td>\n",
       "      <td>3</td>\n",
       "      <td>HOPPER</td>\n",
       "      <td>Where’s that coming from? Joyce pauses at the ...</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>True</td>\n",
       "      <td>HOPPER</td>\n",
       "      <td>Keep</td>\n",
       "      <td>165</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>HOPPER</td>\n",
       "      <td>3</td>\n",
       "      <td>HOPPER</td>\n",
       "      <td>Hey, dipshits! The men drop their tools and st...</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>True</td>\n",
       "      <td>HOPPER</td>\n",
       "      <td>Keep</td>\n",
       "      <td>80</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Character_Fix  index Character  \\\n",
       "5        HOPPER      3    HOPPER   \n",
       "6        HOPPER      3    HOPPER   \n",
       "7        HOPPER      3    HOPPER   \n",
       "\n",
       "                                              Dialog  Season  Episode  \\\n",
       "5  Looks like somebody’s home. As the truck parks...       3        5   \n",
       "6  Where’s that coming from? Joyce pauses at the ...       3        5   \n",
       "7  Hey, dipshits! The men drop their tools and st...       3        5   \n",
       "\n",
       "  Fix Name Spoke_Before Character_Keep  Length Dialog  Dialog_Check  \\\n",
       "5     True       HOPPER           Keep            341         False   \n",
       "6     True       HOPPER           Keep            165         False   \n",
       "7     True       HOPPER           Keep             80         False   \n",
       "\n",
       "   Remove_Lines  \n",
       "5         False  \n",
       "6         False  \n",
       "7         False  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_charlines2.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4807a581",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5451, 12)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_charlines2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5b86d740",
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly as py\n",
    "import plotly.express as px\n",
    "import numpy as np\n",
    "import re\n",
    "import plotly.graph_objs as go\n",
    "from plotly.offline import init_notebook_mode, iplot\n",
    "from collections import Counter\n",
    "from nltk import word_tokenize\n",
    "import nltk\n",
    "\n",
    "#order = df[\"Character_Fix\"].unique()\n",
    "#dialog = df[\"Dialog\"]\n",
    "#dialogCounts = dialog.value_counts()\n",
    "#dialogCounts = dialogCounts.sort_values(ascending = False)[0:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f8b971a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_charlines2.copy()\n",
    "df['Dialog_clean'] = df['Dialog'].apply(lambda x: re.sub(r'[^\\w\\s]','',x))\n",
    "df['Dialog_tokenized'] = df['Dialog_clean'].apply(lambda x: word_tokenize(x.strip().lower()))\n",
    "df['Dialog_Count'] = df['Dialog_tokenized'].apply(lambda x: len(x))\n",
    "LinesSpokeDf = df.groupby(['Character_Fix'])[\"Dialog_Count\"].sum().reset_index()\n",
    "LinesSpokeDf = LinesSpokeDf.sort_values(by=[\"Dialog_Count\"], ascending = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a7b6896c",
   "metadata": {},
   "outputs": [],
   "source": [
    "_20df = LinesSpokeDf.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7c0e769d",
   "metadata": {},
   "outputs": [],
   "source": [
    "top20chars = list(_20df.Character_Fix.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a7200416",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['HOPPER', 'MIKE', 'STEVE', 'DUSTIN', 'JOYCE', 'NANCY', 'ROBIN', 'MAX', 'JONATHAN', 'LUCAS', 'WILL', 'ELEVEN', 'MURRAY', 'BILLY', 'ERICA', 'KAREN', 'DR BRENNER', 'MAYOR KLINE', 'DR OWENS', 'TOM']\n"
     ]
    }
   ],
   "source": [
    "print(top20chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "eed36db6",
   "metadata": {},
   "outputs": [],
   "source": [
    "top20df = df[df['Character_Fix'].isin(top20chars)].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6069c599",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['HOPPER', 'JOYCE', 'JONATHAN', 'ROBIN', 'STEVE', 'NANCY', 'TOM', 'MAX', 'ELEVEN', 'DUSTIN', 'ERICA', 'LUCAS', 'WILL', 'KAREN', 'MIKE', 'MURRAY', 'BILLY', 'MAYOR KLINE', 'DR BRENNER', 'DR OWENS']\n"
     ]
    }
   ],
   "source": [
    "print(list(top20df.Character_Fix.unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3bb122c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "top20dfv2 = top20df[top20df['Season']==3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "78da2248",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 4]\n",
      "[4 2]\n"
     ]
    }
   ],
   "source": [
    "### DR BRENNER is assumed to be dead in season 3 so it makes sense he is not in the top 20\n",
    "### characters for season 3 and then dr owens is only is season 2 and season 4 and we \n",
    "### briefly see him in season 3 but not speaking.\n",
    "\n",
    "drb = top20df[top20df['Character_Fix']==\"DR BRENNER\"]\n",
    "dro = top20df[top20df['Character_Fix']==\"DR OWENS\"]\n",
    "print(drb.Season.unique())\n",
    "print(dro.Season.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e1398f32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['HOPPER', 'JOYCE', 'JONATHAN', 'ROBIN', 'STEVE', 'NANCY', 'TOM', 'MAX', 'ELEVEN', 'DUSTIN', 'ERICA', 'LUCAS', 'WILL', 'KAREN', 'MIKE', 'MURRAY', 'BILLY', 'MAYOR KLINE']\n"
     ]
    }
   ],
   "source": [
    "print(list(top20dfv2.Character_Fix.unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e9947bfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "top20df_S3 = top20dfv2.groupby(['Character_Fix','Episode'])['Dialog_Count'].sum().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "04c5c3fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Character_Fix</th>\n",
       "      <th>Episode</th>\n",
       "      <th>Dialog_Count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>BILLY</td>\n",
       "      <td>1</td>\n",
       "      <td>447</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>BILLY</td>\n",
       "      <td>2</td>\n",
       "      <td>246</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>BILLY</td>\n",
       "      <td>3</td>\n",
       "      <td>236</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>BILLY</td>\n",
       "      <td>4</td>\n",
       "      <td>340</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>BILLY</td>\n",
       "      <td>6</td>\n",
       "      <td>447</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Character_Fix  Episode  Dialog_Count\n",
       "0         BILLY        1           447\n",
       "1         BILLY        2           246\n",
       "2         BILLY        3           236\n",
       "3         BILLY        4           340\n",
       "4         BILLY        6           447"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top20df_S3.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6b181333",
   "metadata": {},
   "outputs": [],
   "source": [
    "gender = pd.read_csv('https://raw.githubusercontent.com/limesarelife/SIADS_696_Milestone2/main/Data/Seasons_Episodes/output/gender_st.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "986950b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Character_Fix</th>\n",
       "      <th>Gender</th>\n",
       "      <th>Age</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AGENT FRAZIER</td>\n",
       "      <td>Female</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AGENT HARMON</td>\n",
       "      <td>Male</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ALEXEI</td>\n",
       "      <td>Male</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ANDY</td>\n",
       "      <td>Male</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ARGYLE</td>\n",
       "      <td>Male</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>VICKIE</td>\n",
       "      <td>Female</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>VICTOR CREEL</td>\n",
       "      <td>Male</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>WAYNE</td>\n",
       "      <td>Male</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>WILL</td>\n",
       "      <td>Male</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>YURI</td>\n",
       "      <td>Male</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>62 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    Character_Fix  Gender  Age\n",
       "0   AGENT FRAZIER  Female    1\n",
       "1    AGENT HARMON    Male    1\n",
       "2          ALEXEI    Male    1\n",
       "3            ANDY    Male    2\n",
       "4          ARGYLE    Male    2\n",
       "..            ...     ...  ...\n",
       "57         VICKIE  Female    2\n",
       "58   VICTOR CREEL    Male    1\n",
       "59          WAYNE    Male    1\n",
       "60           WILL    Male    2\n",
       "61           YURI    Male    1\n",
       "\n",
       "[62 rows x 3 columns]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gender"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "320d60ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "gender_20df = pd.merge(_20df, gender, on=\"Character_Fix\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "79bc57c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "gender_20df['Gender'] = gender_20df['Gender'].apply(lambda x: x.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "aaa7e3a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Character_Fix</th>\n",
       "      <th>Gender</th>\n",
       "      <th>Age</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AGENT FRAZIER</td>\n",
       "      <td>Female</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AGENT HARMON</td>\n",
       "      <td>Male</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ALEXEI</td>\n",
       "      <td>Male</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ANDY</td>\n",
       "      <td>Male</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ARGYLE</td>\n",
       "      <td>Male</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>VICKIE</td>\n",
       "      <td>Female</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>VICTOR CREEL</td>\n",
       "      <td>Male</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>WAYNE</td>\n",
       "      <td>Male</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>WILL</td>\n",
       "      <td>Male</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>YURI</td>\n",
       "      <td>Male</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>62 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    Character_Fix  Gender  Age\n",
       "0   AGENT FRAZIER  Female    1\n",
       "1    AGENT HARMON    Male    1\n",
       "2          ALEXEI    Male    1\n",
       "3            ANDY    Male    2\n",
       "4          ARGYLE    Male    2\n",
       "..            ...     ...  ...\n",
       "57         VICKIE  Female    2\n",
       "58   VICTOR CREEL    Male    1\n",
       "59          WAYNE    Male    1\n",
       "60           WILL    Male    2\n",
       "61           YURI    Male    1\n",
       "\n",
       "[62 rows x 3 columns]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gender"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "3d5ae9e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "fulldf_gen = pd.merge(df, gender, on=\"Character_Fix\")\n",
    "fulldf_gen['Gender'] = fulldf_gen['Gender'].apply(lambda x: x.strip())\n",
    "eggos = fulldf_gen.groupby(['Age'])['Dialog_Count'].sum().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "38cf8010",
   "metadata": {},
   "outputs": [],
   "source": [
    "Total = eggos['Dialog_Count'].sum()\n",
    "eggos['percent'] = eggos.Dialog_Count.apply(lambda x: x/Total)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "3f019c1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_dict = top20dfv2.groupby('Character_Fix')['Dialog_clean'].apply(list).reset_index()#.apply(list).to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "7c20cf2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_dict['Dialog_clean'] = sent_dict['Dialog_clean'].apply(lambda x: \".\".join(str(e) for e in x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "4e09569b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Character_Fix</th>\n",
       "      <th>Dialog_clean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>BILLY</td>\n",
       "      <td>Dont be afraid Itll be over  soon Just try and...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>DUSTIN</td>\n",
       "      <td>Shit Shit.Why dont these buttons work.What do ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ELEVEN</td>\n",
       "      <td>JONATHAN . KAREN WHEELER .I found him MIKE WI...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ERICA</td>\n",
       "      <td>Press the button.JUST THEN the elevator comes ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>HOPPER</td>\n",
       "      <td>Looks like somebodys home As the truck parks w...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Character_Fix                                       Dialog_clean\n",
       "0         BILLY  Dont be afraid Itll be over  soon Just try and...\n",
       "1        DUSTIN  Shit Shit.Why dont these buttons work.What do ...\n",
       "2        ELEVEN   JONATHAN . KAREN WHEELER .I found him MIKE WI...\n",
       "3         ERICA  Press the button.JUST THEN the elevator comes ...\n",
       "4        HOPPER  Looks like somebodys home As the truck parks w..."
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent_dict.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "fb8afbf3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     /Users/jacquelineskunda/nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "polarity checked successfull\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('vader_lexicon')\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "\n",
    "sid = SentimentIntensityAnalyzer()\n",
    "\n",
    "\n",
    "def polarity(text):\n",
    "    pol = sid.polarity_scores(text)\n",
    "    return pol\n",
    "sent_dict['Dialog_clean'] = sent_dict['Dialog_clean'].apply(lambda x: x.lower())\n",
    "sent_dict['polarity'] = sent_dict['Dialog_clean'].apply(polarity)  #polarity checking\n",
    "sent_dict['compound']  = sent_dict['polarity'].apply(lambda score_dict: score_dict['compound'])\n",
    "\n",
    "sent_dict['sentiment'] = sent_dict['compound'].apply(lambda x: \"Positive\" if x>=0.5 \\\n",
    "                                                     else(\"Negative\" if x<=-0.5 \\\n",
    "                                                     else \"Neutral\"))\n",
    "print(\"polarity checked successfull\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "edec5f24",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Character_Fix</th>\n",
       "      <th>Dialog_clean</th>\n",
       "      <th>polarity</th>\n",
       "      <th>compound</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>BILLY</td>\n",
       "      <td>dont be afraid itll be over  soon just try and...</td>\n",
       "      <td>{'neg': 0.116, 'neu': 0.815, 'pos': 0.069, 'co...</td>\n",
       "      <td>-0.9982</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>DUSTIN</td>\n",
       "      <td>shit shit.why dont these buttons work.what do ...</td>\n",
       "      <td>{'neg': 0.091, 'neu': 0.803, 'pos': 0.106, 'co...</td>\n",
       "      <td>0.9970</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ELEVEN</td>\n",
       "      <td>jonathan . karen wheeler .i found him mike wi...</td>\n",
       "      <td>{'neg': 0.088, 'neu': 0.819, 'pos': 0.093, 'co...</td>\n",
       "      <td>0.9724</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Character_Fix                                       Dialog_clean  \\\n",
       "0         BILLY  dont be afraid itll be over  soon just try and...   \n",
       "1        DUSTIN  shit shit.why dont these buttons work.what do ...   \n",
       "2        ELEVEN   jonathan . karen wheeler .i found him mike wi...   \n",
       "\n",
       "                                            polarity  compound sentiment  \n",
       "0  {'neg': 0.116, 'neu': 0.815, 'pos': 0.069, 'co...   -0.9982  Negative  \n",
       "1  {'neg': 0.091, 'neu': 0.803, 'pos': 0.106, 'co...    0.9970  Positive  \n",
       "2  {'neg': 0.088, 'neu': 0.819, 'pos': 0.093, 'co...    0.9724  Positive  "
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent_dict.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c731544d",
   "metadata": {},
   "source": [
    "### Start of supervised Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "29f35efc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_show = df[['Character_Fix','Season',\"Episode\",\"Dialog\"]].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "71d2fbb0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Character_Fix</th>\n",
       "      <th>Season</th>\n",
       "      <th>Episode</th>\n",
       "      <th>Dialog</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>HOPPER</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>Looks like somebody’s home. As the truck parks...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>HOPPER</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>Where’s that coming from? Joyce pauses at the ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>HOPPER</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>Hey, dipshits! The men drop their tools and st...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Character_Fix  Season  Episode  \\\n",
       "5        HOPPER       3        5   \n",
       "6        HOPPER       3        5   \n",
       "7        HOPPER       3        5   \n",
       "\n",
       "                                              Dialog  \n",
       "5  Looks like somebody’s home. As the truck parks...  \n",
       "6  Where’s that coming from? Joyce pauses at the ...  \n",
       "7  Hey, dipshits! The men drop their tools and st...  "
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_show.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "12c6bb73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to preprocess text\n",
    "def cleaner(text):\n",
    "  \t# Create doc object\n",
    "    doc = nlp(text)\n",
    "    \n",
    "    # Generate lemmas and lower case and remove punctuation\n",
    "    lemmas = [token.lemma_.lower() for token in doc if not token.is_punct\\\n",
    "              and token.is_alpha]\n",
    "    #token.is_stop == False \n",
    "    # Remove stopwords, extra characters like numbers (second check)\n",
    "    #all_lemmas = [lemma for lemma in lemmas if lemma.isalpha]\n",
    "    \n",
    "    return \" \".join(lemmas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "658303d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy import displacy\n",
    "import seaborn as sns\n",
    "import string\n",
    "import nltk\n",
    "import en_core_web_sm\n",
    "#nlp = en_core_web_sm.load()\n",
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "74eb8f24",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_show['token_lemma'] = df_show['Dialog'].apply(cleaner)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "5b4776c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Character_Fix</th>\n",
       "      <th>Season</th>\n",
       "      <th>Episode</th>\n",
       "      <th>Dialog</th>\n",
       "      <th>token_lemma</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>HOPPER</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>Looks like somebody’s home. As the truck parks...</td>\n",
       "      <td>look like somebody home as the truck park we r...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>HOPPER</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>Where’s that coming from? Joyce pauses at the ...</td>\n",
       "      <td>where that come from joyce pause at the foot o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>HOPPER</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>Hey, dipshits! The men drop their tools and st...</td>\n",
       "      <td>hey dipshit the man drop their tool and stand ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Character_Fix  Season  Episode  \\\n",
       "5        HOPPER       3        5   \n",
       "6        HOPPER       3        5   \n",
       "7        HOPPER       3        5   \n",
       "\n",
       "                                              Dialog  \\\n",
       "5  Looks like somebody’s home. As the truck parks...   \n",
       "6  Where’s that coming from? Joyce pauses at the ...   \n",
       "7  Hey, dipshits! The men drop their tools and st...   \n",
       "\n",
       "                                         token_lemma  \n",
       "5  look like somebody home as the truck park we r...  \n",
       "6  where that come from joyce pause at the foot o...  \n",
       "7  hey dipshit the man drop their tool and stand ...  "
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_show.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12c62d7a",
   "metadata": {},
   "source": [
    "Supervised learning classification techniques:   \n",
    "Logistic Regression: to understand the strength of relationships by leveraging signals to approximate the most appropriate classification. This will be our baseline for classification.    \n",
    "Random Forest Classifier: to perform classification tasks on noisy and/or highly dimensional data by making use of an ensemble of decision trees for classification.     \n",
    "LinearSVM based regression: generalization capability of the words/lines with high prediction accuracy for character name, robust to outliers. Will need to transform all lines of text into vectors in order to encode and leverage the power of SVM.     \n",
    "\n",
    "Evaluation Metrics:\n",
    "F1 Score : mean of precision and recall scores    \n",
    "Precision : explains how many of the correctly predicted cases actually turned out to be positive. Precision is useful where False Positives are a higher concern than False Negatives.     \n",
    "AUC (Area Under the Curve): to measure how well the classifier was able to distinguish different classes, if our dataset proves to be imbalanced which might be the case since characters do speak different volumes of lines etc.    \n",
    "K-Fold cross validation accuracy: to measure and test if our various models' accuracy would be better, worse or the same if we had used a different section of the data set as a validation set. Evaluation is performed using different shuffling and chunking of the dataset through various iterations. This will be great because we can see the various scoring/evaluation metrics above at once (such as accuracy, precision, recall and f1)    \n",
    "Visualizations:\n",
    "Confusion matrix: in order to compare actual label versus predicted label (TP, TN, FP, FN) and identify proper (best) evaluation metrics/scoring to use.  \n",
    "Histograms: to show frequency of characters predicted (by probability with use of bins)\n",
    "We will try to inspect model performance via visualizations between different characters (classes).   \n",
    "Line charts for visualizing AUC-ROC and other results/metrics.    \n",
    "All visualizations will keep the Stranger Things aesthetic as well. Will look into building a visualization which is composed of characters face/faces with model output probabilities showing the confidence prediction. Or sentiment of characters lines spoken, we are sure there are characters which have more negative sentiment overall than others. We will scope out over the course of project work.    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d7eb007",
   "metadata": {},
   "source": [
    "### Random Forest Classifier "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "b81f6a45",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn import model_selection\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "cb32dca7",
   "metadata": {},
   "outputs": [],
   "source": [
    "RANDOM_SEED = 655"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "adfca401",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Character_Fix</th>\n",
       "      <th>Season</th>\n",
       "      <th>Episode</th>\n",
       "      <th>Dialog</th>\n",
       "      <th>token_lemma</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>HOPPER</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>Looks like somebody’s home. As the truck parks...</td>\n",
       "      <td>look like somebody home as the truck park we r...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>HOPPER</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>Where’s that coming from? Joyce pauses at the ...</td>\n",
       "      <td>where that come from joyce pause at the foot o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>HOPPER</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>Hey, dipshits! The men drop their tools and st...</td>\n",
       "      <td>hey dipshit the man drop their tool and stand ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Character_Fix  Season  Episode  \\\n",
       "5        HOPPER       3        5   \n",
       "6        HOPPER       3        5   \n",
       "7        HOPPER       3        5   \n",
       "\n",
       "                                              Dialog  \\\n",
       "5  Looks like somebody’s home. As the truck parks...   \n",
       "6  Where’s that coming from? Joyce pauses at the ...   \n",
       "7  Hey, dipshits! The men drop their tools and st...   \n",
       "\n",
       "                                         token_lemma  \n",
       "5  look like somebody home as the truck park we r...  \n",
       "6  where that come from joyce pause at the foot o...  \n",
       "7  hey dipshit the man drop their tool and stand ...  "
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_show.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "e2ba21ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_show['length'] = df_show.token_lemma.apply(lambda x: len(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "84661590",
   "metadata": {},
   "outputs": [],
   "source": [
    "check = df_show.groupby(['length']).min().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "d998ae69",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>length</th>\n",
       "      <th>Character_Fix</th>\n",
       "      <th>Season</th>\n",
       "      <th>Episode</th>\n",
       "      <th>Dialog</th>\n",
       "      <th>token_lemma</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>STEVE</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>2/8</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>BILLY</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-- Hi...</td>\n",
       "      <td>go</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>DEPUTY CALLAHAN</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>... God --</td>\n",
       "      <td>and</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>ARGYLE</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Boop.</td>\n",
       "      <td>boop</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>BILLY</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>, LUCAS</td>\n",
       "      <td>a man</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   length    Character_Fix  Season  Episode       Dialog token_lemma\n",
       "0       0            STEVE       3        5          2/8            \n",
       "1       2            BILLY       1        1   -- Hi...            go\n",
       "2       3  DEPUTY CALLAHAN       1        1  ... God --          and\n",
       "3       4           ARGYLE       1        1        Boop.        boop\n",
       "4       5            BILLY       1        1     , LUCAS        a man"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "check.iloc[0:5,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "d8611322",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_show2= df_show.explode(column='token_lemma').dropna(subset=['token_lemma'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "ef3eb141",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Character_Fix</th>\n",
       "      <th>Season</th>\n",
       "      <th>Episode</th>\n",
       "      <th>Dialog</th>\n",
       "      <th>token_lemma</th>\n",
       "      <th>length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>HOPPER</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>Looks like somebody’s home. As the truck parks...</td>\n",
       "      <td>look like somebody home as the truck park we r...</td>\n",
       "      <td>303</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>HOPPER</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>Where’s that coming from? Joyce pauses at the ...</td>\n",
       "      <td>where that come from joyce pause at the foot o...</td>\n",
       "      <td>150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>HOPPER</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>Hey, dipshits! The men drop their tools and st...</td>\n",
       "      <td>hey dipshit the man drop their tool and stand ...</td>\n",
       "      <td>72</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Character_Fix  Season  Episode  \\\n",
       "5        HOPPER       3        5   \n",
       "6        HOPPER       3        5   \n",
       "7        HOPPER       3        5   \n",
       "\n",
       "                                              Dialog  \\\n",
       "5  Looks like somebody’s home. As the truck parks...   \n",
       "6  Where’s that coming from? Joyce pauses at the ...   \n",
       "7  Hey, dipshits! The men drop their tools and st...   \n",
       "\n",
       "                                         token_lemma  length  \n",
       "5  look like somebody home as the truck park we r...     303  \n",
       "6  where that come from joyce pause at the foot o...     150  \n",
       "7  hey dipshit the man drop their tool and stand ...      72  "
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_show2.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "dec12721",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4360 1091\n"
     ]
    }
   ],
   "source": [
    "train_df, test_df = \\\n",
    "              np.split(df_show.sample(frac=1, random_state=RANDOM_SEED), \n",
    "                       [int(.8*len(df_show))])\n",
    "print(len(train_df), len(test_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "a96aa2f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(min_df = 20, stop_words = 'english')\n",
    "X_train = vectorizer.fit_transform(train_df.token_lemma)\n",
    "X_test = vectorizer.transform(test_df.token_lemma)\n",
    "\n",
    "y_train=train_df['Character_Fix'].values\n",
    "y_test=test_df['Character_Fix'].values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "bd08225a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate Random forest classifier \n",
    "clf=RandomForestClassifier(n_estimators=100, random_state = RANDOM_SEED)\n",
    "\n",
    "#Train the model using the training sets y_pred=clf.predict(X_test)\n",
    "clf.fit(X_train,y_train)\n",
    "y_pred=clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "db321390",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.1998166819431714\n"
     ]
    }
   ],
   "source": [
    "#Import scikit-learn metrics module for accuracy calculation\n",
    "from sklearn import metrics\n",
    "# Model Accuracy, how often is the classifier correct?\n",
    "print(\"Accuracy:\",metrics.accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa03a409",
   "metadata": {},
   "source": [
    "### Terrible score now we hyperparameter tune!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "thirty-technical",
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid_ = {'n_estimators': [500, 800, 1500, 2500, 5000],\n",
    "               'max_features': ['auto', 'sqrt', 'log2'],\n",
    "               'max_depth':[10, 20, 30, 40, 50],\n",
    "               'min_samples_split': [2, 5, 10, 15, 20],\n",
    "               'min_samples_leaf': [1, 2, 5, 10, 15],\n",
    "               'criterion':[\"gini\", \"entropy\", \"log_loss\"]\n",
    "              }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "infectious-malaysia",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 100 candidates, totalling 500 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/site-packages/sklearn/model_selection/_split.py:676: UserWarning:\n",
      "\n",
      "The least populated class in y has only 1 members, which is less than n_splits=5.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END criterion=entropy, max_depth=30, max_features=sqrt, min_samples_leaf=15, min_samples_split=15, n_estimators=1500; total time=   6.6s\n",
      "[CV] END criterion=entropy, max_depth=30, max_features=sqrt, min_samples_leaf=15, min_samples_split=15, n_estimators=1500; total time=   9.6s\n",
      "[CV] END criterion=entropy, max_depth=30, max_features=sqrt, min_samples_leaf=15, min_samples_split=15, n_estimators=1500; total time=   7.4s\n",
      "[CV] END criterion=entropy, max_depth=30, max_features=sqrt, min_samples_leaf=15, min_samples_split=15, n_estimators=1500; total time=   9.5s\n",
      "[CV] END criterion=entropy, max_depth=30, max_features=sqrt, min_samples_leaf=15, min_samples_split=15, n_estimators=1500; total time=   9.4s\n",
      "[CV] END criterion=entropy, max_depth=50, max_features=sqrt, min_samples_leaf=2, min_samples_split=10, n_estimators=1500; total time=  20.7s\n",
      "[CV] END criterion=entropy, max_depth=50, max_features=sqrt, min_samples_leaf=2, min_samples_split=10, n_estimators=1500; total time=  21.4s\n",
      "[CV] END criterion=entropy, max_depth=50, max_features=sqrt, min_samples_leaf=2, min_samples_split=10, n_estimators=1500; total time=  20.4s\n",
      "[CV] END criterion=entropy, max_depth=50, max_features=sqrt, min_samples_leaf=2, min_samples_split=10, n_estimators=1500; total time=  20.0s\n",
      "[CV] END criterion=entropy, max_depth=50, max_features=sqrt, min_samples_leaf=2, min_samples_split=10, n_estimators=1500; total time=  18.9s\n",
      "[CV] END criterion=gini, max_depth=50, max_features=sqrt, min_samples_leaf=10, min_samples_split=5, n_estimators=5000; total time=  33.5s\n",
      "[CV] END criterion=gini, max_depth=50, max_features=sqrt, min_samples_leaf=10, min_samples_split=5, n_estimators=5000; total time=  33.8s\n",
      "[CV] END criterion=gini, max_depth=50, max_features=sqrt, min_samples_leaf=10, min_samples_split=5, n_estimators=5000; total time=  27.7s\n",
      "[CV] END criterion=gini, max_depth=50, max_features=sqrt, min_samples_leaf=10, min_samples_split=5, n_estimators=5000; total time=  29.9s\n",
      "[CV] END criterion=gini, max_depth=50, max_features=sqrt, min_samples_leaf=10, min_samples_split=5, n_estimators=5000; total time=  28.8s\n",
      "[CV] END criterion=gini, max_depth=50, max_features=log2, min_samples_leaf=1, min_samples_split=2, n_estimators=1500; total time=  16.4s\n",
      "[CV] END criterion=gini, max_depth=50, max_features=log2, min_samples_leaf=1, min_samples_split=2, n_estimators=1500; total time=  16.2s\n",
      "[CV] END criterion=gini, max_depth=50, max_features=log2, min_samples_leaf=1, min_samples_split=2, n_estimators=1500; total time=  15.9s\n",
      "[CV] END criterion=gini, max_depth=50, max_features=log2, min_samples_leaf=1, min_samples_split=2, n_estimators=1500; total time=  23.6s\n",
      "[CV] END criterion=gini, max_depth=50, max_features=log2, min_samples_leaf=1, min_samples_split=2, n_estimators=1500; total time=  19.6s\n",
      "[CV] END criterion=log_loss, max_depth=20, max_features=auto, min_samples_leaf=10, min_samples_split=5, n_estimators=5000; total time=   2.5s\n",
      "[CV] END criterion=log_loss, max_depth=20, max_features=auto, min_samples_leaf=10, min_samples_split=5, n_estimators=5000; total time=   2.2s\n",
      "[CV] END criterion=log_loss, max_depth=20, max_features=auto, min_samples_leaf=10, min_samples_split=5, n_estimators=5000; total time=   2.7s\n",
      "[CV] END criterion=log_loss, max_depth=20, max_features=auto, min_samples_leaf=10, min_samples_split=5, n_estimators=5000; total time=   2.1s\n",
      "[CV] END criterion=log_loss, max_depth=20, max_features=auto, min_samples_leaf=10, min_samples_split=5, n_estimators=5000; total time=   1.9s\n",
      "[CV] END criterion=gini, max_depth=30, max_features=auto, min_samples_leaf=2, min_samples_split=10, n_estimators=500; total time=   2.9s\n",
      "[CV] END criterion=gini, max_depth=30, max_features=auto, min_samples_leaf=2, min_samples_split=10, n_estimators=500; total time=   3.3s\n",
      "[CV] END criterion=gini, max_depth=30, max_features=auto, min_samples_leaf=2, min_samples_split=10, n_estimators=500; total time=   4.4s\n",
      "[CV] END criterion=gini, max_depth=30, max_features=auto, min_samples_leaf=2, min_samples_split=10, n_estimators=500; total time=   4.3s\n",
      "[CV] END criterion=gini, max_depth=30, max_features=auto, min_samples_leaf=2, min_samples_split=10, n_estimators=500; total time=   2.8s\n",
      "[CV] END criterion=log_loss, max_depth=10, max_features=auto, min_samples_leaf=15, min_samples_split=5, n_estimators=500; total time=   0.2s\n",
      "[CV] END criterion=log_loss, max_depth=10, max_features=auto, min_samples_leaf=15, min_samples_split=5, n_estimators=500; total time=   0.2s\n",
      "[CV] END criterion=log_loss, max_depth=10, max_features=auto, min_samples_leaf=15, min_samples_split=5, n_estimators=500; total time=   0.2s\n",
      "[CV] END criterion=log_loss, max_depth=10, max_features=auto, min_samples_leaf=15, min_samples_split=5, n_estimators=500; total time=   0.2s\n",
      "[CV] END criterion=log_loss, max_depth=10, max_features=auto, min_samples_leaf=15, min_samples_split=5, n_estimators=500; total time=   0.2s\n",
      "[CV] END criterion=entropy, max_depth=40, max_features=log2, min_samples_leaf=1, min_samples_split=15, n_estimators=2500; total time=  33.0s\n",
      "[CV] END criterion=entropy, max_depth=40, max_features=log2, min_samples_leaf=1, min_samples_split=15, n_estimators=2500; total time=  28.5s\n",
      "[CV] END criterion=entropy, max_depth=40, max_features=log2, min_samples_leaf=1, min_samples_split=15, n_estimators=2500; total time=  26.2s\n",
      "[CV] END criterion=entropy, max_depth=40, max_features=log2, min_samples_leaf=1, min_samples_split=15, n_estimators=2500; total time=  27.1s\n",
      "[CV] END criterion=entropy, max_depth=40, max_features=log2, min_samples_leaf=1, min_samples_split=15, n_estimators=2500; total time=  30.0s\n",
      "[CV] END criterion=log_loss, max_depth=10, max_features=log2, min_samples_leaf=15, min_samples_split=15, n_estimators=5000; total time=   2.7s\n",
      "[CV] END criterion=log_loss, max_depth=10, max_features=log2, min_samples_leaf=15, min_samples_split=15, n_estimators=5000; total time=   2.4s\n",
      "[CV] END criterion=log_loss, max_depth=10, max_features=log2, min_samples_leaf=15, min_samples_split=15, n_estimators=5000; total time=   2.5s\n",
      "[CV] END criterion=log_loss, max_depth=10, max_features=log2, min_samples_leaf=15, min_samples_split=15, n_estimators=5000; total time=   2.0s\n",
      "[CV] END criterion=log_loss, max_depth=10, max_features=log2, min_samples_leaf=15, min_samples_split=15, n_estimators=5000; total time=   1.9s\n",
      "[CV] END criterion=gini, max_depth=30, max_features=auto, min_samples_leaf=5, min_samples_split=2, n_estimators=2500; total time=  13.8s\n",
      "[CV] END criterion=gini, max_depth=30, max_features=auto, min_samples_leaf=5, min_samples_split=2, n_estimators=2500; total time=  14.2s\n",
      "[CV] END criterion=gini, max_depth=30, max_features=auto, min_samples_leaf=5, min_samples_split=2, n_estimators=2500; total time=  13.3s\n",
      "[CV] END criterion=gini, max_depth=30, max_features=auto, min_samples_leaf=5, min_samples_split=2, n_estimators=2500; total time=  15.7s\n",
      "[CV] END criterion=gini, max_depth=30, max_features=auto, min_samples_leaf=5, min_samples_split=2, n_estimators=2500; total time=  14.4s\n",
      "[CV] END criterion=gini, max_depth=10, max_features=auto, min_samples_leaf=5, min_samples_split=10, n_estimators=5000; total time=  22.5s\n",
      "[CV] END criterion=gini, max_depth=10, max_features=auto, min_samples_leaf=5, min_samples_split=10, n_estimators=5000; total time=  24.1s\n",
      "[CV] END criterion=gini, max_depth=10, max_features=auto, min_samples_leaf=5, min_samples_split=10, n_estimators=5000; total time=  20.1s\n",
      "[CV] END criterion=gini, max_depth=10, max_features=auto, min_samples_leaf=5, min_samples_split=10, n_estimators=5000; total time=  15.7s\n",
      "[CV] END criterion=gini, max_depth=10, max_features=auto, min_samples_leaf=5, min_samples_split=10, n_estimators=5000; total time=  15.1s\n",
      "[CV] END criterion=log_loss, max_depth=30, max_features=sqrt, min_samples_leaf=2, min_samples_split=15, n_estimators=2500; total time=   0.9s\n",
      "[CV] END criterion=log_loss, max_depth=30, max_features=sqrt, min_samples_leaf=2, min_samples_split=15, n_estimators=2500; total time=   0.9s\n",
      "[CV] END criterion=log_loss, max_depth=30, max_features=sqrt, min_samples_leaf=2, min_samples_split=15, n_estimators=2500; total time=   0.9s\n",
      "[CV] END criterion=log_loss, max_depth=30, max_features=sqrt, min_samples_leaf=2, min_samples_split=15, n_estimators=2500; total time=   0.9s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END criterion=log_loss, max_depth=30, max_features=sqrt, min_samples_leaf=2, min_samples_split=15, n_estimators=2500; total time=   0.9s\n",
      "[CV] END criterion=entropy, max_depth=10, max_features=sqrt, min_samples_leaf=2, min_samples_split=20, n_estimators=800; total time=   4.8s\n",
      "[CV] END criterion=entropy, max_depth=10, max_features=sqrt, min_samples_leaf=2, min_samples_split=20, n_estimators=800; total time=   4.9s\n",
      "[CV] END criterion=entropy, max_depth=10, max_features=sqrt, min_samples_leaf=2, min_samples_split=20, n_estimators=800; total time=   6.1s\n",
      "[CV] END criterion=entropy, max_depth=10, max_features=sqrt, min_samples_leaf=2, min_samples_split=20, n_estimators=800; total time=   6.3s\n",
      "[CV] END criterion=entropy, max_depth=10, max_features=sqrt, min_samples_leaf=2, min_samples_split=20, n_estimators=800; total time=   5.4s\n",
      "[CV] END criterion=gini, max_depth=40, max_features=auto, min_samples_leaf=15, min_samples_split=15, n_estimators=1500; total time=   9.2s\n",
      "[CV] END criterion=gini, max_depth=40, max_features=auto, min_samples_leaf=15, min_samples_split=15, n_estimators=1500; total time=   8.8s\n",
      "[CV] END criterion=gini, max_depth=40, max_features=auto, min_samples_leaf=15, min_samples_split=15, n_estimators=1500; total time=   7.0s\n",
      "[CV] END criterion=gini, max_depth=40, max_features=auto, min_samples_leaf=15, min_samples_split=15, n_estimators=1500; total time=   6.9s\n",
      "[CV] END criterion=gini, max_depth=40, max_features=auto, min_samples_leaf=15, min_samples_split=15, n_estimators=1500; total time=   6.3s\n",
      "[CV] END criterion=log_loss, max_depth=30, max_features=sqrt, min_samples_leaf=10, min_samples_split=20, n_estimators=500; total time=   0.2s\n",
      "[CV] END criterion=log_loss, max_depth=30, max_features=sqrt, min_samples_leaf=10, min_samples_split=20, n_estimators=500; total time=   0.2s\n",
      "[CV] END criterion=log_loss, max_depth=30, max_features=sqrt, min_samples_leaf=10, min_samples_split=20, n_estimators=500; total time=   0.2s\n",
      "[CV] END criterion=log_loss, max_depth=30, max_features=sqrt, min_samples_leaf=10, min_samples_split=20, n_estimators=500; total time=   0.2s\n",
      "[CV] END criterion=log_loss, max_depth=30, max_features=sqrt, min_samples_leaf=10, min_samples_split=20, n_estimators=500; total time=   0.2s\n",
      "[CV] END criterion=log_loss, max_depth=40, max_features=log2, min_samples_leaf=15, min_samples_split=2, n_estimators=5000; total time=   1.9s\n",
      "[CV] END criterion=log_loss, max_depth=40, max_features=log2, min_samples_leaf=15, min_samples_split=2, n_estimators=5000; total time=   2.0s\n",
      "[CV] END criterion=log_loss, max_depth=40, max_features=log2, min_samples_leaf=15, min_samples_split=2, n_estimators=5000; total time=   1.9s\n",
      "[CV] END criterion=log_loss, max_depth=40, max_features=log2, min_samples_leaf=15, min_samples_split=2, n_estimators=5000; total time=   1.9s\n",
      "[CV] END criterion=log_loss, max_depth=40, max_features=log2, min_samples_leaf=15, min_samples_split=2, n_estimators=5000; total time=   2.3s\n",
      "[CV] END criterion=entropy, max_depth=30, max_features=log2, min_samples_leaf=15, min_samples_split=5, n_estimators=5000; total time=  24.8s\n",
      "[CV] END criterion=entropy, max_depth=30, max_features=log2, min_samples_leaf=15, min_samples_split=5, n_estimators=5000; total time=  25.2s\n",
      "[CV] END criterion=entropy, max_depth=30, max_features=log2, min_samples_leaf=15, min_samples_split=5, n_estimators=5000; total time=  19.9s\n",
      "[CV] END criterion=entropy, max_depth=30, max_features=log2, min_samples_leaf=15, min_samples_split=5, n_estimators=5000; total time=  21.4s\n",
      "[CV] END criterion=entropy, max_depth=30, max_features=log2, min_samples_leaf=15, min_samples_split=5, n_estimators=5000; total time=  19.9s\n",
      "[CV] END criterion=gini, max_depth=20, max_features=auto, min_samples_leaf=10, min_samples_split=20, n_estimators=500; total time=   1.7s\n",
      "[CV] END criterion=gini, max_depth=20, max_features=auto, min_samples_leaf=10, min_samples_split=20, n_estimators=500; total time=   2.2s\n",
      "[CV] END criterion=gini, max_depth=20, max_features=auto, min_samples_leaf=10, min_samples_split=20, n_estimators=500; total time=   1.9s\n",
      "[CV] END criterion=gini, max_depth=20, max_features=auto, min_samples_leaf=10, min_samples_split=20, n_estimators=500; total time=   1.9s\n",
      "[CV] END criterion=gini, max_depth=20, max_features=auto, min_samples_leaf=10, min_samples_split=20, n_estimators=500; total time=   1.7s\n",
      "[CV] END criterion=entropy, max_depth=50, max_features=log2, min_samples_leaf=2, min_samples_split=10, n_estimators=800; total time=   5.5s\n",
      "[CV] END criterion=entropy, max_depth=50, max_features=log2, min_samples_leaf=2, min_samples_split=10, n_estimators=800; total time=   5.4s\n",
      "[CV] END criterion=entropy, max_depth=50, max_features=log2, min_samples_leaf=2, min_samples_split=10, n_estimators=800; total time=   5.3s\n",
      "[CV] END criterion=entropy, max_depth=50, max_features=log2, min_samples_leaf=2, min_samples_split=10, n_estimators=800; total time=   5.2s\n",
      "[CV] END criterion=entropy, max_depth=50, max_features=log2, min_samples_leaf=2, min_samples_split=10, n_estimators=800; total time=   5.7s\n",
      "[CV] END criterion=gini, max_depth=30, max_features=auto, min_samples_leaf=5, min_samples_split=2, n_estimators=500; total time=   2.5s\n",
      "[CV] END criterion=gini, max_depth=30, max_features=auto, min_samples_leaf=5, min_samples_split=2, n_estimators=500; total time=   2.4s\n",
      "[CV] END criterion=gini, max_depth=30, max_features=auto, min_samples_leaf=5, min_samples_split=2, n_estimators=500; total time=   2.6s\n",
      "[CV] END criterion=gini, max_depth=30, max_features=auto, min_samples_leaf=5, min_samples_split=2, n_estimators=500; total time=   2.0s\n",
      "[CV] END criterion=gini, max_depth=30, max_features=auto, min_samples_leaf=5, min_samples_split=2, n_estimators=500; total time=   2.0s\n",
      "[CV] END criterion=gini, max_depth=50, max_features=log2, min_samples_leaf=1, min_samples_split=2, n_estimators=500; total time=   3.9s\n",
      "[CV] END criterion=gini, max_depth=50, max_features=log2, min_samples_leaf=1, min_samples_split=2, n_estimators=500; total time=   4.1s\n",
      "[CV] END criterion=gini, max_depth=50, max_features=log2, min_samples_leaf=1, min_samples_split=2, n_estimators=500; total time=   4.3s\n",
      "[CV] END criterion=gini, max_depth=50, max_features=log2, min_samples_leaf=1, min_samples_split=2, n_estimators=500; total time=   4.2s\n",
      "[CV] END criterion=gini, max_depth=50, max_features=log2, min_samples_leaf=1, min_samples_split=2, n_estimators=500; total time=   4.0s\n",
      "[CV] END criterion=log_loss, max_depth=30, max_features=sqrt, min_samples_leaf=5, min_samples_split=10, n_estimators=1500; total time=   0.4s\n",
      "[CV] END criterion=log_loss, max_depth=30, max_features=sqrt, min_samples_leaf=5, min_samples_split=10, n_estimators=1500; total time=   0.5s\n",
      "[CV] END criterion=log_loss, max_depth=30, max_features=sqrt, min_samples_leaf=5, min_samples_split=10, n_estimators=1500; total time=   0.6s\n",
      "[CV] END criterion=log_loss, max_depth=30, max_features=sqrt, min_samples_leaf=5, min_samples_split=10, n_estimators=1500; total time=   0.6s\n",
      "[CV] END criterion=log_loss, max_depth=30, max_features=sqrt, min_samples_leaf=5, min_samples_split=10, n_estimators=1500; total time=   0.5s\n",
      "[CV] END criterion=gini, max_depth=10, max_features=auto, min_samples_leaf=10, min_samples_split=5, n_estimators=500; total time=   1.4s\n",
      "[CV] END criterion=gini, max_depth=10, max_features=auto, min_samples_leaf=10, min_samples_split=5, n_estimators=500; total time=   1.4s\n",
      "[CV] END criterion=gini, max_depth=10, max_features=auto, min_samples_leaf=10, min_samples_split=5, n_estimators=500; total time=   1.4s\n",
      "[CV] END criterion=gini, max_depth=10, max_features=auto, min_samples_leaf=10, min_samples_split=5, n_estimators=500; total time=   1.4s\n",
      "[CV] END criterion=gini, max_depth=10, max_features=auto, min_samples_leaf=10, min_samples_split=5, n_estimators=500; total time=   1.3s\n",
      "[CV] END criterion=log_loss, max_depth=50, max_features=auto, min_samples_leaf=15, min_samples_split=10, n_estimators=800; total time=   0.2s\n",
      "[CV] END criterion=log_loss, max_depth=50, max_features=auto, min_samples_leaf=15, min_samples_split=10, n_estimators=800; total time=   0.2s\n",
      "[CV] END criterion=log_loss, max_depth=50, max_features=auto, min_samples_leaf=15, min_samples_split=10, n_estimators=800; total time=   0.3s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END criterion=log_loss, max_depth=50, max_features=auto, min_samples_leaf=15, min_samples_split=10, n_estimators=800; total time=   0.3s\n",
      "[CV] END criterion=log_loss, max_depth=50, max_features=auto, min_samples_leaf=15, min_samples_split=10, n_estimators=800; total time=   0.3s\n",
      "[CV] END criterion=gini, max_depth=40, max_features=auto, min_samples_leaf=10, min_samples_split=20, n_estimators=1500; total time=   6.3s\n",
      "[CV] END criterion=gini, max_depth=40, max_features=auto, min_samples_leaf=10, min_samples_split=20, n_estimators=1500; total time=   6.1s\n",
      "[CV] END criterion=gini, max_depth=40, max_features=auto, min_samples_leaf=10, min_samples_split=20, n_estimators=1500; total time=   5.9s\n",
      "[CV] END criterion=gini, max_depth=40, max_features=auto, min_samples_leaf=10, min_samples_split=20, n_estimators=1500; total time=   5.6s\n",
      "[CV] END criterion=gini, max_depth=40, max_features=auto, min_samples_leaf=10, min_samples_split=20, n_estimators=1500; total time=   6.6s\n",
      "[CV] END criterion=gini, max_depth=40, max_features=sqrt, min_samples_leaf=1, min_samples_split=20, n_estimators=5000; total time=  31.2s\n",
      "[CV] END criterion=gini, max_depth=40, max_features=sqrt, min_samples_leaf=1, min_samples_split=20, n_estimators=5000; total time=  31.6s\n",
      "[CV] END criterion=gini, max_depth=40, max_features=sqrt, min_samples_leaf=1, min_samples_split=20, n_estimators=5000; total time=  32.2s\n",
      "[CV] END criterion=gini, max_depth=40, max_features=sqrt, min_samples_leaf=1, min_samples_split=20, n_estimators=5000; total time=  31.7s\n",
      "[CV] END criterion=gini, max_depth=40, max_features=sqrt, min_samples_leaf=1, min_samples_split=20, n_estimators=5000; total time=  31.2s\n",
      "[CV] END criterion=entropy, max_depth=30, max_features=sqrt, min_samples_leaf=5, min_samples_split=2, n_estimators=500; total time=   3.6s\n",
      "[CV] END criterion=entropy, max_depth=30, max_features=sqrt, min_samples_leaf=5, min_samples_split=2, n_estimators=500; total time=   3.8s\n",
      "[CV] END criterion=entropy, max_depth=30, max_features=sqrt, min_samples_leaf=5, min_samples_split=2, n_estimators=500; total time=   3.8s\n",
      "[CV] END criterion=entropy, max_depth=30, max_features=sqrt, min_samples_leaf=5, min_samples_split=2, n_estimators=500; total time=   5.8s\n",
      "[CV] END criterion=entropy, max_depth=30, max_features=sqrt, min_samples_leaf=5, min_samples_split=2, n_estimators=500; total time=   4.0s\n",
      "[CV] END criterion=entropy, max_depth=10, max_features=auto, min_samples_leaf=10, min_samples_split=10, n_estimators=1500; total time=   6.6s\n",
      "[CV] END criterion=entropy, max_depth=10, max_features=auto, min_samples_leaf=10, min_samples_split=10, n_estimators=1500; total time=   5.9s\n",
      "[CV] END criterion=entropy, max_depth=10, max_features=auto, min_samples_leaf=10, min_samples_split=10, n_estimators=1500; total time=   6.4s\n",
      "[CV] END criterion=entropy, max_depth=10, max_features=auto, min_samples_leaf=10, min_samples_split=10, n_estimators=1500; total time=   6.5s\n",
      "[CV] END criterion=entropy, max_depth=10, max_features=auto, min_samples_leaf=10, min_samples_split=10, n_estimators=1500; total time=   6.4s\n",
      "[CV] END criterion=gini, max_depth=30, max_features=log2, min_samples_leaf=2, min_samples_split=20, n_estimators=800; total time=   3.0s\n",
      "[CV] END criterion=gini, max_depth=30, max_features=log2, min_samples_leaf=2, min_samples_split=20, n_estimators=800; total time=   2.8s\n",
      "[CV] END criterion=gini, max_depth=30, max_features=log2, min_samples_leaf=2, min_samples_split=20, n_estimators=800; total time=   2.9s\n",
      "[CV] END criterion=gini, max_depth=30, max_features=log2, min_samples_leaf=2, min_samples_split=20, n_estimators=800; total time=   2.9s\n",
      "[CV] END criterion=gini, max_depth=30, max_features=log2, min_samples_leaf=2, min_samples_split=20, n_estimators=800; total time=   2.9s\n",
      "[CV] END criterion=entropy, max_depth=10, max_features=sqrt, min_samples_leaf=5, min_samples_split=10, n_estimators=1500; total time=   7.6s\n",
      "[CV] END criterion=entropy, max_depth=10, max_features=sqrt, min_samples_leaf=5, min_samples_split=10, n_estimators=1500; total time=   7.3s\n",
      "[CV] END criterion=entropy, max_depth=10, max_features=sqrt, min_samples_leaf=5, min_samples_split=10, n_estimators=1500; total time=   7.5s\n",
      "[CV] END criterion=entropy, max_depth=10, max_features=sqrt, min_samples_leaf=5, min_samples_split=10, n_estimators=1500; total time=   7.6s\n",
      "[CV] END criterion=entropy, max_depth=10, max_features=sqrt, min_samples_leaf=5, min_samples_split=10, n_estimators=1500; total time=   7.3s\n",
      "[CV] END criterion=log_loss, max_depth=10, max_features=log2, min_samples_leaf=10, min_samples_split=2, n_estimators=5000; total time=   1.6s\n",
      "[CV] END criterion=log_loss, max_depth=10, max_features=log2, min_samples_leaf=10, min_samples_split=2, n_estimators=5000; total time=   1.7s\n",
      "[CV] END criterion=log_loss, max_depth=10, max_features=log2, min_samples_leaf=10, min_samples_split=2, n_estimators=5000; total time=   1.7s\n",
      "[CV] END criterion=log_loss, max_depth=10, max_features=log2, min_samples_leaf=10, min_samples_split=2, n_estimators=5000; total time=   1.8s\n",
      "[CV] END criterion=log_loss, max_depth=10, max_features=log2, min_samples_leaf=10, min_samples_split=2, n_estimators=5000; total time=   2.0s\n",
      "[CV] END criterion=log_loss, max_depth=40, max_features=sqrt, min_samples_leaf=5, min_samples_split=20, n_estimators=5000; total time=   1.7s\n",
      "[CV] END criterion=log_loss, max_depth=40, max_features=sqrt, min_samples_leaf=5, min_samples_split=20, n_estimators=5000; total time=   2.0s\n",
      "[CV] END criterion=log_loss, max_depth=40, max_features=sqrt, min_samples_leaf=5, min_samples_split=20, n_estimators=5000; total time=   1.5s\n",
      "[CV] END criterion=log_loss, max_depth=40, max_features=sqrt, min_samples_leaf=5, min_samples_split=20, n_estimators=5000; total time=   1.5s\n",
      "[CV] END criterion=log_loss, max_depth=40, max_features=sqrt, min_samples_leaf=5, min_samples_split=20, n_estimators=5000; total time=   1.5s\n",
      "[CV] END criterion=gini, max_depth=10, max_features=auto, min_samples_leaf=15, min_samples_split=15, n_estimators=500; total time=   1.4s\n",
      "[CV] END criterion=gini, max_depth=10, max_features=auto, min_samples_leaf=15, min_samples_split=15, n_estimators=500; total time=   1.3s\n",
      "[CV] END criterion=gini, max_depth=10, max_features=auto, min_samples_leaf=15, min_samples_split=15, n_estimators=500; total time=   1.3s\n",
      "[CV] END criterion=gini, max_depth=10, max_features=auto, min_samples_leaf=15, min_samples_split=15, n_estimators=500; total time=   1.3s\n",
      "[CV] END criterion=gini, max_depth=10, max_features=auto, min_samples_leaf=15, min_samples_split=15, n_estimators=500; total time=   1.3s\n",
      "[CV] END criterion=log_loss, max_depth=20, max_features=auto, min_samples_leaf=10, min_samples_split=15, n_estimators=5000; total time=   1.7s\n",
      "[CV] END criterion=log_loss, max_depth=20, max_features=auto, min_samples_leaf=10, min_samples_split=15, n_estimators=5000; total time=   1.6s\n",
      "[CV] END criterion=log_loss, max_depth=20, max_features=auto, min_samples_leaf=10, min_samples_split=15, n_estimators=5000; total time=   1.8s\n",
      "[CV] END criterion=log_loss, max_depth=20, max_features=auto, min_samples_leaf=10, min_samples_split=15, n_estimators=5000; total time=   1.7s\n",
      "[CV] END criterion=log_loss, max_depth=20, max_features=auto, min_samples_leaf=10, min_samples_split=15, n_estimators=5000; total time=   1.6s\n",
      "[CV] END criterion=entropy, max_depth=30, max_features=log2, min_samples_leaf=1, min_samples_split=2, n_estimators=800; total time=   8.1s\n",
      "[CV] END criterion=entropy, max_depth=30, max_features=log2, min_samples_leaf=1, min_samples_split=2, n_estimators=800; total time=   7.8s\n",
      "[CV] END criterion=entropy, max_depth=30, max_features=log2, min_samples_leaf=1, min_samples_split=2, n_estimators=800; total time=   7.9s\n",
      "[CV] END criterion=entropy, max_depth=30, max_features=log2, min_samples_leaf=1, min_samples_split=2, n_estimators=800; total time=  10.5s\n",
      "[CV] END criterion=entropy, max_depth=30, max_features=log2, min_samples_leaf=1, min_samples_split=2, n_estimators=800; total time=   8.4s\n",
      "[CV] END criterion=log_loss, max_depth=30, max_features=log2, min_samples_leaf=5, min_samples_split=2, n_estimators=500; total time=   0.2s\n",
      "[CV] END criterion=log_loss, max_depth=30, max_features=log2, min_samples_leaf=5, min_samples_split=2, n_estimators=500; total time=   0.2s\n",
      "[CV] END criterion=log_loss, max_depth=30, max_features=log2, min_samples_leaf=5, min_samples_split=2, n_estimators=500; total time=   0.2s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END criterion=log_loss, max_depth=30, max_features=log2, min_samples_leaf=5, min_samples_split=2, n_estimators=500; total time=   0.2s\n",
      "[CV] END criterion=log_loss, max_depth=30, max_features=log2, min_samples_leaf=5, min_samples_split=2, n_estimators=500; total time=   0.2s\n",
      "[CV] END criterion=entropy, max_depth=20, max_features=auto, min_samples_leaf=15, min_samples_split=10, n_estimators=1500; total time=   7.1s\n",
      "[CV] END criterion=entropy, max_depth=20, max_features=auto, min_samples_leaf=15, min_samples_split=10, n_estimators=1500; total time=   6.8s\n",
      "[CV] END criterion=entropy, max_depth=20, max_features=auto, min_samples_leaf=15, min_samples_split=10, n_estimators=1500; total time=   7.4s\n",
      "[CV] END criterion=entropy, max_depth=20, max_features=auto, min_samples_leaf=15, min_samples_split=10, n_estimators=1500; total time=   7.7s\n",
      "[CV] END criterion=entropy, max_depth=20, max_features=auto, min_samples_leaf=15, min_samples_split=10, n_estimators=1500; total time=   7.3s\n",
      "[CV] END criterion=entropy, max_depth=50, max_features=log2, min_samples_leaf=10, min_samples_split=15, n_estimators=800; total time=   3.6s\n",
      "[CV] END criterion=entropy, max_depth=50, max_features=log2, min_samples_leaf=10, min_samples_split=15, n_estimators=800; total time=   3.7s\n",
      "[CV] END criterion=entropy, max_depth=50, max_features=log2, min_samples_leaf=10, min_samples_split=15, n_estimators=800; total time=   3.6s\n",
      "[CV] END criterion=entropy, max_depth=50, max_features=log2, min_samples_leaf=10, min_samples_split=15, n_estimators=800; total time=   3.8s\n",
      "[CV] END criterion=entropy, max_depth=50, max_features=log2, min_samples_leaf=10, min_samples_split=15, n_estimators=800; total time=   3.7s\n",
      "[CV] END criterion=gini, max_depth=10, max_features=log2, min_samples_leaf=5, min_samples_split=5, n_estimators=1500; total time=   3.8s\n",
      "[CV] END criterion=gini, max_depth=10, max_features=log2, min_samples_leaf=5, min_samples_split=5, n_estimators=1500; total time=   4.0s\n",
      "[CV] END criterion=gini, max_depth=10, max_features=log2, min_samples_leaf=5, min_samples_split=5, n_estimators=1500; total time=   3.5s\n",
      "[CV] END criterion=gini, max_depth=10, max_features=log2, min_samples_leaf=5, min_samples_split=5, n_estimators=1500; total time=   3.7s\n",
      "[CV] END criterion=gini, max_depth=10, max_features=log2, min_samples_leaf=5, min_samples_split=5, n_estimators=1500; total time=   3.6s\n",
      "[CV] END criterion=log_loss, max_depth=50, max_features=log2, min_samples_leaf=10, min_samples_split=15, n_estimators=5000; total time=   1.6s\n",
      "[CV] END criterion=log_loss, max_depth=50, max_features=log2, min_samples_leaf=10, min_samples_split=15, n_estimators=5000; total time=   1.7s\n",
      "[CV] END criterion=log_loss, max_depth=50, max_features=log2, min_samples_leaf=10, min_samples_split=15, n_estimators=5000; total time=   1.6s\n",
      "[CV] END criterion=log_loss, max_depth=50, max_features=log2, min_samples_leaf=10, min_samples_split=15, n_estimators=5000; total time=   1.7s\n",
      "[CV] END criterion=log_loss, max_depth=50, max_features=log2, min_samples_leaf=10, min_samples_split=15, n_estimators=5000; total time=   1.6s\n",
      "[CV] END criterion=log_loss, max_depth=40, max_features=auto, min_samples_leaf=15, min_samples_split=10, n_estimators=1500; total time=   0.5s\n",
      "[CV] END criterion=log_loss, max_depth=40, max_features=auto, min_samples_leaf=15, min_samples_split=10, n_estimators=1500; total time=   0.5s\n",
      "[CV] END criterion=log_loss, max_depth=40, max_features=auto, min_samples_leaf=15, min_samples_split=10, n_estimators=1500; total time=   0.5s\n",
      "[CV] END criterion=log_loss, max_depth=40, max_features=auto, min_samples_leaf=15, min_samples_split=10, n_estimators=1500; total time=   0.5s\n",
      "[CV] END criterion=log_loss, max_depth=40, max_features=auto, min_samples_leaf=15, min_samples_split=10, n_estimators=1500; total time=   0.5s\n",
      "[CV] END criterion=gini, max_depth=40, max_features=log2, min_samples_leaf=10, min_samples_split=15, n_estimators=500; total time=   1.7s\n",
      "[CV] END criterion=gini, max_depth=40, max_features=log2, min_samples_leaf=10, min_samples_split=15, n_estimators=500; total time=   1.5s\n",
      "[CV] END criterion=gini, max_depth=40, max_features=log2, min_samples_leaf=10, min_samples_split=15, n_estimators=500; total time=   1.7s\n",
      "[CV] END criterion=gini, max_depth=40, max_features=log2, min_samples_leaf=10, min_samples_split=15, n_estimators=500; total time=   1.7s\n",
      "[CV] END criterion=gini, max_depth=40, max_features=log2, min_samples_leaf=10, min_samples_split=15, n_estimators=500; total time=   1.7s\n",
      "[CV] END criterion=entropy, max_depth=10, max_features=auto, min_samples_leaf=2, min_samples_split=15, n_estimators=5000; total time=  28.1s\n",
      "[CV] END criterion=entropy, max_depth=10, max_features=auto, min_samples_leaf=2, min_samples_split=15, n_estimators=5000; total time=  27.6s\n",
      "[CV] END criterion=entropy, max_depth=10, max_features=auto, min_samples_leaf=2, min_samples_split=15, n_estimators=5000; total time=  26.7s\n",
      "[CV] END criterion=entropy, max_depth=10, max_features=auto, min_samples_leaf=2, min_samples_split=15, n_estimators=5000; total time=  28.6s\n",
      "[CV] END criterion=entropy, max_depth=10, max_features=auto, min_samples_leaf=2, min_samples_split=15, n_estimators=5000; total time=  27.6s\n",
      "[CV] END criterion=log_loss, max_depth=30, max_features=sqrt, min_samples_leaf=10, min_samples_split=10, n_estimators=5000; total time=   1.5s\n",
      "[CV] END criterion=log_loss, max_depth=30, max_features=sqrt, min_samples_leaf=10, min_samples_split=10, n_estimators=5000; total time=   1.4s\n",
      "[CV] END criterion=log_loss, max_depth=30, max_features=sqrt, min_samples_leaf=10, min_samples_split=10, n_estimators=5000; total time=   1.7s\n",
      "[CV] END criterion=log_loss, max_depth=30, max_features=sqrt, min_samples_leaf=10, min_samples_split=10, n_estimators=5000; total time=   1.8s\n",
      "[CV] END criterion=log_loss, max_depth=30, max_features=sqrt, min_samples_leaf=10, min_samples_split=10, n_estimators=5000; total time=   1.7s\n",
      "[CV] END criterion=gini, max_depth=20, max_features=log2, min_samples_leaf=15, min_samples_split=15, n_estimators=5000; total time=  13.0s\n",
      "[CV] END criterion=gini, max_depth=20, max_features=log2, min_samples_leaf=15, min_samples_split=15, n_estimators=5000; total time=  12.6s\n",
      "[CV] END criterion=gini, max_depth=20, max_features=log2, min_samples_leaf=15, min_samples_split=15, n_estimators=5000; total time=  12.7s\n",
      "[CV] END criterion=gini, max_depth=20, max_features=log2, min_samples_leaf=15, min_samples_split=15, n_estimators=5000; total time=  13.0s\n",
      "[CV] END criterion=gini, max_depth=20, max_features=log2, min_samples_leaf=15, min_samples_split=15, n_estimators=5000; total time=  13.6s\n",
      "[CV] END criterion=entropy, max_depth=50, max_features=auto, min_samples_leaf=1, min_samples_split=10, n_estimators=500; total time=   7.3s\n",
      "[CV] END criterion=entropy, max_depth=50, max_features=auto, min_samples_leaf=1, min_samples_split=10, n_estimators=500; total time=   7.2s\n",
      "[CV] END criterion=entropy, max_depth=50, max_features=auto, min_samples_leaf=1, min_samples_split=10, n_estimators=500; total time=   7.4s\n",
      "[CV] END criterion=entropy, max_depth=50, max_features=auto, min_samples_leaf=1, min_samples_split=10, n_estimators=500; total time=   7.2s\n",
      "[CV] END criterion=entropy, max_depth=50, max_features=auto, min_samples_leaf=1, min_samples_split=10, n_estimators=500; total time=   6.8s\n",
      "[CV] END criterion=entropy, max_depth=50, max_features=log2, min_samples_leaf=5, min_samples_split=15, n_estimators=800; total time=   4.5s\n",
      "[CV] END criterion=entropy, max_depth=50, max_features=log2, min_samples_leaf=5, min_samples_split=15, n_estimators=800; total time=   4.5s\n",
      "[CV] END criterion=entropy, max_depth=50, max_features=log2, min_samples_leaf=5, min_samples_split=15, n_estimators=800; total time=   4.4s\n",
      "[CV] END criterion=entropy, max_depth=50, max_features=log2, min_samples_leaf=5, min_samples_split=15, n_estimators=800; total time=   4.2s\n",
      "[CV] END criterion=entropy, max_depth=50, max_features=log2, min_samples_leaf=5, min_samples_split=15, n_estimators=800; total time=   4.6s\n",
      "[CV] END criterion=log_loss, max_depth=30, max_features=sqrt, min_samples_leaf=10, min_samples_split=5, n_estimators=800; total time=   0.3s\n",
      "[CV] END criterion=log_loss, max_depth=30, max_features=sqrt, min_samples_leaf=10, min_samples_split=5, n_estimators=800; total time=   0.3s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END criterion=log_loss, max_depth=30, max_features=sqrt, min_samples_leaf=10, min_samples_split=5, n_estimators=800; total time=   0.2s\n",
      "[CV] END criterion=log_loss, max_depth=30, max_features=sqrt, min_samples_leaf=10, min_samples_split=5, n_estimators=800; total time=   0.3s\n",
      "[CV] END criterion=log_loss, max_depth=30, max_features=sqrt, min_samples_leaf=10, min_samples_split=5, n_estimators=800; total time=   0.2s\n",
      "[CV] END criterion=gini, max_depth=20, max_features=log2, min_samples_leaf=10, min_samples_split=2, n_estimators=800; total time=   2.0s\n",
      "[CV] END criterion=gini, max_depth=20, max_features=log2, min_samples_leaf=10, min_samples_split=2, n_estimators=800; total time=   2.0s\n",
      "[CV] END criterion=gini, max_depth=20, max_features=log2, min_samples_leaf=10, min_samples_split=2, n_estimators=800; total time=   2.0s\n",
      "[CV] END criterion=gini, max_depth=20, max_features=log2, min_samples_leaf=10, min_samples_split=2, n_estimators=800; total time=   2.0s\n",
      "[CV] END criterion=gini, max_depth=20, max_features=log2, min_samples_leaf=10, min_samples_split=2, n_estimators=800; total time=   2.1s\n",
      "[CV] END criterion=log_loss, max_depth=10, max_features=sqrt, min_samples_leaf=15, min_samples_split=10, n_estimators=500; total time=   0.2s\n",
      "[CV] END criterion=log_loss, max_depth=10, max_features=sqrt, min_samples_leaf=15, min_samples_split=10, n_estimators=500; total time=   0.1s\n",
      "[CV] END criterion=log_loss, max_depth=10, max_features=sqrt, min_samples_leaf=15, min_samples_split=10, n_estimators=500; total time=   0.1s\n",
      "[CV] END criterion=log_loss, max_depth=10, max_features=sqrt, min_samples_leaf=15, min_samples_split=10, n_estimators=500; total time=   0.2s\n",
      "[CV] END criterion=log_loss, max_depth=10, max_features=sqrt, min_samples_leaf=15, min_samples_split=10, n_estimators=500; total time=   0.2s\n",
      "[CV] END criterion=gini, max_depth=40, max_features=auto, min_samples_leaf=5, min_samples_split=2, n_estimators=2500; total time=  11.9s\n",
      "[CV] END criterion=gini, max_depth=40, max_features=auto, min_samples_leaf=5, min_samples_split=2, n_estimators=2500; total time=  11.5s\n",
      "[CV] END criterion=gini, max_depth=40, max_features=auto, min_samples_leaf=5, min_samples_split=2, n_estimators=2500; total time=  11.8s\n",
      "[CV] END criterion=gini, max_depth=40, max_features=auto, min_samples_leaf=5, min_samples_split=2, n_estimators=2500; total time=  11.6s\n",
      "[CV] END criterion=gini, max_depth=40, max_features=auto, min_samples_leaf=5, min_samples_split=2, n_estimators=2500; total time=  11.7s\n",
      "[CV] END criterion=gini, max_depth=10, max_features=auto, min_samples_leaf=1, min_samples_split=5, n_estimators=5000; total time=  18.8s\n",
      "[CV] END criterion=gini, max_depth=10, max_features=auto, min_samples_leaf=1, min_samples_split=5, n_estimators=5000; total time=  16.1s\n",
      "[CV] END criterion=gini, max_depth=10, max_features=auto, min_samples_leaf=1, min_samples_split=5, n_estimators=5000; total time=  16.1s\n",
      "[CV] END criterion=gini, max_depth=10, max_features=auto, min_samples_leaf=1, min_samples_split=5, n_estimators=5000; total time=  16.5s\n",
      "[CV] END criterion=gini, max_depth=10, max_features=auto, min_samples_leaf=1, min_samples_split=5, n_estimators=5000; total time=  16.6s\n",
      "[CV] END criterion=gini, max_depth=10, max_features=log2, min_samples_leaf=5, min_samples_split=10, n_estimators=2500; total time=   5.9s\n",
      "[CV] END criterion=gini, max_depth=10, max_features=log2, min_samples_leaf=5, min_samples_split=10, n_estimators=2500; total time=   6.2s\n",
      "[CV] END criterion=gini, max_depth=10, max_features=log2, min_samples_leaf=5, min_samples_split=10, n_estimators=2500; total time=   5.9s\n",
      "[CV] END criterion=gini, max_depth=10, max_features=log2, min_samples_leaf=5, min_samples_split=10, n_estimators=2500; total time=   6.1s\n",
      "[CV] END criterion=gini, max_depth=10, max_features=log2, min_samples_leaf=5, min_samples_split=10, n_estimators=2500; total time=   5.9s\n",
      "[CV] END criterion=gini, max_depth=40, max_features=sqrt, min_samples_leaf=1, min_samples_split=15, n_estimators=1500; total time=  10.9s\n",
      "[CV] END criterion=gini, max_depth=40, max_features=sqrt, min_samples_leaf=1, min_samples_split=15, n_estimators=1500; total time=  10.6s\n",
      "[CV] END criterion=gini, max_depth=40, max_features=sqrt, min_samples_leaf=1, min_samples_split=15, n_estimators=1500; total time=  10.7s\n",
      "[CV] END criterion=gini, max_depth=40, max_features=sqrt, min_samples_leaf=1, min_samples_split=15, n_estimators=1500; total time=  10.9s\n",
      "[CV] END criterion=gini, max_depth=40, max_features=sqrt, min_samples_leaf=1, min_samples_split=15, n_estimators=1500; total time=  11.9s\n",
      "[CV] END criterion=entropy, max_depth=30, max_features=auto, min_samples_leaf=5, min_samples_split=15, n_estimators=500; total time=   4.2s\n",
      "[CV] END criterion=entropy, max_depth=30, max_features=auto, min_samples_leaf=5, min_samples_split=15, n_estimators=500; total time=   3.9s\n",
      "[CV] END criterion=entropy, max_depth=30, max_features=auto, min_samples_leaf=5, min_samples_split=15, n_estimators=500; total time=   4.2s\n",
      "[CV] END criterion=entropy, max_depth=30, max_features=auto, min_samples_leaf=5, min_samples_split=15, n_estimators=500; total time=   4.0s\n",
      "[CV] END criterion=entropy, max_depth=30, max_features=auto, min_samples_leaf=5, min_samples_split=15, n_estimators=500; total time=   4.0s\n",
      "[CV] END criterion=entropy, max_depth=40, max_features=sqrt, min_samples_leaf=10, min_samples_split=5, n_estimators=5000; total time=  33.3s\n",
      "[CV] END criterion=entropy, max_depth=40, max_features=sqrt, min_samples_leaf=10, min_samples_split=5, n_estimators=5000; total time=  31.2s\n",
      "[CV] END criterion=entropy, max_depth=40, max_features=sqrt, min_samples_leaf=10, min_samples_split=5, n_estimators=5000; total time=  33.4s\n",
      "[CV] END criterion=entropy, max_depth=40, max_features=sqrt, min_samples_leaf=10, min_samples_split=5, n_estimators=5000; total time=  31.3s\n",
      "[CV] END criterion=entropy, max_depth=40, max_features=sqrt, min_samples_leaf=10, min_samples_split=5, n_estimators=5000; total time=  31.8s\n",
      "[CV] END criterion=gini, max_depth=10, max_features=auto, min_samples_leaf=10, min_samples_split=2, n_estimators=800; total time=   2.0s\n",
      "[CV] END criterion=gini, max_depth=10, max_features=auto, min_samples_leaf=10, min_samples_split=2, n_estimators=800; total time=   2.0s\n",
      "[CV] END criterion=gini, max_depth=10, max_features=auto, min_samples_leaf=10, min_samples_split=2, n_estimators=800; total time=   2.2s\n",
      "[CV] END criterion=gini, max_depth=10, max_features=auto, min_samples_leaf=10, min_samples_split=2, n_estimators=800; total time=   2.2s\n",
      "[CV] END criterion=gini, max_depth=10, max_features=auto, min_samples_leaf=10, min_samples_split=2, n_estimators=800; total time=   2.3s\n",
      "[CV] END criterion=log_loss, max_depth=50, max_features=auto, min_samples_leaf=2, min_samples_split=20, n_estimators=500; total time=   0.2s\n",
      "[CV] END criterion=log_loss, max_depth=50, max_features=auto, min_samples_leaf=2, min_samples_split=20, n_estimators=500; total time=   0.2s\n",
      "[CV] END criterion=log_loss, max_depth=50, max_features=auto, min_samples_leaf=2, min_samples_split=20, n_estimators=500; total time=   0.3s\n",
      "[CV] END criterion=log_loss, max_depth=50, max_features=auto, min_samples_leaf=2, min_samples_split=20, n_estimators=500; total time=   0.3s\n",
      "[CV] END criterion=log_loss, max_depth=50, max_features=auto, min_samples_leaf=2, min_samples_split=20, n_estimators=500; total time=   0.4s\n",
      "[CV] END criterion=entropy, max_depth=10, max_features=sqrt, min_samples_leaf=5, min_samples_split=20, n_estimators=1500; total time=  11.7s\n",
      "[CV] END criterion=entropy, max_depth=10, max_features=sqrt, min_samples_leaf=5, min_samples_split=20, n_estimators=1500; total time=  11.1s\n",
      "[CV] END criterion=entropy, max_depth=10, max_features=sqrt, min_samples_leaf=5, min_samples_split=20, n_estimators=1500; total time=  11.0s\n",
      "[CV] END criterion=entropy, max_depth=10, max_features=sqrt, min_samples_leaf=5, min_samples_split=20, n_estimators=1500; total time=   8.3s\n",
      "[CV] END criterion=entropy, max_depth=10, max_features=sqrt, min_samples_leaf=5, min_samples_split=20, n_estimators=1500; total time=   8.2s\n",
      "[CV] END criterion=log_loss, max_depth=40, max_features=log2, min_samples_leaf=5, min_samples_split=10, n_estimators=1500; total time=   0.6s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END criterion=log_loss, max_depth=40, max_features=log2, min_samples_leaf=5, min_samples_split=10, n_estimators=1500; total time=   0.6s\n",
      "[CV] END criterion=log_loss, max_depth=40, max_features=log2, min_samples_leaf=5, min_samples_split=10, n_estimators=1500; total time=   0.6s\n",
      "[CV] END criterion=log_loss, max_depth=40, max_features=log2, min_samples_leaf=5, min_samples_split=10, n_estimators=1500; total time=   0.5s\n",
      "[CV] END criterion=log_loss, max_depth=40, max_features=log2, min_samples_leaf=5, min_samples_split=10, n_estimators=1500; total time=   0.6s\n",
      "[CV] END criterion=entropy, max_depth=50, max_features=auto, min_samples_leaf=15, min_samples_split=20, n_estimators=800; total time=   4.5s\n",
      "[CV] END criterion=entropy, max_depth=50, max_features=auto, min_samples_leaf=15, min_samples_split=20, n_estimators=800; total time=   4.3s\n",
      "[CV] END criterion=entropy, max_depth=50, max_features=auto, min_samples_leaf=15, min_samples_split=20, n_estimators=800; total time=   4.5s\n",
      "[CV] END criterion=entropy, max_depth=50, max_features=auto, min_samples_leaf=15, min_samples_split=20, n_estimators=800; total time=   4.3s\n",
      "[CV] END criterion=entropy, max_depth=50, max_features=auto, min_samples_leaf=15, min_samples_split=20, n_estimators=800; total time=   4.3s\n",
      "[CV] END criterion=gini, max_depth=50, max_features=auto, min_samples_leaf=1, min_samples_split=10, n_estimators=2500; total time=  21.2s\n",
      "[CV] END criterion=gini, max_depth=50, max_features=auto, min_samples_leaf=1, min_samples_split=10, n_estimators=2500; total time=  25.8s\n",
      "[CV] END criterion=gini, max_depth=50, max_features=auto, min_samples_leaf=1, min_samples_split=10, n_estimators=2500; total time=  23.5s\n",
      "[CV] END criterion=gini, max_depth=50, max_features=auto, min_samples_leaf=1, min_samples_split=10, n_estimators=2500; total time=  21.1s\n",
      "[CV] END criterion=gini, max_depth=50, max_features=auto, min_samples_leaf=1, min_samples_split=10, n_estimators=2500; total time=  20.0s\n",
      "[CV] END criterion=entropy, max_depth=50, max_features=log2, min_samples_leaf=15, min_samples_split=5, n_estimators=500; total time=   1.8s\n",
      "[CV] END criterion=entropy, max_depth=50, max_features=log2, min_samples_leaf=15, min_samples_split=5, n_estimators=500; total time=   1.8s\n",
      "[CV] END criterion=entropy, max_depth=50, max_features=log2, min_samples_leaf=15, min_samples_split=5, n_estimators=500; total time=   1.7s\n",
      "[CV] END criterion=entropy, max_depth=50, max_features=log2, min_samples_leaf=15, min_samples_split=5, n_estimators=500; total time=   1.7s\n",
      "[CV] END criterion=entropy, max_depth=50, max_features=log2, min_samples_leaf=15, min_samples_split=5, n_estimators=500; total time=   1.7s\n",
      "[CV] END criterion=entropy, max_depth=20, max_features=auto, min_samples_leaf=2, min_samples_split=15, n_estimators=5000; total time=  38.5s\n",
      "[CV] END criterion=entropy, max_depth=20, max_features=auto, min_samples_leaf=2, min_samples_split=15, n_estimators=5000; total time=  38.0s\n",
      "[CV] END criterion=entropy, max_depth=20, max_features=auto, min_samples_leaf=2, min_samples_split=15, n_estimators=5000; total time=  38.2s\n",
      "[CV] END criterion=entropy, max_depth=20, max_features=auto, min_samples_leaf=2, min_samples_split=15, n_estimators=5000; total time=  37.3s\n",
      "[CV] END criterion=entropy, max_depth=20, max_features=auto, min_samples_leaf=2, min_samples_split=15, n_estimators=5000; total time=  38.6s\n",
      "[CV] END criterion=log_loss, max_depth=10, max_features=auto, min_samples_leaf=15, min_samples_split=15, n_estimators=500; total time=   0.2s\n",
      "[CV] END criterion=log_loss, max_depth=10, max_features=auto, min_samples_leaf=15, min_samples_split=15, n_estimators=500; total time=   0.2s\n",
      "[CV] END criterion=log_loss, max_depth=10, max_features=auto, min_samples_leaf=15, min_samples_split=15, n_estimators=500; total time=   0.2s\n",
      "[CV] END criterion=log_loss, max_depth=10, max_features=auto, min_samples_leaf=15, min_samples_split=15, n_estimators=500; total time=   0.2s\n",
      "[CV] END criterion=log_loss, max_depth=10, max_features=auto, min_samples_leaf=15, min_samples_split=15, n_estimators=500; total time=   0.2s\n",
      "[CV] END criterion=gini, max_depth=40, max_features=log2, min_samples_leaf=5, min_samples_split=2, n_estimators=5000; total time=  18.0s\n",
      "[CV] END criterion=gini, max_depth=40, max_features=log2, min_samples_leaf=5, min_samples_split=2, n_estimators=5000; total time=  17.7s\n",
      "[CV] END criterion=gini, max_depth=40, max_features=log2, min_samples_leaf=5, min_samples_split=2, n_estimators=5000; total time=  16.9s\n",
      "[CV] END criterion=gini, max_depth=40, max_features=log2, min_samples_leaf=5, min_samples_split=2, n_estimators=5000; total time=  17.4s\n",
      "[CV] END criterion=gini, max_depth=40, max_features=log2, min_samples_leaf=5, min_samples_split=2, n_estimators=5000; total time=  16.5s\n",
      "[CV] END criterion=entropy, max_depth=50, max_features=auto, min_samples_leaf=5, min_samples_split=2, n_estimators=1500; total time=  12.8s\n",
      "[CV] END criterion=entropy, max_depth=50, max_features=auto, min_samples_leaf=5, min_samples_split=2, n_estimators=1500; total time=  12.1s\n",
      "[CV] END criterion=entropy, max_depth=50, max_features=auto, min_samples_leaf=5, min_samples_split=2, n_estimators=1500; total time=  11.8s\n",
      "[CV] END criterion=entropy, max_depth=50, max_features=auto, min_samples_leaf=5, min_samples_split=2, n_estimators=1500; total time=  11.8s\n",
      "[CV] END criterion=entropy, max_depth=50, max_features=auto, min_samples_leaf=5, min_samples_split=2, n_estimators=1500; total time=  11.8s\n",
      "[CV] END criterion=entropy, max_depth=40, max_features=sqrt, min_samples_leaf=1, min_samples_split=10, n_estimators=800; total time=  12.5s\n",
      "[CV] END criterion=entropy, max_depth=40, max_features=sqrt, min_samples_leaf=1, min_samples_split=10, n_estimators=800; total time=  12.2s\n",
      "[CV] END criterion=entropy, max_depth=40, max_features=sqrt, min_samples_leaf=1, min_samples_split=10, n_estimators=800; total time=  15.7s\n",
      "[CV] END criterion=entropy, max_depth=40, max_features=sqrt, min_samples_leaf=1, min_samples_split=10, n_estimators=800; total time=  10.7s\n",
      "[CV] END criterion=entropy, max_depth=40, max_features=sqrt, min_samples_leaf=1, min_samples_split=10, n_estimators=800; total time=  11.0s\n",
      "[CV] END criterion=gini, max_depth=10, max_features=sqrt, min_samples_leaf=1, min_samples_split=2, n_estimators=500; total time=   1.8s\n",
      "[CV] END criterion=gini, max_depth=10, max_features=sqrt, min_samples_leaf=1, min_samples_split=2, n_estimators=500; total time=   1.7s\n",
      "[CV] END criterion=gini, max_depth=10, max_features=sqrt, min_samples_leaf=1, min_samples_split=2, n_estimators=500; total time=   1.7s\n",
      "[CV] END criterion=gini, max_depth=10, max_features=sqrt, min_samples_leaf=1, min_samples_split=2, n_estimators=500; total time=   2.0s\n",
      "[CV] END criterion=gini, max_depth=10, max_features=sqrt, min_samples_leaf=1, min_samples_split=2, n_estimators=500; total time=   2.0s\n",
      "[CV] END criterion=gini, max_depth=40, max_features=sqrt, min_samples_leaf=2, min_samples_split=2, n_estimators=5000; total time=  43.1s\n",
      "[CV] END criterion=gini, max_depth=40, max_features=sqrt, min_samples_leaf=2, min_samples_split=2, n_estimators=5000; total time=  39.4s\n",
      "[CV] END criterion=gini, max_depth=40, max_features=sqrt, min_samples_leaf=2, min_samples_split=2, n_estimators=5000; total time=  39.7s\n",
      "[CV] END criterion=gini, max_depth=40, max_features=sqrt, min_samples_leaf=2, min_samples_split=2, n_estimators=5000; total time=  40.0s\n",
      "[CV] END criterion=gini, max_depth=40, max_features=sqrt, min_samples_leaf=2, min_samples_split=2, n_estimators=5000; total time=  41.0s\n",
      "[CV] END criterion=log_loss, max_depth=50, max_features=auto, min_samples_leaf=15, min_samples_split=5, n_estimators=1500; total time=   0.7s\n",
      "[CV] END criterion=log_loss, max_depth=50, max_features=auto, min_samples_leaf=15, min_samples_split=5, n_estimators=1500; total time=   0.9s\n",
      "[CV] END criterion=log_loss, max_depth=50, max_features=auto, min_samples_leaf=15, min_samples_split=5, n_estimators=1500; total time=   0.7s\n",
      "[CV] END criterion=log_loss, max_depth=50, max_features=auto, min_samples_leaf=15, min_samples_split=5, n_estimators=1500; total time=   0.8s\n",
      "[CV] END criterion=log_loss, max_depth=50, max_features=auto, min_samples_leaf=15, min_samples_split=5, n_estimators=1500; total time=   0.8s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END criterion=gini, max_depth=20, max_features=sqrt, min_samples_leaf=2, min_samples_split=5, n_estimators=5000; total time=  29.0s\n",
      "[CV] END criterion=gini, max_depth=20, max_features=sqrt, min_samples_leaf=2, min_samples_split=5, n_estimators=5000; total time=  30.3s\n",
      "[CV] END criterion=gini, max_depth=20, max_features=sqrt, min_samples_leaf=2, min_samples_split=5, n_estimators=5000; total time=  24.0s\n",
      "[CV] END criterion=gini, max_depth=20, max_features=sqrt, min_samples_leaf=2, min_samples_split=5, n_estimators=5000; total time=  27.0s\n",
      "[CV] END criterion=gini, max_depth=20, max_features=sqrt, min_samples_leaf=2, min_samples_split=5, n_estimators=5000; total time=  28.3s\n",
      "[CV] END criterion=gini, max_depth=10, max_features=auto, min_samples_leaf=2, min_samples_split=20, n_estimators=1500; total time=   4.3s\n",
      "[CV] END criterion=gini, max_depth=10, max_features=auto, min_samples_leaf=2, min_samples_split=20, n_estimators=1500; total time=   4.2s\n"
     ]
    }
   ],
   "source": [
    "RCF = RandomForestClassifier(random_state=1)\n",
    "RCF_random = RandomizedSearchCV(estimator=RCF,\n",
    "                                param_distributions=param_grid_,\n",
    "                               n_iter=100,\n",
    "                               cv=5,\n",
    "                               verbose=2,\n",
    "                               random_state=RANDOM_SEED,\n",
    "                               )\n",
    "\n",
    "RCF_random.fit(X_train,y_train)\n",
    "\n",
    "print(RCF_random.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54d5398c",
   "metadata": {},
   "source": [
    "### Accuracy is still terrible so we will do two things to try and help out the data set.   \n",
    "### 1. Only use characters who are top 20 characters (1st attempt)\n",
    "### 2. 1st attempt along with only getting spoken dialog where there are more than 2 or 3 words."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d00ed28d",
   "metadata": {},
   "source": [
    "### We can also add the gender along with season and episode and age (1 = adult, 2 = child/teenage, 0 = no age)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "b9dc15e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(list(zip(y_test,y_pred)), columns = ['Character', 'Predicted'])\n",
    "#df['Error'] = df['Character'] - df['Predicted']\n",
    "\n",
    "#Plot the joint distribution\n",
    "#Let's start by seeing how the two distributions line up\n",
    "#sns.barplot(data=df, x='Character', y='Predicted', kind=\"hex\", color=\"red\",\n",
    "#             )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "d7a5f003",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tf = pd.DataFrame(X_train.toarray(), columns=vectorizer.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "6edef798",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4360, 285)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_tf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "71684e60",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>actually</th>\n",
       "      <th>ago</th>\n",
       "      <th>ahead</th>\n",
       "      <th>alexei</th>\n",
       "      <th>answer</th>\n",
       "      <th>approach</th>\n",
       "      <th>arm</th>\n",
       "      <th>ask</th>\n",
       "      <th>attention</th>\n",
       "      <th>away</th>\n",
       "      <th>...</th>\n",
       "      <th>whoa</th>\n",
       "      <th>window</th>\n",
       "      <th>word</th>\n",
       "      <th>work</th>\n",
       "      <th>world</th>\n",
       "      <th>wrong</th>\n",
       "      <th>yeah</th>\n",
       "      <th>year</th>\n",
       "      <th>yes</th>\n",
       "      <th>yuri</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.717594</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 285 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   actually  ago  ahead  alexei  answer  approach  arm  ask  attention  away  \\\n",
       "0       0.0  0.0    0.0     0.0     0.0       0.0  0.0  0.0        0.0   0.0   \n",
       "1       0.0  0.0    0.0     0.0     0.0       0.0  0.0  0.0        0.0   0.0   \n",
       "2       0.0  0.0    0.0     0.0     0.0       0.0  0.0  0.0        0.0   0.0   \n",
       "\n",
       "   ...  whoa  window  word  work  world  wrong  yeah  year       yes  yuri  \n",
       "0  ...   0.0     0.0   0.0   0.0    0.0    0.0   0.0   0.0  0.717594   0.0  \n",
       "1  ...   0.0     0.0   0.0   0.0    0.0    0.0   0.0   0.0  0.000000   0.0  \n",
       "2  ...   0.0     0.0   0.0   0.0    0.0    0.0   0.0   0.0  0.000000   0.0  \n",
       "\n",
       "[3 rows x 285 columns]"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_tf.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "c37c5379",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4360, 6)"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "e1c61393",
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_df = train_df.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "c815a9e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_w_feat = pd.concat([df_tf,tr_df],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "489d65ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4360, 292)"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_w_feat.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "59a29bf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_w_feat.drop(['Dialog','token_lemma','index'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "6dc1d283",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_w_features = pd.merge(train_w_feat, gender, on=\"Character_Fix\",how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "951b9f04",
   "metadata": {},
   "outputs": [],
   "source": [
    "fix = train_w_features[train_w_features.Gender.isnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "eac42d6f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>actually</th>\n",
       "      <th>ago</th>\n",
       "      <th>ahead</th>\n",
       "      <th>alexei</th>\n",
       "      <th>answer</th>\n",
       "      <th>approach</th>\n",
       "      <th>arm</th>\n",
       "      <th>ask</th>\n",
       "      <th>attention</th>\n",
       "      <th>away</th>\n",
       "      <th>...</th>\n",
       "      <th>yeah</th>\n",
       "      <th>year</th>\n",
       "      <th>yes</th>\n",
       "      <th>yuri</th>\n",
       "      <th>Character_Fix</th>\n",
       "      <th>Season</th>\n",
       "      <th>Episode</th>\n",
       "      <th>length</th>\n",
       "      <th>Gender</th>\n",
       "      <th>Age</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>0 rows × 291 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [actually, ago, ahead, alexei, answer, approach, arm, ask, attention, away, baby, bad, barb, beat, begin, believe, big, billy, blood, blue, body, box, boy, break, brenner, bruce, byers, car, care, carry, catch, chair, change, check, child, close, code, come, continue, cool, copy, cover, crazy, dad, dark, day, dead, die, door, driscoll, drive, drop, dude, dustin, eat, el, end, enter, erica, exactly, eye, face, fall, family, feel, figure, fine, flashlight, flayer, floor, follow, foot, friend, fun, gate, girl, glance, god, good, grab, great, grigori, ground, guard, guess, gun, guy, hair, hand, hang, happen, hard, hawkins, head, hear, heather, hell, hello, help, hey, ...]\n",
       "Index: []\n",
       "\n",
       "[0 rows x 291 columns]"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "1a2273da",
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_w_features2 =train_w_features[train_w_features['length']>=3].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "e02ef0a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_w_features3 = train_w_features.drop(['Character_Fix'], axis=1).copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "0c74cc7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_w_features3['Gender'].replace(['Female ', 'Female','Male','Male ','Neutral'],\n",
    "                        [1, 1,2,2,0], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "b0e61115",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>actually</th>\n",
       "      <th>ago</th>\n",
       "      <th>ahead</th>\n",
       "      <th>alexei</th>\n",
       "      <th>answer</th>\n",
       "      <th>approach</th>\n",
       "      <th>arm</th>\n",
       "      <th>ask</th>\n",
       "      <th>attention</th>\n",
       "      <th>away</th>\n",
       "      <th>...</th>\n",
       "      <th>wrong</th>\n",
       "      <th>yeah</th>\n",
       "      <th>year</th>\n",
       "      <th>yes</th>\n",
       "      <th>yuri</th>\n",
       "      <th>Season</th>\n",
       "      <th>Episode</th>\n",
       "      <th>length</th>\n",
       "      <th>Gender</th>\n",
       "      <th>Age</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.717594</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>36</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>19</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>159</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 290 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   actually  ago  ahead  alexei  answer  approach  arm  ask  attention  away  \\\n",
       "0       0.0  0.0    0.0     0.0     0.0       0.0  0.0  0.0        0.0   0.0   \n",
       "1       0.0  0.0    0.0     0.0     0.0       0.0  0.0  0.0        0.0   0.0   \n",
       "2       0.0  0.0    0.0     0.0     0.0       0.0  0.0  0.0        0.0   0.0   \n",
       "\n",
       "   ...  wrong  yeah  year       yes  yuri  Season  Episode  length  Gender  \\\n",
       "0  ...    0.0   0.0   0.0  0.717594   0.0       1        2      36       1   \n",
       "1  ...    0.0   0.0   0.0  0.000000   0.0       4        4      19       2   \n",
       "2  ...    0.0   0.0   0.0  0.000000   0.0       3        3     159       2   \n",
       "\n",
       "   Age  \n",
       "0    1  \n",
       "1    2  \n",
       "2    2  \n",
       "\n",
       "[3 rows x 290 columns]"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_w_features3.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "a1cbeaa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "RCF_model_v2 = RandomForestClassifier(n_estimators=100, \\\n",
    "                              random_state=RANDOM_SEED,\\\n",
    "                              max_features='sqrt').fit(train_w_features3, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "fdf1e48f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4360"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "6e35a480",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1091"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "a0020189",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tf_test = pd.DataFrame(X_test.toarray(), columns=vectorizer.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "0c536e8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_df_test = test_df.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "fd9397b0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1091, 290)"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_w_feat = pd.concat([df_tf_test,tr_df_test],axis=1)\n",
    "test_w_feat.drop(['Dialog','token_lemma','index'], axis=1, inplace=True)\n",
    "test_w_features = pd.merge(test_w_feat, gender, on=\"Character_Fix\",how='left')\n",
    "test_w_features3 = test_w_features.drop(['Character_Fix'], axis=1).copy()\n",
    "test_w_features3['Gender'].replace(['Female ', 'Female','Male','Male ','Neutral'],\n",
    "                        [1, 1,2,2,0], inplace=True)\n",
    "test_w_features3.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "1541ba53",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_RCF_modelv2 =RCF_model_v2.predict(test_w_features3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "49e7c9e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1091"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(y_pred_RCF_modelv2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "6ed6cdff",
   "metadata": {},
   "outputs": [],
   "source": [
    "accm2 = metrics.accuracy_score(y_test,y_pred_RCF_modelv2)\n",
    "\n",
    "#lg_model_scr2 = lg_model.score(test_w_features3, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "d6ab3f15",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.47296058661778184"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accm2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b518b733",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_score_no_tune_with_features = RCF_model_v2.score(train_w_features3, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83f81202",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_score_no_tune_with_features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbbb0a33",
   "metadata": {},
   "source": [
    "### Try this with our new hyper parameters since it went from 17% to 19% and is now out 39.5% with adding some features to the data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "0d46ec2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.47296058661778184\n",
      "0.9855504587155963\n"
     ]
    }
   ],
   "source": [
    "\n",
    "RCF_model_tuned = RandomForestClassifier(n_estimators=100, \\\n",
    "                              random_state=RANDOM_SEED,\\\n",
    "                              max_features='sqrt').fit(train_w_features3, y_train)\n",
    "\n",
    "y_pred_RCF_model_tuned = RCF_model_tuned.predict(test_w_features3)\n",
    "\n",
    "\n",
    "accm_tuned = metrics.accuracy_score(y_test,y_pred_RCF_model_tuned)\n",
    "\n",
    "print(accm_tuned)\n",
    "\n",
    "acc_train = RCF_model_tuned.score(train_w_features3, y_train)\n",
    "\n",
    "print(acc_train)\n",
    "\n",
    "# lg_model_tuned = LogisticRegression(max_iter = 500, \\ class_weight = 'balanced',\n",
    "#                               random_state=RANDOM_SEED,\\\n",
    "#                               solver = 'liblinear',penalty= 'l1',\n",
    "#                               C = 0.1, verbose = 15, ).fit(train_w_features3, y_train)\n",
    "\n",
    "# y_pred_lg_model_tuned = lg_model_tuned.predict(test_w_features3)\n",
    "\n",
    "# accm_tuned = metrics.accuracy_score(y_test,y_pred_lg_model_tuned)\n",
    "\n",
    "# print(accm_tuned)\n",
    "\n",
    "# acc_train = lg_model_tuned.score(train_w_features3, y_train)\n",
    "\n",
    "# print(acc_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72727852",
   "metadata": {},
   "source": [
    "### It only performed 3.2% better so we are not getting far, I think dropping data from the org data based on length of line spoken and how often a character occurs along with text difficulty as well as keeping our features we already just used (gender, season, episode) will help this process along.  We can try and add in who spoke durng scene as well to see if it helps.  We would be thrilled if this was able to get over 50%.  We knew after performing EDA and QAing the data there were issues with just using pdf's in general and then through the show there actually isnt much dialog as one would hope, it is more scene setting and action."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8104d241",
   "metadata": {},
   "source": [
    "### Next drop out rows where length is less than 2 or 3 and redo training/hyperparameter tuning and/or characters who speak less than 5 times, maybe standard scale new df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8549471",
   "metadata": {},
   "source": [
    "### I think what also needs to happen is hyper parameter tuning with these features and data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "478e5b6e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Character_Fix</th>\n",
       "      <th>Season</th>\n",
       "      <th>Episode</th>\n",
       "      <th>Dialog</th>\n",
       "      <th>token_lemma</th>\n",
       "      <th>length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>HOPPER</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>Looks like somebody’s home. As the truck parks...</td>\n",
       "      <td>look like somebody home as the truck park -pro...</td>\n",
       "      <td>322</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>HOPPER</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>Where’s that coming from? Joyce pauses at the ...</td>\n",
       "      <td>where that come from joyce pause at the foot o...</td>\n",
       "      <td>153</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>HOPPER</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>Hey, dipshits! The men drop their tools and st...</td>\n",
       "      <td>hey dipshit the man drop -pron- tool and stand...</td>\n",
       "      <td>72</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>HOPPER</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>Dr. Alexei steps forward, putting himself betw...</td>\n",
       "      <td>alexei step forward put -pron- between hopper ...</td>\n",
       "      <td>148</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>HOPPER</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>English! You speak English? Alexei tries to ca...</td>\n",
       "      <td>english -pron- speak english alexei try to cal...</td>\n",
       "      <td>61</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Character_Fix  Season  Episode  \\\n",
       "5         HOPPER       3        5   \n",
       "6         HOPPER       3        5   \n",
       "7         HOPPER       3        5   \n",
       "8         HOPPER       3        5   \n",
       "10        HOPPER       3        5   \n",
       "\n",
       "                                               Dialog  \\\n",
       "5   Looks like somebody’s home. As the truck parks...   \n",
       "6   Where’s that coming from? Joyce pauses at the ...   \n",
       "7   Hey, dipshits! The men drop their tools and st...   \n",
       "8   Dr. Alexei steps forward, putting himself betw...   \n",
       "10  English! You speak English? Alexei tries to ca...   \n",
       "\n",
       "                                          token_lemma  length  \n",
       "5   look like somebody home as the truck park -pro...     322  \n",
       "6   where that come from joyce pause at the foot o...     153  \n",
       "7   hey dipshit the man drop -pron- tool and stand...      72  \n",
       "8   alexei step forward put -pron- between hopper ...     148  \n",
       "10  english -pron- speak english alexei try to cal...      61  "
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### starting with the data and features - not split yet.\n",
    "df_show.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "46648a55",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sh2 = df_show.merge(gender, how=\"left\", on ='Character_Fix')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "8eb14c8b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Character_Fix</th>\n",
       "      <th>Season</th>\n",
       "      <th>Episode</th>\n",
       "      <th>Dialog</th>\n",
       "      <th>token_lemma</th>\n",
       "      <th>length</th>\n",
       "      <th>Gender</th>\n",
       "      <th>Age</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>HOPPER</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>Looks like somebody’s home. As the truck parks...</td>\n",
       "      <td>look like somebody home as the truck park -pro...</td>\n",
       "      <td>322</td>\n",
       "      <td>Male</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>HOPPER</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>Where’s that coming from? Joyce pauses at the ...</td>\n",
       "      <td>where that come from joyce pause at the foot o...</td>\n",
       "      <td>153</td>\n",
       "      <td>Male</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>HOPPER</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>Hey, dipshits! The men drop their tools and st...</td>\n",
       "      <td>hey dipshit the man drop -pron- tool and stand...</td>\n",
       "      <td>72</td>\n",
       "      <td>Male</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>HOPPER</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>Dr. Alexei steps forward, putting himself betw...</td>\n",
       "      <td>alexei step forward put -pron- between hopper ...</td>\n",
       "      <td>148</td>\n",
       "      <td>Male</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>HOPPER</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>English! You speak English? Alexei tries to ca...</td>\n",
       "      <td>english -pron- speak english alexei try to cal...</td>\n",
       "      <td>61</td>\n",
       "      <td>Male</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5446</th>\n",
       "      <td>SUSAN</td>\n",
       "      <td>2</td>\n",
       "      <td>9</td>\n",
       "      <td>See -- pretty -- !!</td>\n",
       "      <td>see pretty</td>\n",
       "      <td>10</td>\n",
       "      <td>Female</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5447</th>\n",
       "      <td>SUSAN</td>\n",
       "      <td>2</td>\n",
       "      <td>9</td>\n",
       "      <td>... Now -- you surrrre you don't  want to wear...</td>\n",
       "      <td>now -pron- surrrre -pron- do want to wear a dr...</td>\n",
       "      <td>103</td>\n",
       "      <td>Female</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5448</th>\n",
       "      <td>CLAUDIA</td>\n",
       "      <td>2</td>\n",
       "      <td>9</td>\n",
       "      <td>Where did you see it last?</td>\n",
       "      <td>where do -pron- see -pron- last</td>\n",
       "      <td>31</td>\n",
       "      <td>Female</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5449</th>\n",
       "      <td>CLAUDIA</td>\n",
       "      <td>2</td>\n",
       "      <td>9</td>\n",
       "      <td>What's in there that's so important  anyway?  ...</td>\n",
       "      <td>what in there that so important anyway -pron- ...</td>\n",
       "      <td>123</td>\n",
       "      <td>Female</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5450</th>\n",
       "      <td>CLAUDIA</td>\n",
       "      <td>2</td>\n",
       "      <td>9</td>\n",
       "      <td>That's your ride!  Dusty??? The bag from Melva...</td>\n",
       "      <td>that -pron- ride dusty the bag from melvald up...</td>\n",
       "      <td>60</td>\n",
       "      <td>Female</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5451 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Character_Fix  Season  Episode  \\\n",
       "0           HOPPER       3        5   \n",
       "1           HOPPER       3        5   \n",
       "2           HOPPER       3        5   \n",
       "3           HOPPER       3        5   \n",
       "4           HOPPER       3        5   \n",
       "...            ...     ...      ...   \n",
       "5446         SUSAN       2        9   \n",
       "5447         SUSAN       2        9   \n",
       "5448       CLAUDIA       2        9   \n",
       "5449       CLAUDIA       2        9   \n",
       "5450       CLAUDIA       2        9   \n",
       "\n",
       "                                                 Dialog  \\\n",
       "0     Looks like somebody’s home. As the truck parks...   \n",
       "1     Where’s that coming from? Joyce pauses at the ...   \n",
       "2     Hey, dipshits! The men drop their tools and st...   \n",
       "3     Dr. Alexei steps forward, putting himself betw...   \n",
       "4     English! You speak English? Alexei tries to ca...   \n",
       "...                                                 ...   \n",
       "5446                                See -- pretty -- !!   \n",
       "5447  ... Now -- you surrrre you don't  want to wear...   \n",
       "5448                         Where did you see it last?   \n",
       "5449  What's in there that's so important  anyway?  ...   \n",
       "5450  That's your ride!  Dusty??? The bag from Melva...   \n",
       "\n",
       "                                            token_lemma  length  Gender  Age  \n",
       "0     look like somebody home as the truck park -pro...     322    Male    1  \n",
       "1     where that come from joyce pause at the foot o...     153    Male    1  \n",
       "2     hey dipshit the man drop -pron- tool and stand...      72    Male    1  \n",
       "3     alexei step forward put -pron- between hopper ...     148    Male    1  \n",
       "4     english -pron- speak english alexei try to cal...      61    Male    1  \n",
       "...                                                 ...     ...     ...  ...  \n",
       "5446                                         see pretty      10  Female    1  \n",
       "5447  now -pron- surrrre -pron- do want to wear a dr...     103  Female    1  \n",
       "5448                    where do -pron- see -pron- last      31  Female    1  \n",
       "5449  what in there that so important anyway -pron- ...     123  Female    1  \n",
       "5450  that -pron- ride dusty the bag from melvald up...      60  Female    1  \n",
       "\n",
       "[5451 rows x 8 columns]"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_sh2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "pharmaceutical-rider",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting textstat\n",
      "  Downloading textstat-0.7.3-py3-none-any.whl (105 kB)\n",
      "\u001b[K     |████████████████████████████████| 105 kB 43.8 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting pyphen\n",
      "  Downloading pyphen-0.13.0-py3-none-any.whl (2.0 MB)\n",
      "\u001b[K     |████████████████████████████████| 2.0 MB 47.9 MB/s eta 0:00:01\n",
      "\u001b[?25hInstalling collected packages: pyphen, textstat\n",
      "Successfully installed pyphen-0.13.0 textstat-0.7.3\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "#pip install textstat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "43d76aff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import textstat\n",
    "df_sh2['grade'] = df_sh2.Dialog.apply(lambda x: textstat.flesch_kincaid_grade(x))\n",
    "df_sh2['difficulty'] = df_sh2.Dialog.apply(lambda x: textstat.difficult_words(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "b503a1d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Character_Fix</th>\n",
       "      <th>Season</th>\n",
       "      <th>Episode</th>\n",
       "      <th>Dialog</th>\n",
       "      <th>token_lemma</th>\n",
       "      <th>length</th>\n",
       "      <th>Gender</th>\n",
       "      <th>Age</th>\n",
       "      <th>grade</th>\n",
       "      <th>difficulty</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>HOPPER</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>Looks like somebody’s home. As the truck parks...</td>\n",
       "      <td>look like somebody home as the truck park -pro...</td>\n",
       "      <td>322</td>\n",
       "      <td>Male</td>\n",
       "      <td>1</td>\n",
       "      <td>2.6</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>HOPPER</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>Where’s that coming from? Joyce pauses at the ...</td>\n",
       "      <td>where that come from joyce pause at the foot o...</td>\n",
       "      <td>153</td>\n",
       "      <td>Male</td>\n",
       "      <td>1</td>\n",
       "      <td>2.8</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>HOPPER</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>Hey, dipshits! The men drop their tools and st...</td>\n",
       "      <td>hey dipshit the man drop -pron- tool and stand...</td>\n",
       "      <td>72</td>\n",
       "      <td>Male</td>\n",
       "      <td>1</td>\n",
       "      <td>0.3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>HOPPER</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>Dr. Alexei steps forward, putting himself betw...</td>\n",
       "      <td>alexei step forward put -pron- between hopper ...</td>\n",
       "      <td>148</td>\n",
       "      <td>Male</td>\n",
       "      <td>1</td>\n",
       "      <td>4.3</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>HOPPER</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>English! You speak English? Alexei tries to ca...</td>\n",
       "      <td>english -pron- speak english alexei try to cal...</td>\n",
       "      <td>61</td>\n",
       "      <td>Male</td>\n",
       "      <td>1</td>\n",
       "      <td>5.2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5446</th>\n",
       "      <td>SUSAN</td>\n",
       "      <td>2</td>\n",
       "      <td>9</td>\n",
       "      <td>See -- pretty -- !!</td>\n",
       "      <td>see pretty</td>\n",
       "      <td>10</td>\n",
       "      <td>Female</td>\n",
       "      <td>1</td>\n",
       "      <td>2.9</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5447</th>\n",
       "      <td>SUSAN</td>\n",
       "      <td>2</td>\n",
       "      <td>9</td>\n",
       "      <td>... Now -- you surrrre you don't  want to wear...</td>\n",
       "      <td>now -pron- surrrre -pron- do want to wear a dr...</td>\n",
       "      <td>103</td>\n",
       "      <td>Female</td>\n",
       "      <td>1</td>\n",
       "      <td>0.1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5448</th>\n",
       "      <td>CLAUDIA</td>\n",
       "      <td>2</td>\n",
       "      <td>9</td>\n",
       "      <td>Where did you see it last?</td>\n",
       "      <td>where do -pron- see -pron- last</td>\n",
       "      <td>31</td>\n",
       "      <td>Female</td>\n",
       "      <td>1</td>\n",
       "      <td>-1.5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5449</th>\n",
       "      <td>CLAUDIA</td>\n",
       "      <td>2</td>\n",
       "      <td>9</td>\n",
       "      <td>What's in there that's so important  anyway?  ...</td>\n",
       "      <td>what in there that so important anyway -pron- ...</td>\n",
       "      <td>123</td>\n",
       "      <td>Female</td>\n",
       "      <td>1</td>\n",
       "      <td>3.2</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5450</th>\n",
       "      <td>CLAUDIA</td>\n",
       "      <td>2</td>\n",
       "      <td>9</td>\n",
       "      <td>That's your ride!  Dusty??? The bag from Melva...</td>\n",
       "      <td>that -pron- ride dusty the bag from melvald up...</td>\n",
       "      <td>60</td>\n",
       "      <td>Female</td>\n",
       "      <td>1</td>\n",
       "      <td>1.9</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5451 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Character_Fix  Season  Episode  \\\n",
       "0           HOPPER       3        5   \n",
       "1           HOPPER       3        5   \n",
       "2           HOPPER       3        5   \n",
       "3           HOPPER       3        5   \n",
       "4           HOPPER       3        5   \n",
       "...            ...     ...      ...   \n",
       "5446         SUSAN       2        9   \n",
       "5447         SUSAN       2        9   \n",
       "5448       CLAUDIA       2        9   \n",
       "5449       CLAUDIA       2        9   \n",
       "5450       CLAUDIA       2        9   \n",
       "\n",
       "                                                 Dialog  \\\n",
       "0     Looks like somebody’s home. As the truck parks...   \n",
       "1     Where’s that coming from? Joyce pauses at the ...   \n",
       "2     Hey, dipshits! The men drop their tools and st...   \n",
       "3     Dr. Alexei steps forward, putting himself betw...   \n",
       "4     English! You speak English? Alexei tries to ca...   \n",
       "...                                                 ...   \n",
       "5446                                See -- pretty -- !!   \n",
       "5447  ... Now -- you surrrre you don't  want to wear...   \n",
       "5448                         Where did you see it last?   \n",
       "5449  What's in there that's so important  anyway?  ...   \n",
       "5450  That's your ride!  Dusty??? The bag from Melva...   \n",
       "\n",
       "                                            token_lemma  length  Gender  Age  \\\n",
       "0     look like somebody home as the truck park -pro...     322    Male    1   \n",
       "1     where that come from joyce pause at the foot o...     153    Male    1   \n",
       "2     hey dipshit the man drop -pron- tool and stand...      72    Male    1   \n",
       "3     alexei step forward put -pron- between hopper ...     148    Male    1   \n",
       "4     english -pron- speak english alexei try to cal...      61    Male    1   \n",
       "...                                                 ...     ...     ...  ...   \n",
       "5446                                         see pretty      10  Female    1   \n",
       "5447  now -pron- surrrre -pron- do want to wear a dr...     103  Female    1   \n",
       "5448                    where do -pron- see -pron- last      31  Female    1   \n",
       "5449  what in there that so important anyway -pron- ...     123  Female    1   \n",
       "5450  that -pron- ride dusty the bag from melvald up...      60  Female    1   \n",
       "\n",
       "      grade  difficulty  \n",
       "0       2.6          11  \n",
       "1       2.8           3  \n",
       "2       0.3           1  \n",
       "3       4.3           6  \n",
       "4       5.2           2  \n",
       "...     ...         ...  \n",
       "5446    2.9           0  \n",
       "5447    0.1           2  \n",
       "5448   -1.5           0  \n",
       "5449    3.2           3  \n",
       "5450    1.9           3  \n",
       "\n",
       "[5451 rows x 10 columns]"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_sh2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "b9a9ea47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "polarity checked successfull\n"
     ]
    }
   ],
   "source": [
    "#adding the compound score on again but with the token lemma\n",
    "\n",
    "df_sh2['polarity'] = df_sh2['token_lemma'].apply(polarity)  #polarity checking\n",
    "df_sh2['compound']  = df_sh2['polarity'].apply(lambda score_dict: score_dict['compound'])\n",
    "\n",
    "df_sh2['sentiment'] = df_sh2['compound'].apply(lambda x: \"Positive\" if x>=0.5 \\\n",
    "                                                     else(\"Negative\" if x<=-0.5 \\\n",
    "                                                     else \"Neutral\"))\n",
    "print(\"polarity checked successfull\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "b4dc1d39",
   "metadata": {},
   "outputs": [],
   "source": [
    "#removing short spoken lines like, no, what, so , hey, yep etc \n",
    "#basically when someone spoke 3 words or less\n",
    "df_sh2['length_spk'] = df_sh2['token_lemma'].apply(lambda x: len(x.split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "5b967864",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sh2 = df_sh2[df_sh2['length_spk'] >3].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "6c27d513",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Character_Fix</th>\n",
       "      <th>Season</th>\n",
       "      <th>Episode</th>\n",
       "      <th>Dialog</th>\n",
       "      <th>token_lemma</th>\n",
       "      <th>length</th>\n",
       "      <th>Gender</th>\n",
       "      <th>Age</th>\n",
       "      <th>grade</th>\n",
       "      <th>difficulty</th>\n",
       "      <th>polarity</th>\n",
       "      <th>compound</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>length_spk</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>HOPPER</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>Looks like somebody’s home. As the truck parks...</td>\n",
       "      <td>look like somebody home as the truck park -pro...</td>\n",
       "      <td>322</td>\n",
       "      <td>Male</td>\n",
       "      <td>1</td>\n",
       "      <td>2.6</td>\n",
       "      <td>11</td>\n",
       "      <td>{'neg': 0.089, 'neu': 0.881, 'pos': 0.03, 'com...</td>\n",
       "      <td>-0.3612</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>HOPPER</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>Where’s that coming from? Joyce pauses at the ...</td>\n",
       "      <td>where that come from joyce pause at the foot o...</td>\n",
       "      <td>153</td>\n",
       "      <td>Male</td>\n",
       "      <td>1</td>\n",
       "      <td>2.8</td>\n",
       "      <td>3</td>\n",
       "      <td>{'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound...</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>HOPPER</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>Hey, dipshits! The men drop their tools and st...</td>\n",
       "      <td>hey dipshit the man drop -pron- tool and stand...</td>\n",
       "      <td>72</td>\n",
       "      <td>Male</td>\n",
       "      <td>1</td>\n",
       "      <td>0.3</td>\n",
       "      <td>1</td>\n",
       "      <td>{'neg': 0.255, 'neu': 0.588, 'pos': 0.157, 'co...</td>\n",
       "      <td>-0.2500</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>HOPPER</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>Dr. Alexei steps forward, putting himself betw...</td>\n",
       "      <td>alexei step forward put -pron- between hopper ...</td>\n",
       "      <td>148</td>\n",
       "      <td>Male</td>\n",
       "      <td>1</td>\n",
       "      <td>4.3</td>\n",
       "      <td>6</td>\n",
       "      <td>{'neg': 0.17, 'neu': 0.83, 'pos': 0.0, 'compou...</td>\n",
       "      <td>-0.5719</td>\n",
       "      <td>Negative</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>HOPPER</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>English! You speak English? Alexei tries to ca...</td>\n",
       "      <td>english -pron- speak english alexei try to cal...</td>\n",
       "      <td>61</td>\n",
       "      <td>Male</td>\n",
       "      <td>1</td>\n",
       "      <td>5.2</td>\n",
       "      <td>2</td>\n",
       "      <td>{'neg': 0.0, 'neu': 0.796, 'pos': 0.204, 'comp...</td>\n",
       "      <td>0.3182</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Character_Fix  Season  Episode  \\\n",
       "0        HOPPER       3        5   \n",
       "1        HOPPER       3        5   \n",
       "2        HOPPER       3        5   \n",
       "3        HOPPER       3        5   \n",
       "4        HOPPER       3        5   \n",
       "\n",
       "                                              Dialog  \\\n",
       "0  Looks like somebody’s home. As the truck parks...   \n",
       "1  Where’s that coming from? Joyce pauses at the ...   \n",
       "2  Hey, dipshits! The men drop their tools and st...   \n",
       "3  Dr. Alexei steps forward, putting himself betw...   \n",
       "4  English! You speak English? Alexei tries to ca...   \n",
       "\n",
       "                                         token_lemma  length Gender  Age  \\\n",
       "0  look like somebody home as the truck park -pro...     322   Male    1   \n",
       "1  where that come from joyce pause at the foot o...     153   Male    1   \n",
       "2  hey dipshit the man drop -pron- tool and stand...      72   Male    1   \n",
       "3  alexei step forward put -pron- between hopper ...     148   Male    1   \n",
       "4  english -pron- speak english alexei try to cal...      61   Male    1   \n",
       "\n",
       "   grade  difficulty                                           polarity  \\\n",
       "0    2.6          11  {'neg': 0.089, 'neu': 0.881, 'pos': 0.03, 'com...   \n",
       "1    2.8           3  {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound...   \n",
       "2    0.3           1  {'neg': 0.255, 'neu': 0.588, 'pos': 0.157, 'co...   \n",
       "3    4.3           6  {'neg': 0.17, 'neu': 0.83, 'pos': 0.0, 'compou...   \n",
       "4    5.2           2  {'neg': 0.0, 'neu': 0.796, 'pos': 0.204, 'comp...   \n",
       "\n",
       "   compound sentiment  length_spk  \n",
       "0   -0.3612   Neutral          59  \n",
       "1    0.0000   Neutral          31  \n",
       "2   -0.2500   Neutral          15  \n",
       "3   -0.5719  Negative          25  \n",
       "4    0.3182   Neutral          10  "
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_sh2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "7539f254",
   "metadata": {},
   "outputs": [],
   "source": [
    "check_appearance = df_sh2.groupby(['Character_Fix'])['Character_Fix'].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "e08ba8a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "appearance = pd.DataFrame({'Character_Fix':check_appearance.index, 'Num':check_appearance.values})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "22fa6beb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Character_Fix</th>\n",
       "      <th>Num</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ANDY</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>TROY</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>TOMMY</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>JAMES</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>DEMOGORGON</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>VICTOR CREEL</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>VICKIE</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>TERRY</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>BOB</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>HENRY</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AGENT FRAZIER</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>HOLLY</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>IVAN</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>CLAUDIA</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AGENT HARMON</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>OFFICER POWELL</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>FLORENCE</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>CAROL</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>COLONEL SULLIVAN</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>TED</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>LONNIE</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>SUZIE</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>GRIGORI</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>EDDIE</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>DEPUTY CALLAHAN</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>VECNA</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>JASON</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>MRS DRISCOLL</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>HEATHER</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>SUSAN</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>BARBARA</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>BRUCE</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>MR CLARKE</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ARGYLE</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ALEXEI</td>\n",
       "      <td>27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>YURI</td>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>TOM</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>BENNY</td>\n",
       "      <td>31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>DMITRI</td>\n",
       "      <td>33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>DR OWENS</td>\n",
       "      <td>33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>MAYOR KLINE</td>\n",
       "      <td>56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>DR BRENNER</td>\n",
       "      <td>58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>KAREN</td>\n",
       "      <td>87</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>BILLY</td>\n",
       "      <td>116</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>ERICA</td>\n",
       "      <td>121</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>MURRAY</td>\n",
       "      <td>133</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>WILL</td>\n",
       "      <td>151</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>ELEVEN</td>\n",
       "      <td>171</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>JONATHAN</td>\n",
       "      <td>185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>LUCAS</td>\n",
       "      <td>216</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>MAX</td>\n",
       "      <td>237</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>ROBIN</td>\n",
       "      <td>260</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>NANCY</td>\n",
       "      <td>309</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>JOYCE</td>\n",
       "      <td>336</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>STEVE</td>\n",
       "      <td>372</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>DUSTIN</td>\n",
       "      <td>387</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>MIKE</td>\n",
       "      <td>442</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>HOPPER</td>\n",
       "      <td>501</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Character_Fix  Num\n",
       "3               ANDY    1\n",
       "52              TROY    1\n",
       "51             TOMMY    1\n",
       "29             JAMES    1\n",
       "13        DEMOGORGON    1\n",
       "55      VICTOR CREEL    2\n",
       "54            VICKIE    2\n",
       "49             TERRY    2\n",
       "8                BOB    2\n",
       "25             HENRY    2\n",
       "0      AGENT FRAZIER    3\n",
       "26             HOLLY    4\n",
       "28              IVAN    5\n",
       "11           CLAUDIA    5\n",
       "1       AGENT HARMON    6\n",
       "43    OFFICER POWELL    7\n",
       "22          FLORENCE    8\n",
       "10             CAROL    9\n",
       "12  COLONEL SULLIVAN    9\n",
       "48               TED   11\n",
       "34            LONNIE   11\n",
       "47             SUZIE   14\n",
       "23           GRIGORI   14\n",
       "19             EDDIE   15\n",
       "14   DEPUTY CALLAHAN   15\n",
       "53             VECNA   17\n",
       "30             JASON   17\n",
       "40      MRS DRISCOLL   17\n",
       "24           HEATHER   19\n",
       "46             SUSAN   20\n",
       "5            BARBARA   20\n",
       "9              BRUCE   22\n",
       "39         MR CLARKE   25\n",
       "4             ARGYLE   26\n",
       "2             ALEXEI   27\n",
       "57              YURI   29\n",
       "50               TOM   30\n",
       "6              BENNY   31\n",
       "15            DMITRI   33\n",
       "17          DR OWENS   33\n",
       "37       MAYOR KLINE   56\n",
       "16        DR BRENNER   58\n",
       "33             KAREN   87\n",
       "7              BILLY  116\n",
       "21             ERICA  121\n",
       "41            MURRAY  133\n",
       "56              WILL  151\n",
       "20            ELEVEN  171\n",
       "31          JONATHAN  185\n",
       "35             LUCAS  216\n",
       "36               MAX  237\n",
       "44             ROBIN  260\n",
       "42             NANCY  309\n",
       "32             JOYCE  336\n",
       "45             STEVE  372\n",
       "18            DUSTIN  387\n",
       "38              MIKE  442\n",
       "27            HOPPER  501"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "appearance.sort_values(by='Num')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "62a13f96",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sh2 = df_sh2.merge(appearance, how=\"left\", on ='Character_Fix').copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "03a11882",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sh2 = df_sh2[df_sh2['Num']> 2].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "3aeca194",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sh2['Gender'].replace(['Female ', 'Female','Male','Male ','Neutral'],\n",
    "                        [1, 1,2,2,0], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "1f741358",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sh2['sentiment'].replace(['Negative', 'Neutral','Positive'],\n",
    "                        [-1,0,1], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "8f000123",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sh2.drop(columns=['Dialog','length','polarity'],inplace=True, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "0f327411",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Character_Fix</th>\n",
       "      <th>Season</th>\n",
       "      <th>Episode</th>\n",
       "      <th>token_lemma</th>\n",
       "      <th>Gender</th>\n",
       "      <th>Age</th>\n",
       "      <th>grade</th>\n",
       "      <th>difficulty</th>\n",
       "      <th>compound</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>length_spk</th>\n",
       "      <th>Num</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>HOPPER</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>look like somebody home as the truck park -pro...</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2.6</td>\n",
       "      <td>11</td>\n",
       "      <td>-0.3612</td>\n",
       "      <td>0</td>\n",
       "      <td>59</td>\n",
       "      <td>501</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>HOPPER</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>where that come from joyce pause at the foot o...</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2.8</td>\n",
       "      <td>3</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0</td>\n",
       "      <td>31</td>\n",
       "      <td>501</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>HOPPER</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>hey dipshit the man drop -pron- tool and stand...</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0.3</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.2500</td>\n",
       "      <td>0</td>\n",
       "      <td>15</td>\n",
       "      <td>501</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>HOPPER</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>alexei step forward put -pron- between hopper ...</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>4.3</td>\n",
       "      <td>6</td>\n",
       "      <td>-0.5719</td>\n",
       "      <td>-1</td>\n",
       "      <td>25</td>\n",
       "      <td>501</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>HOPPER</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>english -pron- speak english alexei try to cal...</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>5.2</td>\n",
       "      <td>2</td>\n",
       "      <td>0.3182</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>501</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Character_Fix  Season  Episode  \\\n",
       "0        HOPPER       3        5   \n",
       "1        HOPPER       3        5   \n",
       "2        HOPPER       3        5   \n",
       "3        HOPPER       3        5   \n",
       "4        HOPPER       3        5   \n",
       "\n",
       "                                         token_lemma  Gender  Age  grade  \\\n",
       "0  look like somebody home as the truck park -pro...       2    1    2.6   \n",
       "1  where that come from joyce pause at the foot o...       2    1    2.8   \n",
       "2  hey dipshit the man drop -pron- tool and stand...       2    1    0.3   \n",
       "3  alexei step forward put -pron- between hopper ...       2    1    4.3   \n",
       "4  english -pron- speak english alexei try to cal...       2    1    5.2   \n",
       "\n",
       "   difficulty  compound  sentiment  length_spk  Num  \n",
       "0          11   -0.3612          0          59  501  \n",
       "1           3    0.0000          0          31  501  \n",
       "2           1   -0.2500          0          15  501  \n",
       "3           6   -0.5719         -1          25  501  \n",
       "4           2    0.3182          0          10  501  "
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_sh2.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4efa9f20",
   "metadata": {},
   "source": [
    "### Let's vectorize and train with our hyper parameters designated above before we re-tune with this new df with more features and removal of characters who appeared two times or less and removal of character rows when they spoke who spoke 3 words or less."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9269e509",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_sh2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fa6828d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_train2 = df_sh2.drop(columns=['Character_Fix'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "ab021339",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from sklearn.model_selection import train_test_split\n",
    "#X_train, X_test, y_train, y_test = train_test_split(x_var_2d, y_var, test_size = .2)\n",
    "train_df2_fin, test_df2_fin = \\\n",
    "              np.split(df_sh2.sample(frac=1, random_state=RANDOM_SEED), \n",
    "                       [int(.8*len(df_sh2))])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "681317ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3712 928\n"
     ]
    }
   ],
   "source": [
    "print(len(train_df2_fin), len(test_df2_fin))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "d92f66a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer2 = TfidfVectorizer(min_df = 20, stop_words = 'english')\n",
    "X_train2 = vectorizer2.fit_transform(train_df2_fin.token_lemma)\n",
    "X_test2 = vectorizer2.transform(test_df2_fin.token_lemma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "17723dcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train2=train_df2_fin['Character_Fix'].values\n",
    "y_test2=test_df2_fin['Character_Fix'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23d1b9ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_scaler2 = preprocessing.StandardScaler(with_mean=False).fit_transform(X_train2)\n",
    "# X_test_Scaler2 = preprocessing.StandardScaler(with_mean=False).fit_transform(X_test2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "3fc053d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tf2_fin = pd.DataFrame(X_train2.toarray(), columns=vectorizer2.get_feature_names_out())\n",
    "train_df2_fin.reset_index(inplace=True)\n",
    "fin_train_w_feat = pd.concat([df_tf2_fin,train_df2_fin],axis=1)\n",
    "fin_train_w_feat.drop(['token_lemma','index'], axis=1, inplace=True)\n",
    "\n",
    "#doing the same to the test data\n",
    "df_tf_test_fin = pd.DataFrame(X_test2.toarray(), columns=vectorizer2.get_feature_names_out())\n",
    "final_df_test = test_df2_fin.reset_index()\n",
    "test_w_feat_final = pd.concat([df_tf_test_fin,final_df_test],axis=1)\n",
    "test_w_feat_final.drop(['token_lemma','index'], axis=1, inplace=True)\n",
    "#drpping character label names\n",
    "test_w_feat_final.drop(['Character_Fix'],axis=1,inplace = True)\n",
    "fin_train_w_feat.drop(['Character_Fix'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "3af64ed6",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8868534482758621\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "#now we try it out\n",
    "\n",
    "\n",
    "\n",
    "RCF_model_tuned_v2 = RandomForestClassifier(n_estimators=100, \\\n",
    "                              random_state=RANDOM_SEED,\\\n",
    "                              max_features='sqrt').fit(fin_train_w_feat, y_train2)\n",
    "\n",
    "y_pred_RCF_model_tuned2 = RCF_model_tuned_v2.predict(test_w_feat_final)\n",
    "\n",
    "\n",
    "accm_tuned2 = metrics.accuracy_score(y_test2,y_pred_RCF_model_tuned2)\n",
    "\n",
    "print(accm_tuned2)\n",
    "\n",
    "acc_train2 = RCF_model_tuned_v2.score(fin_train_w_feat, y_train2)\n",
    "\n",
    "print(acc_train2)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# lg_model_tuned_v2 = LogisticRegression(max_iter = 500,\n",
    "#                               random_state=RANDOM_SEED,\n",
    "#                               solver = 'liblinear',penalty= 'l1',\n",
    "#                               C = 0.1).fit(fin_train_w_feat, y_train2)\n",
    "\n",
    "# y_pred_lg_model_tuned2 = lg_model_tuned_v2.predict(test_w_feat_final)\n",
    "\n",
    "# accm_tuned2 = metrics.accuracy_score(y_test2,y_pred_lg_model_tuned2)\n",
    "\n",
    "# print(accm_tuned2)\n",
    "\n",
    "# acc_train2 = lg_model_tuned_v2.score(fin_train_w_feat, y_train2)\n",
    "\n",
    "# print(acc_train2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "inner-ceramic",
   "metadata": {},
   "source": [
    "done"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab4836bc",
   "metadata": {},
   "source": [
    "#### Tuning with our model which has features and row removed (based on too few appearences overall and too few words spoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a597c4a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train_scaler_final = preprocessing.StandardScaler(with_mean=False).fit_transform(fin_train_w_feat)\n",
    "# X_test_scaler_final = preprocessing.StandardScaler(with_mean=False).fit_transform(test_w_feat_final)\n",
    "\n",
    "param_grid_ 2 = {'n_estimators': [500, 800, 1500, 2500, 5000],\n",
    "               'max_features': ['auto', 'sqrt', 'log2'],\n",
    "               'max_depth':[10, 20, 30, 40, 50],\n",
    "               'min_samples_split': [2, 5, 10, 15, 20],\n",
    "               'min_samples_leaf': [1, 2, 5, 10, 15]\n",
    "              }\n",
    "\n",
    "\n",
    "RCF2 = RandomForestClassifier(random_state=RANDOM_SEED)\n",
    "RCF_random2 = RandomizedSearchCV(estimator=RCF2,\n",
    "                                param_distributions=param_grid_,\n",
    "                               n_iter=100,\n",
    "                               cv=5,\n",
    "                               verbose=2,\n",
    "                               random_state=RANDOM_SEED,\n",
    "                               )\n",
    "\n",
    "RCF_random2.fit(fin_train_w_feat,y_train2)\n",
    "\n",
    "print(RCF_random2.best_score_)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "d72a479f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8868534482758621\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "X has 275 features, but DecisionTreeClassifier is expecting 290 features as input.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-143-9209285d33b4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maccm_tuned3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m \u001b[0macc_train3\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRCF_model_tuned\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfin_train_w_feat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0macc_train3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/sklearn/base.py\u001b[0m in \u001b[0;36mscore\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    498\u001b[0m         \"\"\"\n\u001b[1;32m    499\u001b[0m         \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0maccuracy_score\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 500\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0maccuracy_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    501\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    502\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_more_tags\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/sklearn/ensemble/_forest.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    628\u001b[0m             \u001b[0mThe\u001b[0m \u001b[0mpredicted\u001b[0m \u001b[0mclasses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    629\u001b[0m         \"\"\"\n\u001b[0;32m--> 630\u001b[0;31m         \u001b[0mproba\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_proba\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    631\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    632\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_outputs_\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/sklearn/ensemble/_forest.py\u001b[0m in \u001b[0;36mpredict_proba\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    672\u001b[0m         \u001b[0mcheck_is_fitted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    673\u001b[0m         \u001b[0;31m# Check data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 674\u001b[0;31m         \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_X_predict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    675\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    676\u001b[0m         \u001b[0;31m# Assign chunk of trees to jobs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/sklearn/ensemble/_forest.py\u001b[0m in \u001b[0;36m_validate_X_predict\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    420\u001b[0m         \u001b[0mcheck_is_fitted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    421\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 422\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mestimators_\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_X_predict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcheck_input\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    423\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    424\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/sklearn/tree/_classes.py\u001b[0m in \u001b[0;36m_validate_X_predict\u001b[0;34m(self, X, check_input)\u001b[0m\n\u001b[1;32m    401\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcheck_input\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    402\u001b[0m             X = self._validate_data(X, dtype=DTYPE, accept_sparse=\"csr\",\n\u001b[0;32m--> 403\u001b[0;31m                                     reset=False)\n\u001b[0m\u001b[1;32m    404\u001b[0m             if issparse(X) and (X.indices.dtype != np.intc or\n\u001b[1;32m    405\u001b[0m                                 X.indptr.dtype != np.intc):\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/sklearn/base.py\u001b[0m in \u001b[0;36m_validate_data\u001b[0;34m(self, X, y, reset, validate_separately, **check_params)\u001b[0m\n\u001b[1;32m    435\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    436\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcheck_params\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'ensure_2d'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 437\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_n_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    438\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    439\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/sklearn/base.py\u001b[0m in \u001b[0;36m_check_n_features\u001b[0;34m(self, X, reset)\u001b[0m\n\u001b[1;32m    364\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mn_features\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_features_in_\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    365\u001b[0m             raise ValueError(\n\u001b[0;32m--> 366\u001b[0;31m                 \u001b[0;34mf\"X has {n_features} features, but {self.__class__.__name__} \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    367\u001b[0m                 f\"is expecting {self.n_features_in_} features as input.\")\n\u001b[1;32m    368\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: X has 275 features, but DecisionTreeClassifier is expecting 290 features as input."
     ]
    }
   ],
   "source": [
    "#now we try it out\n",
    "\n",
    "\n",
    "\n",
    "RCF_model_tuned3 = RandomForestClassifier(n_estimators=100, \\\n",
    "                              random_state=RANDOM_SEED,\\\n",
    "                              max_features='sqrt').fit(fin_train_w_feat, y_train2)\n",
    "\n",
    "y_pred_RCF_model_tuned3 = RCF_model_tuned3.predict(test_w_feat_final)\n",
    "\n",
    "\n",
    "accm_tuned3 = metrics.accuracy_score(y_test2,y_pred_RCF_model_tuned3)\n",
    "\n",
    "print(accm_tuned3)\n",
    "\n",
    "acc_train3 = RCF_model_tuned3.score(fin_train_w_feat, y_train2)\n",
    "\n",
    "print(acc_train3)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "1c3bc18c",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'y_pred_lg_model_tuned3' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-144-7db420ede467>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m print('Classification report of imbalanced logistic regression \\n',classification_report(y_test2,\n\u001b[0;32m----> 5\u001b[0;31m                                                                                          \u001b[0my_pred_lg_model_tuned3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m                                                                                         zero_division = 0))\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'y_pred_lg_model_tuned3' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "import matplotlib.pyplot as plt\n",
    "print('Classification report of imbalanced logistic regression \\n',classification_report(y_test2,\n",
    "                                                                                         y_pred_lg_model_tuned3,\n",
    "                                                                                        zero_division = 0))\n",
    "\n",
    "cm = confusion_matrix(y_test2,y_pred_RCF_model_tuned3,normalize='true',\n",
    "                      labels=RCF_model_tuned_v3.classes_)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm,\n",
    "                              display_labels=RCF_model_tuned_v3.classes_)\n",
    "fig2, ax2 = plot.subplots(figsize=(12,12))\n",
    "disp.plot(cmap='magma', ax=ax2,xticks_rotation='vertical')\n",
    "plt.grid(False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f32da76",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from sklearn.metrics import RocCurveDisplay\n",
    "#RocCurveDisplay.from_predictions(y_test2,y_pred_lg_model_tuned3)\n",
    "\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "688305d1",
   "metadata": {},
   "source": [
    "### Liblinear had the same test score but slightly lower training we will keep with Saga"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ed51dcc",
   "metadata": {},
   "source": [
    "### Last filter to help the classification with features added to the data is to only use top 20 Characters, this value is based off length of lines spoken summed for a character across the entire data set.  Hopefully the classification will be helped by this filter on the data since it will have more lines to learn off of for a character. Where it is really failing on characters with not a lot of line or characters who have spoken to the same character often, example is Eleven and Karen Wheeler who speak to Mike often and in a similar manner.  While Karen is Mike's mother and Eleven is Mike's girlfriend - although Karen has less lines than Eleven both are still main characters and speak to Mike in a similar manner - for some reason this is not surprising this is happening.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9291350",
   "metadata": {},
   "source": [
    "### It might be a nice add on to explore this failure by taking the characters and seeing their cosine similarity scores based on lines spoken compared to other characters for the Top 20."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "cd72e350",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20\n",
      "20\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>Character_Fix</th>\n",
       "      <th>Season</th>\n",
       "      <th>Episode</th>\n",
       "      <th>token_lemma</th>\n",
       "      <th>Gender</th>\n",
       "      <th>Age</th>\n",
       "      <th>grade</th>\n",
       "      <th>difficulty</th>\n",
       "      <th>compound</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>length_spk</th>\n",
       "      <th>Num</th>\n",
       "      <th>TOP_20</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2858</td>\n",
       "      <td>MIKE</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>-pron- just -pron- do feel good -pron- wake up...</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>6.8</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.6469</td>\n",
       "      <td>-1</td>\n",
       "      <td>48</td>\n",
       "      <td>442</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2681</td>\n",
       "      <td>NANCY</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>no -pron- mean -pron- can be late -pron- like ...</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.2023</td>\n",
       "      <td>0</td>\n",
       "      <td>15</td>\n",
       "      <td>309</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3106</td>\n",
       "      <td>KAREN</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>-pron- not mad at -pron-</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1.9</td>\n",
       "      <td>0</td>\n",
       "      <td>0.3875</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>87</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>774</td>\n",
       "      <td>BILLY</td>\n",
       "      <td>3</td>\n",
       "      <td>8</td>\n",
       "      <td>go the mind flayer tentacle have nearly envelo...</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>5.6</td>\n",
       "      <td>3</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>116</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>474</td>\n",
       "      <td>HOPPER</td>\n",
       "      <td>3</td>\n",
       "      <td>8</td>\n",
       "      <td>murray all set down there</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>501</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   index Character_Fix  Season  Episode  \\\n",
       "0   2858          MIKE       1        2   \n",
       "2   2681         NANCY       3        1   \n",
       "3   3106         KAREN       1        2   \n",
       "4    774         BILLY       3        8   \n",
       "5    474        HOPPER       3        8   \n",
       "\n",
       "                                         token_lemma  Gender  Age  grade  \\\n",
       "0  -pron- just -pron- do feel good -pron- wake up...       2    2    6.8   \n",
       "2  no -pron- mean -pron- can be late -pron- like ...       1    2    0.3   \n",
       "3                           -pron- not mad at -pron-       1    1   -1.9   \n",
       "4  go the mind flayer tentacle have nearly envelo...       2    2    5.6   \n",
       "5                          murray all set down there       2    1    0.5   \n",
       "\n",
       "   difficulty  compound  sentiment  length_spk  Num  TOP_20  \n",
       "0           1   -0.6469         -1          48  442    True  \n",
       "2           0   -0.2023          0          15  309    True  \n",
       "3           0    0.3875          0           5   87    True  \n",
       "4           3    0.0000          0          12  116    True  \n",
       "5           1    0.0000          0           5  501    True  "
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_top20_fin=train_df2_fin.copy()\n",
    "\n",
    "train_top20_fin['TOP_20'] = train_top20_fin['Character_Fix'].apply(lambda x: \\\n",
    "                                True if x in top20chars else False)\n",
    "\n",
    "print(len(top20chars))\n",
    "\n",
    "train_top20_fin = train_top20_fin[train_top20_fin['TOP_20']==True].copy()\n",
    "\n",
    "print(len(list(train_top20_fin.Character_Fix.unique())))\n",
    "\n",
    "train_top20_fin.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "7af4d993",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Character_Fix\n",
       "BILLY          4\n",
       "DR BRENNER     4\n",
       "DR OWENS       4\n",
       "DUSTIN         4\n",
       "ELEVEN         4\n",
       "ERICA          4\n",
       "HOPPER         4\n",
       "JONATHAN       4\n",
       "JOYCE          4\n",
       "KAREN          4\n",
       "LUCAS          4\n",
       "MAX            4\n",
       "MAYOR KLINE    4\n",
       "MIKE           4\n",
       "MURRAY         4\n",
       "NANCY          4\n",
       "ROBIN          4\n",
       "STEVE          4\n",
       "TOM            7\n",
       "WILL           4\n",
       "Name: length_spk, dtype: int64"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_top20_fin.groupby(['Character_Fix'])['length_spk'].min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "654b7664",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Character_Fix</th>\n",
       "      <th>Season</th>\n",
       "      <th>Episode</th>\n",
       "      <th>token_lemma</th>\n",
       "      <th>Gender</th>\n",
       "      <th>Age</th>\n",
       "      <th>grade</th>\n",
       "      <th>difficulty</th>\n",
       "      <th>compound</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>length_spk</th>\n",
       "      <th>Num</th>\n",
       "      <th>TOP_20</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3076</th>\n",
       "      <td>NANCY</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>nice the search party have move to the immedia...</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>6.6</td>\n",
       "      <td>12</td>\n",
       "      <td>0.8720</td>\n",
       "      <td>1</td>\n",
       "      <td>45</td>\n",
       "      <td>309</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4260</th>\n",
       "      <td>LUCAS</td>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "      <td>-pron- identify -pron- most likely next victim...</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>6.4</td>\n",
       "      <td>3</td>\n",
       "      <td>-0.3321</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>216</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4296</th>\n",
       "      <td>JONATHAN</td>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "      <td>sure -pron- get -pron- measurement right</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2.1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.3182</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>185</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2279</th>\n",
       "      <td>WILL</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>-pron- build -pron- to stop eleven</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.2960</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>151</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1199</th>\n",
       "      <td>TOM</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>no as heather stand up billy kneel in front of...</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.5574</td>\n",
       "      <td>-1</td>\n",
       "      <td>16</td>\n",
       "      <td>30</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Character_Fix  Season  Episode  \\\n",
       "3076         NANCY       1        2   \n",
       "4260         LUCAS       4        8   \n",
       "4296      JONATHAN       4        8   \n",
       "2279          WILL       3        7   \n",
       "1199           TOM       3        4   \n",
       "\n",
       "                                            token_lemma  Gender  Age  grade  \\\n",
       "3076  nice the search party have move to the immedia...       1    2    6.6   \n",
       "4260  -pron- identify -pron- most likely next victim...       2    2    6.4   \n",
       "4296           sure -pron- get -pron- measurement right       2    2    2.1   \n",
       "2279                 -pron- build -pron- to stop eleven       2    2    0.9   \n",
       "1199  no as heather stand up billy kneel in front of...       2    1    0.5   \n",
       "\n",
       "      difficulty  compound  sentiment  length_spk  Num  TOP_20  \n",
       "3076          12    0.8720          1          45  309    True  \n",
       "4260           3   -0.3321          0           8  216    True  \n",
       "4296           1    0.3182          0           6  185    True  \n",
       "2279           0   -0.2960          0           6  151    True  \n",
       "1199           1   -0.5574         -1          16   30    True  "
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_top20_fin=test_df2_fin.copy()\n",
    "\n",
    "test_top20_fin['TOP_20'] = test_top20_fin['Character_Fix'].apply(lambda x: \\\n",
    "                                True if x in top20chars else False)\n",
    "\n",
    "#print(len(top20chars))\n",
    "\n",
    "test_top20_fin = test_top20_fin[test_top20_fin['TOP_20']==True].copy()\n",
    "\n",
    "print(len(list(test_top20_fin.Character_Fix.unique())))\n",
    "\n",
    "test_top20_fin.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "3818b444",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer_final_top20 = TfidfVectorizer(min_df = 20, stop_words = 'english')\n",
    "X_train_top20 = vectorizer_final_top20.fit_transform(train_top20_fin.token_lemma)\n",
    "X_test_top20 = vectorizer_final_top20.transform(test_top20_fin.token_lemma)\n",
    "\n",
    "y_train_top20=train_top20_fin['Character_Fix'].values\n",
    "y_test_top20=test_top20_fin['Character_Fix'].values\n",
    "\n",
    "#scaling since it is sparce and the tuner didnt like it not scaled at any variatio or \n",
    "#iteration before\n",
    "X_scaler_top20 = preprocessing.StandardScaler(with_mean=False).fit_transform(X_train_top20)\n",
    "X_test_Scaler_top20 = preprocessing.StandardScaler(with_mean=False).fit_transform(X_test_top20)\n",
    "\n",
    "#adding features back on to both train and test\n",
    "df_top20_fin = pd.DataFrame(X_scaler_top20.toarray(), \\\n",
    "                          columns=vectorizer_final_top20.get_feature_names())\n",
    "train_top20_fin.reset_index(inplace=True)\n",
    "X_train_20fin = pd.concat([df_top20_fin,train_top20_fin],axis=1)\n",
    "X_train_20fin.drop(['token_lemma','index','TOP_20'], axis=1, inplace=True)\n",
    "\n",
    "#doing the same to the test data-adding features back on\n",
    "df_tf_test_fin = pd.DataFrame(X_test_Scaler_top20.toarray(), columns=vectorizer_final_top20.get_feature_names())\n",
    "test_top20_fin.reset_index(inplace = True)\n",
    "X_test_20fin = pd.concat([df_tf_test_fin,test_top20_fin],axis=1)\n",
    "X_test_20fin.drop(['token_lemma','index','TOP_20'], axis=1, inplace=True)\n",
    "#drpping character label names\n",
    "X_test_20fin.drop(['Character_Fix'],axis=1,inplace = True)\n",
    "X_train_20fin.drop(['Character_Fix'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "b14903b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_20fin.isna().values.any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "02618f40",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test_20fin.isna().values.any()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a05158b",
   "metadata": {},
   "source": [
    "### comment drop level_0 out if not needed be for shape error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "c0d413ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_train_20fin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "3b791807",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_20fin.drop(['level_0'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "78c792f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy Score: 1.0\n",
      "Test Accuracy Score: 0.9142185663924794\n",
      "Test Accuracy Score: 0.9142185663924794\n"
     ]
    }
   ],
   "source": [
    "RCF_final_top20 = RandomForestClassifier(n_estimators=100, \\\n",
    "                              random_state=RANDOM_SEED,\\\n",
    "                              max_features='sqrt').fit(X_train_20fin, y_train_top20)\n",
    "\n",
    "y_pred_rcf_top20 = RCF_final_top20.predict(X_test_20fin)\n",
    "\n",
    "\n",
    "accm20 = metrics.accuracy_score(y_test_top20,y_pred_rcf_top20)\n",
    "\n",
    "\n",
    "RCF_model_SCR20 = RCF_final_top20.score(X_test_20fin, y_test_top20)\n",
    "\n",
    "\n",
    "\n",
    "RCF_model_train_scr_top20 = RCF_final_top20.score(X_train_20fin, y_train_top20)\n",
    "\n",
    "\n",
    "print(\"Train Accuracy Score:\",RCF_model_train_scr_top20)\n",
    "\n",
    "print(\"Test Accuracy Score:\",accm20)\n",
    "\n",
    "print(\"Test Accuracy Score:\",RCF_model_SCR20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "obvious-sierra",
   "metadata": {},
   "source": [
    "DONE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "unexpected-dialogue",
   "metadata": {},
   "source": [
    "print('Classification report of imbalanced logistic regression \\n',classification_report(y_test2,\n",
    "                                                                                         y_pred_lg_model_tuned3,\n",
    "                                                                                        zero_division = 0))\n",
    "\n",
    "cm20 = confusion_matrix(y_test_top20,y_pred_lg_top20,normalize='true',\n",
    "                      labels=lg_final_top20.classes_)\n",
    "disp20 = ConfusionMatrixDisplay(confusion_matrix=cm20,\n",
    "                              display_labels=lg_final_top20.classes_)\n",
    "fig2, ax2 = plot.subplots(figsize=(14,14))\n",
    "disp20.plot(cmap='magma', ax=ax2,xticks_rotation='vertical')\n",
    "plt.grid(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4731958f",
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid_top20 = {'C': [0.01,0.1,0.5,1.0,5,10],\n",
    "                'penalty':['l1','l2'],\n",
    "                'solver':['liblinear','lbfgs','sag','saga'],\n",
    "                }\n",
    "#bow_search = model_selection.GridSearchCV(LogisticRegression(), cv=5, param_grid=param_grid_)\n",
    "final_tune_top20 = model_selection.GridSearchCV(LogisticRegression(random_state = RANDOM_SEED,\\\n",
    "                                                              max_iter = 250),n_jobs = 50,\n",
    "                                                              param_grid=param_grid_top20,\n",
    "                                                              scoring = 'accuracy',\n",
    "                                                              cv = 2)\n",
    "\n",
    "final_tune_top20.fit(X_train_20fin, y_train_top20)\n",
    "\n",
    "print(\"Tuned Hyperparameters:\", final_tune_top20.best_params_)\n",
    "print(\"Accuracy:\",final_tune_top20.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1710d44",
   "metadata": {},
   "source": [
    "### So with the chopped down data set to only top 20 characters it is saying the accuracy could be 70% which is better but still doesnt make complete sense.  It is also saying the new solver should be liblinear.  We also need to check with class_weight =balanced and then the max_iter but we will do that when we figure out which data/features are best for our model. Also C is now 1.0 instead of 0.1 or 0.5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c67d3c88",
   "metadata": {},
   "outputs": [],
   "source": [
    "lg_final_top20 = LogisticRegression(max_iter = 1000,\n",
    "                              random_state=RANDOM_SEED,\n",
    "                              solver = 'liblinear',penalty= 'l1',\n",
    "                              C = 1.0).fit(X_train_20fin, y_train_top20)\n",
    "\n",
    "y_pred_lg_top20 = lg_final_top20.predict(X_test_20fin)\n",
    "\n",
    "accm20 = metrics.accuracy_score(y_test_top20,y_pred_lg_top20)\n",
    "lg_model_scr20 = lg_final_top20.score(X_test_20fin, y_test_top20)\n",
    "\n",
    "lg_model_Train_scr_top20 = lg_final_top20.score(X_train_20fin, y_train_top20)\n",
    "\n",
    "print(\"Train Accuracy Score:\",lg_model_Train_scr_top20)\n",
    "\n",
    "print(\"Test Accuracy Score:\",accm20)\n",
    "\n",
    "print(\"Test Accuracy Score:\",lg_model_scr20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c71e1cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Classification report of imbalanced logistic regression \\n',classification_report(y_test2,\n",
    "                                                                                         y_pred_lg_model_tuned3,\n",
    "                                                                                        zero_division = 0))\n",
    "\n",
    "cm20v2 = confusion_matrix(y_test_top20,y_pred_lg_top20,normalize='true',\n",
    "                      labels=lg_final_top20.classes_)\n",
    "disp20v2 = ConfusionMatrixDisplay(confusion_matrix=cm20,\n",
    "                              display_labels=lg_final_top20.classes_)\n",
    "fig2, ax2 = plot.subplots(figsize=(14,14))\n",
    "disp20v2.plot(cmap='magma', ax=ax2,xticks_rotation='vertical')\n",
    "plt.grid(False)\n",
    "plt.rcParams.update({'text.color': \"white\",\n",
    "                     'axes.labelcolor': \"white\"})\n",
    "plt.title(\"Logistic Regression: Top 20 Characters only and All Features\\nC: 1.0, solver:liblinear, penalty: L1, max_iter = 1000\",\n",
    "          {'fontsize': 12,'color': \"white\"})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "659bd557",
   "metadata": {},
   "source": [
    "### Wow the test accuracy went up to 75.44% with using just rows/data for the top 20 characters in the show and the training score shot up to 93.1%.  Regardless of changes we make such a feature additions such as gender, age (age group), season, episode, text difficulty, text grade level, sentiment compound value, sentiment category based on compound value, length of token for a line, total num words spoken and removal of instances where the character spoke 3 words or less based on token_lemma length."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1996b6ef",
   "metadata": {},
   "source": [
    "### Lets look at the correlation of the different features and categorize the character names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f5fa0b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dython.nominal import associations\n",
    "from dython.nominal import identify_nominal_columns\n",
    "\n",
    "train_corre=train_df2_fin.copy()\n",
    "\n",
    "train_corre['TOP_20'] = train_corre['Character_Fix'].apply(lambda x: \\\n",
    "                                True if x in top20chars else False)\n",
    "test_corre=test_df2_fin.copy()\n",
    "\n",
    "test_corre['TOP_20'] = test_corre['Character_Fix'].apply(lambda x: \\\n",
    "                                True if x in top20chars else False)\n",
    "\n",
    "check_corre = pd.concat([train_corre, test_corre], ignore_index=True, axis=0)\n",
    "\n",
    "check_corre.drop(columns=['index','token_lemma'],axis = 1,inplace = True)\n",
    "\n",
    "f = lambda x: 1 if x==True else 0\n",
    "check_corre['TOP_20'] = check_corre['TOP_20'].apply(f)\n",
    "print(check_corre.shape)\n",
    "#!pip3 install dython\n",
    "\n",
    "categorical_features=identify_nominal_columns(check_corre)\n",
    "print(categorical_features)\n",
    "\n",
    "associations(check_corre, nominal_columns='auto', \n",
    "             numerical_columns=None, mark_columns=False, \n",
    "             nom_nom_assoc='cramer', num_num_assoc='pearson', \n",
    "             cramers_v_bias_correction=True, nan_strategy=False,\n",
    "             nan_replace_value=False, ax =sns.set(rc ={'xtick.labelcolor':'white',\n",
    "                                                        'ytick.labelcolor':'white',\n",
    "                                                       'figure.facecolor':'black'}),\n",
    "             figsize=(12,10), annot=True, fmt='.2f', cmap=\"inferno\",\n",
    "             sv_color='black', cbar=True, vmax=1.0, vmin=None, plot=True,\n",
    "             compute_only=False, clustering=False, title=None, filename=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a3e0ea6",
   "metadata": {},
   "source": [
    "### As we can see the Character Fix column does has correlation between the various features we added especially the season, episode, gender, age and num of words spoken in total along with top 20.  But we only kept where top 20 was True lets see what that correlation looks like on that dataframe instead, maybe we need to not filter out the data but run the model with the column as a feature in total.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "793a8078",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_20_corre = train_top20_fin.copy()\n",
    "test_20_corre = test_top20_fin.copy()\n",
    "train_20_corre['TOP_20'] = train_20_corre['Character_Fix'].apply(lambda x: \\\n",
    "                                True if x in top20chars else False)\n",
    "\n",
    "test_20_corre['TOP_20'] = test_20_corre['Character_Fix'].apply(lambda x: \\\n",
    "                                True if x in top20chars else False)\n",
    "\n",
    "check_corre20 = pd.concat([train_20_corre, test_20_corre], ignore_index=True, axis=0)\n",
    "\n",
    "check_corre20.drop(columns=['level_0','index','token_lemma'],axis = 1,inplace = True)\n",
    "print(check_corre20.shape)\n",
    "f = lambda x: 1 if x==True else 0\n",
    "check_corre20['TOP_20'] = check_corre20['TOP_20'].apply(f)\n",
    "\n",
    "#!pip3 install dython\n",
    "\n",
    "categorical_features20=identify_nominal_columns(check_corre20)\n",
    "print(categorical_features20)\n",
    "\n",
    "associations(check_corre20, nominal_columns='auto', \n",
    "             numerical_columns=None, mark_columns=False, \n",
    "             nom_nom_assoc='cramer', num_num_assoc='pearson', \n",
    "             cramers_v_bias_correction=True, nan_strategy=False,\n",
    "             nan_replace_value=False, ax =sns.set(rc ={'xtick.labelcolor':'white',\n",
    "                                                        'ytick.labelcolor':'white',\n",
    "                                                       'figure.facecolor':'black'}),\n",
    "             figsize=(12,10), annot=True, fmt='.2f', cmap=\"inferno\",\n",
    "             sv_color='black', cbar=True, vmax=1.0, vmin=None, plot=True,\n",
    "             compute_only=False, clustering=False, title=None, filename=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c7e1939",
   "metadata": {},
   "source": [
    "### Some of the correlations are the same but some are lower for Character fix when just using top20 character data only, such as the drop in correlation for season and episode and the gender correlation to num of total lines spoken for a character was almost neutral a -0.06 but now its weighted at 0.28.  I think filter for just top 20 was not correct and we shoul have just left it as a feature instead."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cf6b5ec",
   "metadata": {},
   "source": [
    "### Running the model again but with top 20 as feature instead of a filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79757f46",
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_corre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53dfd62e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_corre\n",
    "#test_corre\n",
    "vectorizer_final_model = TfidfVectorizer(min_df = 20, stop_words = 'english')\n",
    "X_train_Final = vectorizer_final_model.fit_transform(train_corre.token_lemma)\n",
    "X_test_Final = vectorizer_final_model.transform(test_corre.token_lemma)\n",
    "\n",
    "y_train_Final=train_corre['Character_Fix'].values\n",
    "y_test_Final=test_corre['Character_Fix'].values\n",
    "\n",
    "#scaling since it is sparce and the tuner didnt like it not scaled at any variatio or \n",
    "#iteration before\n",
    "X_scaler_Final = preprocessing.StandardScaler(with_mean=False).fit_transform(X_train_Final)\n",
    "X_test_scaler_Final = preprocessing.StandardScaler(with_mean=False).fit_transform(X_test_Final)\n",
    "\n",
    "#adding features back on to both train and test\n",
    "df_Final = pd.DataFrame(X_scaler_Final.toarray(), \\\n",
    "                          columns=vectorizer_final_model.get_feature_names_out())\n",
    "train_corre.reset_index(inplace=True)\n",
    "X_train_Final_Model = pd.concat([df_Final,train_corre],axis=1)\n",
    "X_train_Final_Model.drop(['token_lemma','index','TOP_20'], axis=1, inplace=True)\n",
    "\n",
    "#doing the same to the test data-adding features back on\n",
    "df_test_Final = pd.DataFrame(X_test_scaler_Final.toarray(), columns=vectorizer_final_model.get_feature_names_out())\n",
    "test_corre.reset_index(inplace = True)\n",
    "X_test_Final_Model = pd.concat([df_test_Final,test_corre],axis=1)\n",
    "X_test_Final_Model.drop(['token_lemma','index','TOP_20'], axis=1, inplace=True)\n",
    "#drpping character label names\n",
    "X_test_Final_Model.drop(['Character_Fix'],axis=1,inplace = True)\n",
    "X_train_Final_Model.drop(['Character_Fix'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a59da074",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_Final_Model.drop(['level_0'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "786b0171",
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_test_Final_Model.drop(['level_0'],axis=1,inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb0bcc9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "lg_final_features = LogisticRegression(max_iter = 500,\n",
    "                              random_state=RANDOM_SEED,\n",
    "                              solver = 'liblinear',penalty= 'l1',\n",
    "                              C = 1.0).fit(X_train_Final_Model, y_train_Final)\n",
    "\n",
    "y_pred_FINALft = lg_final_features.predict(X_test_Final_Model)\n",
    "\n",
    "accmfin = metrics.accuracy_score(y_test_Final,y_pred_FINALft)\n",
    "lg_model_scrfin = lg_final_features.score(X_test_Final_Model, y_test_Final)\n",
    "\n",
    "lg_model_Train_scr_Final = lg_final_features.score(X_train_Final_Model, y_train_Final)\n",
    "\n",
    "print(\"Train Accuracy Score:\",lg_model_Train_scr_Final)\n",
    "\n",
    "print(\"Test Accuracy Score:\",accmfin)\n",
    "\n",
    "print(\"Test Accuracy Score:\",lg_model_scrfin)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "630b443d",
   "metadata": {},
   "source": [
    "### Not bad for keeping all the characters, all the featues and using top 20 as a feature instead of a filter but I think we will keep the top 20 as a filter with all the features since overall it performed the best so far at 76% on test data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40847f90",
   "metadata": {},
   "source": [
    "### Failure Analysis or Reasoning for mismarked lines spoken by a charcter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1534371",
   "metadata": {},
   "source": [
    "### checking cosine similarity to see why certain characters lines are being picked up by a different characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f567d7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_df = sent_dict[['Character_Fix','Dialog_clean','compound']].copy()\n",
    "vectorizer_sentiment = TfidfVectorizer(stop_words = 'english')\n",
    "sent_vect = vectorizer_sentiment.fit_transform(sentiment_df.Dialog_clean)\n",
    "\n",
    "\n",
    "#adding features back on to both train and test\n",
    "df_s = pd.DataFrame(sent_vect.toarray(), \\\n",
    "                          columns=vectorizer_sentiment.get_feature_names_out())\n",
    "#train_corre.reset_index(inplace=True)\n",
    "sentiment_send = pd.concat([sentiment_df,df_s],axis=1)\n",
    "sentiment_send.drop(['Dialog_clean'], axis=1, inplace=True)\n",
    "sentiment_send_fin = pd.merge(left = sentiment_send,right =gender, on=['Character_Fix'])\n",
    "sentiment_send_fin['Age'] = sentiment_send_fin['Age'].apply(lambda x: 1 if x==\"Female\" \\\n",
    "                                                     else(2 if x==\"Negative\" \\\n",
    "                                                     else 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e510e2c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#sentiment_send.set_index('Character_Fix')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f6e4262",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "#from scipy.spatial.distance import cosine\n",
    "\n",
    "def calc_cos_sim(df):\n",
    "    \"\"\"\n",
    "    Takes in a DataFrame and computes all pairwise Cosine Similarity\n",
    "    \"\"\"\n",
    "    s = pd.Series(['BILLY', 'DUSTIN', 'ELEVEN', 'ERICA', 'HOPPER', 'JONATHAN',\n",
    "       'JOYCE', 'KAREN', 'LUCAS', 'MAX', 'MAYOR KLINE', 'MIKE', 'MURRAY',\n",
    "       'NANCY', 'ROBIN', 'STEVE', 'TOM', 'WILL'])\n",
    "    cols = df.columns.to_list()\n",
    "    cos_sim_df = pd.DataFrame(cosine_similarity(df),columns=s)\n",
    "    cos_sim_df.set_index([s])\n",
    "    \n",
    "    return cos_sim_df.set_index([s]).fillna(1.0)\n",
    "\n",
    "fail_analysis_reason = calc_cos_sim(sentiment_send.set_index('Character_Fix'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16898d85",
   "metadata": {},
   "source": [
    "### make final dataframe of different score iterations for comparison in report and to other classification models we use/test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bb2f9dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "fail_analysis_reason.to_csv(\"sentiment_sim_fail_label.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1373e683",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e27cee3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
