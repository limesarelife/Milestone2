{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8d483ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import spacy\n",
    "from spacy import displacy\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import seaborn as sns\n",
    "import string\n",
    "import nltk\n",
    "import en_core_web_sm\n",
    "#nlp = en_core_web_sm.load()\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation as LDA\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import random\n",
    "#!pip3 install https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.0.0/en_core_web_sm-3.0.0.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "452b3414",
   "metadata": {},
   "outputs": [],
   "source": [
    "dftry2 = pd.read_csv(\"posts_st.txt\",\n",
    "                    sep = '|',\n",
    "                    names = ['AUTHOR','ID','SCORE','CREATED_DATE','TITLE','NUM_COMMENTS','SELFTEXT'],\n",
    "                    header=None, lineterminator = \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83b63d35",
   "metadata": {},
   "source": [
    "### Removing where the title or the selftext are null and removing the posts where the moderator for the Subreddit deleted/removed the post or the original author deleted/removed the post."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38e03e11",
   "metadata": {},
   "outputs": [],
   "source": [
    "dftry2.dropna(subset=['TITLE'], inplace = True)\n",
    "dftry2.dropna(subset=['SELFTEXT'], inplace = True)\n",
    "df_touse = dftry2[dftry2['SELFTEXT'] != \"[removed]\"].copy()\n",
    "df_to_use_fin = df_touse[df_touse['SELFTEXT'] != \"[deleted]\"].copy()\n",
    "df_to_use_fin['FULL_POSTS'] = df_to_use_fin['TITLE']+\".\"+\" \"+df_touse['SELFTEXT']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0bc9fdb",
   "metadata": {},
   "source": [
    "### Only keeping posts from the release date of season 3 to current. This just mirrors our supervised learning portion - although unrelated to one another except for both surround Stranger Things - we did not expect to bring back so much data from the pushshift api."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c002b1f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_to_use_fin['CREATED_DATE'] = pd.to_datetime(df_to_use_fin['CREATED_DATE'])\n",
    "df_to_use_fin = df_to_use_fin[df_to_use_fin['CREATED_DATE']> \"2019-07-04\"].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "203299a6",
   "metadata": {},
   "source": [
    "### Text Cleaning Function, removes web address which holds links and appears when someone posts a png or jpg. They do many hand drawings of characters or scenes from the show in the subreddit. Then the function sets text to lower case and then removes all punctuation. Then utilizing spacy nlp we find the words lemma and then kick it out if it is a stop words, or is an instance of non alpha characters (such as numbers), then it removes if the word itself is less than 2 chars and finally removes the word if the word's part of speech is not in the allowed parts of speech list we have.Â¶"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4602aa15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text Cleaning Function\n",
    "import re\n",
    "\n",
    "def clean(text):\n",
    "    allowed_postags=[\"NOUN\", \"VERB\", \"ADJ\", \"ADV\", \"ADP\", \"PROPN\"]\n",
    "    \n",
    "    try: \n",
    "        text1 = re.sub(r\"http\\S+\", \"\", text)\n",
    "        #Convert text in lower case\n",
    "        text2 = text1.lower()\n",
    "        #text3 = text2.replace(\"[^a-zA-Z#]\", \" \")\n",
    "        #Removing Punctuations\n",
    "        punc_removed = text2.translate(str.maketrans('', '', string.punctuation))\n",
    "        doc= nlp(punc_removed)\n",
    "        text_out = [token.lemma_ for token in doc if token.is_stop == False and \\\n",
    "                    token.is_alpha and len(token)>2 and token.pos_ in allowed_postags\\\n",
    "                   ]\n",
    "        #txt = ' '.join(text_out)\n",
    "    except:\n",
    "        #txt = ''\n",
    "        text_out = ''\n",
    "    return text_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e55bc89",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_to_use_fin['token_lemma'] = df_to_use_fin['FULL_POSTS'].apply(clean)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf5943df",
   "metadata": {},
   "source": [
    "### Getting the sentiment by utilizing vader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a918270",
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('vader_lexicon')\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "\n",
    "sid = SentimentIntensityAnalyzer()\n",
    "\n",
    "def polarity(text):\n",
    "    pol = sid.polarity_scores(text)\n",
    "    return pol\n",
    "#polarity checking\n",
    "def joiner(text):\n",
    "    txt = ' '.join(text)\n",
    "    return txt\n",
    "df_to_use_fin['FULL_POSTS_CLEAN'] = df_to_use_fin['token_lemma'].apply(joiner)\n",
    "df_to_use_fin['polarity'] = df_to_use_fin['FULL_POSTS_CLEAN'].apply(polarity)\n",
    "df_to_use_fin['compound']  = df_to_use_fin['polarity'].apply(lambda score_dict: score_dict['compound'])\n",
    "\n",
    "df_to_use_fin['sentiment'] = df_to_use_fin['compound'].apply(lambda x: \"Positive\" if x>0 else(\"Negative\" if x<0 else \"Neutral\") )\n",
    "print(\"polarity checked successfull\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bca8c1f",
   "metadata": {},
   "source": [
    "### Removal of posts where the tokenized and lemmatized title+selftext, aka POSTS, have a length less than 5. We believe this will help create more defined topics and also alleviates the issues where the poster only posted a short blurb of words and some sort of image/vid or a hyperlink elsewhere."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90aa15f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_to_use_fin['tok_len'] = df_to_use_fin['token_lemma'].apply(lambda x: len(x))\n",
    "df_to_use_fin.groupby(['tok_len'])['tok_len'].count()[2759]\n",
    "df_to_use_fin = df_to_use_fin[df_to_use_fin['tok_len']>=5].copy()\n",
    "#checking shape to see how much data we have\n",
    "df_to_use_fin.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a017d08",
   "metadata": {},
   "source": [
    "### Vectorization method - TF_IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b055922",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import pandas as pd\n",
    "tf_idf_vectorizor = TfidfVectorizer(max_features = 20000) #1\n",
    "tf_idf = tf_idf_vectorizor.fit_transform(list(df_to_use_fin['token'])) #2\n",
    "#tf_idf_array = tf_idf.toarray() \n",
    "pd.DataFrame(tf_idf.toarray(),columns=tf_idf_vectorizor.get_feature_names()).head() #4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2232ea5",
   "metadata": {},
   "source": [
    "### LDA via sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d302e16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Search Param\n",
    "search_params = {'n_components': [4,5,6,7, 8,9,10,11,12], 'learning_decay': [.5, .7, .9]}\n",
    "\n",
    "# Init the Model\n",
    "lda = LDA()\n",
    "\n",
    "# Init Grid Search Class\n",
    "model = GridSearchCV(lda, param_grid=search_params)\n",
    "\n",
    "# Do the Grid Search\n",
    "model.fit(vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e916eeb",
   "metadata": {},
   "outputs": [],
   "source": [
    " Best Model\n",
    "best_lda_model = model.best_estimator_\n",
    "\n",
    "# Model Parameters\n",
    "print(\"Best Model's Params: \", model.best_params_)\n",
    "\n",
    "# Log Likelihood Score\n",
    "print(\"Best Log Likelihood Score: \", model.best_score_)\n",
    "\n",
    "# Perplexity\n",
    "print(\"Model Perplexity: \", best_lda_model.perplexity(vectors))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8acfa641",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get Log Likelyhoods from Grid Search\n",
    "n_topics = [4,5,6,7, 8,9,10,11,12]\n",
    "log_likelyhoods_5 = [round(gscore.mean_validation_score) for gscore in model.grid_scores_ if gscore.parameters['learning_decay']==0.5]\n",
    "log_likelyhoods_7 = [round(gscore.mean_validation_score) for gscore in model.grid_scores_ if gscore.parameters['learning_decay']==0.7]\n",
    "log_likelyhoods_9 = [round(gscore.mean_validation_score) for gscore in model.grid_scores_ if gscore.parameters['learning_decay']==0.9]\n",
    "\n",
    "# Plot Topics by Log Likelihood\n",
    "plt.figure(figsize=(12, 10))\n",
    "plt.plot(n_topics, log_likelyhoods_5, label='0.5')\n",
    "plt.plot(n_topics, log_likelyhoods_7, label='0.7')\n",
    "plt.plot(n_topics, log_likelyhoods_9, label='0.9')\n",
    "plt.title(\"Choosing Optimal LDA Model\")\n",
    "plt.xlabel(\"Num Topics\")\n",
    "plt.ylabel(\"Log Likelyhood Scores\")\n",
    "plt.legend(title='Learning decay', loc='best');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aec69c39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tweak the two parameters below\n",
    "number_topics = 6\n",
    "number_words = 10\n",
    "# Create and fit the LDA model\n",
    "lda_fin = LDA(n_components=number_topics, n_jobs=-1, learning_decay=0.9)\n",
    "\n",
    "LatentDirichletAllocation(batch_size=128, doc_topic_prior=None,\n",
    "             evaluate_every=-1, learning_decay=0.9, learning_method=None,\n",
    "             learning_offset=10.0, max_doc_update_iter=100, max_iter=10,\n",
    "             mean_change_tol=0.001, n_components=6, n_jobs=-1,\n",
    "             n_topics=None, perp_tol=0.1, random_state=None,\n",
    "             topic_word_prior=True, total_samples=1000000.0, verbose=0)\n",
    "# Helper function\n",
    "def print_topics(model,n_top_words):\n",
    "    words = vocab\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        print(\"\\nTopic #%d:\" % topic_idx)\n",
    "        print(\" \".join([words[i]\n",
    "                        for i in topic.argsort()[:-n_top_words - 1:-1]]))\n",
    "# Print the topics found by the LDA model\n",
    "print(\"Topics found via LDA:\")\n",
    "print_topics(lda, number_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3afa8c85",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyLDAvis\n",
    "import pyLDAvis.sklearn\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "pyLDAvis.enable_notebook()\n",
    "panel = pyLDAvis.sklearn.prepare(lda, vectors, vectorizer, mds='tsne')\n",
    "panel"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
